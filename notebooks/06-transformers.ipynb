{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e4e7d3",
   "metadata": {},
   "source": [
    "# Lesson 5: Transformers\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Building our first transformer\n",
    "2. Visualizing the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb36e8f",
   "metadata": {},
   "source": [
    "## Bach Chorale Generation with Transformers\n",
    "\n",
    "We will use the same data set as last time and prepocess it the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd863ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset using urllib and extract with tarfile\n",
    "download_link = \"https://github.com/iCorv/jsb-chorales-dataset/raw/main/jsb_chorales.tar\"\n",
    "data_dir = Path('resources/_data/jsb_chorales')\n",
    "tar_path = data_dir / 'jsb_chorales.tar'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the file if it doesn't already exist\n",
    "if not tar_path.exists():\n",
    "    print(f\"Downloading dataset from {download_link}\")\n",
    "    urllib.request.urlretrieve(download_link, tar_path)\n",
    "    print(f\"Downloaded to {tar_path}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {tar_path}\")\n",
    "\n",
    "# Extract the tar file\n",
    "if tar_path.exists() and not (data_dir / 'jsb_chorales').exists():\n",
    "    print(f\"Extracting {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        tar.extractall(path=data_dir)\n",
    "    print(f\"Extracted to {data_dir}\")\n",
    "\n",
    "filepath = str(tar_path)\n",
    "print(f\"Dataset available at: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources._code.synthesizer import SimpleSynth\n",
    "\n",
    "baroque_synth = SimpleSynth(tempo=160, amplitude=0.1, sample_rate=44100, baroque_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def preprocess(window):\n",
    "    # Shift values: keep 0 as 0 (silence), shift other notes to start from 1\n",
    "    window = torch.where(window == 0, window, window - min_note + 1)\n",
    "    return window.reshape(-1)  # convert to arpeggio (flatten to 1D sequence)\n",
    "\n",
    "class BachDataset(Dataset):\n",
    "    def __init__(self, chorales, window_size=64, window_shift=32):\n",
    "        self.chorales = chorales\n",
    "        self.window_size = window_size\n",
    "        self.window_shift = window_shift\n",
    "        self.windows = self._create_windows()\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        for chorale in self.chorales:\n",
    "            chorale_tensor = torch.tensor(chorale, dtype=torch.long)\n",
    "            \n",
    "            # Create sliding windows\n",
    "            for i in range(0, len(chorale) - self.window_size, self.window_shift):\n",
    "                window = chorale_tensor[i:i + self.window_size + 1]\n",
    "                if len(window) == self.window_size + 1:  # Ensure full window\n",
    "                    windows.append(window)\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        # Preprocess: shift note values and flatten\n",
    "        preprocessed = preprocess(window)\n",
    "        \n",
    "        # Create input/target pairs \n",
    "        X = preprocessed[:-1]\n",
    "        Y = preprocessed[1:] # predict next note in each arpegio, at each step\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "def bach_dataloader(chorales, batch_size=32, shuffle=False, window_size=32, window_shift=16):\n",
    "    \n",
    "    dataset = BachDataset(chorales, window_size, window_shift)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1995c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_set = bach_dataloader(train_chorales, shuffle=True)\n",
    "valid_set = bach_dataloader(valid_chorales)\n",
    "test_set = bach_dataloader(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947b231",
   "metadata": {},
   "source": [
    "Now we will build our Transformer from scratch. We will make a decoder only transformer (like a gpt model) and train our Transformer on Bach chorales and compare it with the RNN approach. We'll use the same JSB Chorales dataset but leverage the Transformer's attention mechanism for better long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c359832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False) # the Value parameters\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False) # the Key parameters\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False) # the Query parameters\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False) # the output parameters\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, attention_mask=None, key_padding_mask=None):        \n",
    "        d_k = query.size(-1)\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "\n",
    "        # logits = query * key^T / sqrt(d_k)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n",
    "\n",
    "        # Attention mask here\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n",
    "                \n",
    "        # Key mask here\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n",
    "            logits = logits + key_padding_mask\n",
    "        \n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, head_dim)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "    \n",
    "    def split_into_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2) # Final dim will be (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, attention_mask=None, key_padding_mask=None):\n",
    "   \n",
    "        q = self.Wq(q) # (batch_size, query_sequence_length, d_model)\n",
    "        k = self.Wk(k) # (batch_size, key_sequence_length, d_model)\n",
    "        v = self.Wv(v) # (batch_size, key_sequence_length, d_model)\n",
    "\n",
    "        q = self.split_into_heads(q)\n",
    "        k = self.split_into_heads(k)\n",
    "        v = self.split_into_heads(v)\n",
    "        \n",
    "        attention_values, attention_weights  = self.scaled_dot_product_attention(query=q, key=k, value=v, attention_mask=attention_mask, key_padding_mask=key_padding_mask)\n",
    "        grouped = self.combine_heads(attention_values)\n",
    "        output = self.Wo(grouped)\n",
    "        \n",
    "        self.attention_weights = attention_weights\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * d_model  # Standard 4x expansion\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The first Multi-Head Attention has a mask to avoid looking at the future\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.self_attention1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.self_attention2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.ff = PositionWiseFeedForward(d_model)\n",
    "        \n",
    "        # Add proper dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout) \n",
    "        \n",
    "        \n",
    "    def forward(self, tgt, tgt_mask=None, tgt_padding_mask=None):\n",
    "        masked_att_output = self.self_attention1(q=self.norm1(tgt), k=self.norm1(tgt), v=self.norm1(tgt), attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)\n",
    "        x = tgt + self.dropout1(masked_att_output)\n",
    "\n",
    "        masked_att_output = self.self_attention2(q=self.norm2(x), k=self.norm2(x), v=self.norm2(x), attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)\n",
    "        x = x + self.dropout1(masked_att_output)\n",
    "        \n",
    "        ff_output = self.ff(self.norm3(x))\n",
    "        output = x + self.dropout2(ff_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, dropout, num_decoder_blocks, num_heads, shared_embedding):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = shared_embedding\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "          \n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, dropout, num_heads) for _ in range(num_decoder_blocks)])\n",
    "        \n",
    "        \n",
    "    def forward(self, tgt, tgt_mask=None, tgt_padding_mask=None):\n",
    "        x = self.embedding(tgt) \n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, tgt_mask=tgt_mask, tgt_padding_mask=tgt_padding_mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        for k, v in kwargs.items():\n",
    "            print(f\" * {k}={v}\")\n",
    "        \n",
    "        self.n_notes = kwargs.get('n_notes')\n",
    "        self.d_model = kwargs.get('d_model', kwargs.get('model_dim'))  # Support both names\n",
    "        self.dropout = kwargs.get('dropout')\n",
    "        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n",
    "        self.n_heads = kwargs.get('n_heads')\n",
    "        self.batch_size = kwargs.get('batch_size')\n",
    "\n",
    "        self.shared_embedding = nn.Embedding(self.n_notes, self.d_model)\n",
    "\n",
    "        self.decoder = Decoder(self.d_model, self.dropout, self.n_decoder_layers, self.n_heads, self.shared_embedding)\n",
    "        self.fc = nn.Linear(self.d_model, self.n_notes)\n",
    "        self.fc.weight = self.shared_embedding.weight  # Weight sharing!\n",
    "\n",
    "        \n",
    "\n",
    "    @staticmethod    \n",
    "    def generate_square_subsequent_mask(size, device=None):\n",
    "            mask = (1 - torch.triu(torch.ones(size, size, device=device), diagonal=1)).bool()\n",
    "            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "            return mask\n",
    "\n",
    "    \n",
    "    # def decode(self, tgt) -> torch.Tensor:    \n",
    "    #     decoder_output = self.decoder(tgt=tgt, tgt_mask=self.generate_square_subsequent_mask(tgt.size(1), device=tgt.device))  \n",
    "    #     # output = self.fc(decoder_output)  # shape (B, L, C)\n",
    "    #     return decoder_output\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x ) -> torch.Tensor:\n",
    "        # Manually apply embeddings\n",
    "        # x_embedded = self.shared_embedding(x) * math.sqrt(self.d_model)\n",
    "        # y_embedded = self.shared_embedding(y)  # No scaling for decoder\n",
    "        tgt_mask = self.generate_square_subsequent_mask(x.size(1), device=x.device)\n",
    "        # Decoder output shape (B, L, C)\n",
    "        decoder_output = self.decoder(tgt=x, tgt_mask=tgt_mask)  \n",
    "        output = self.fc(decoder_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x) -> torch.Tensor:\n",
    "        decoder_output = self.decoder(tgt=x, tgt_mask=None)  \n",
    "        output = self.fc(decoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9933fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader): \n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: encoder gets source, decoder gets shifted target\n",
    "        outputs = model(src)\n",
    "        \n",
    "        # Reshape for cross entropy loss\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "        tgt_flat = tgt.reshape(-1)\n",
    "        \n",
    "        loss = criterion(outputs_flat, tgt_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()        \n",
    " \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs_flat.max(dim=1)\n",
    "        total += tgt_flat.size(0)\n",
    "        correct += predicted.eq(tgt_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(valid_loader):  \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass: encoder gets source, decoder gets shifted target\n",
    "            outputs = model(src)\n",
    "            \n",
    "            # Reshape for cross entropy loss\n",
    "            outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, tgt_flat)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += tgt_flat.size(0)\n",
    "            correct += predicted.eq(tgt_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(valid_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop \n",
    "def train_model(model, train_loader, valid_loader, optimizer, lr_scheduler, criterion, epochs=15, device='cpu'):\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Use local lists to avoid duplication\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "        valid_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), valid_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Transformer model\n",
    "config = {\n",
    "    'n_notes': 47,\n",
    "    'd_model': 128, \n",
    "    'dropout': 0,\n",
    "    'n_decoder_layers': 4,\n",
    "    'n_heads': 8,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "\n",
    "model = Transformer(**config)\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)  # Fixed: uncommented this line\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "   \n",
    "# Run training\n",
    "train_model(model=model, train_loader=train_set, valid_loader=valid_set,  optimizer=optimizer, lr_scheduler=None, criterion=criterion, epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73020c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(test_loader): \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass: encoder gets source, decoder gets shifted target\n",
    "            outputs = model(src)\n",
    "            \n",
    "            # Reshape for cross entropy loss\n",
    "            outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, tgt_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += tgt_flat.size(0)\n",
    "            correct += predicted.eq(tgt_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "        print(f'Test Results:')\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "    \n",
    "# Evaluate the model on test set\n",
    "test_loss, test_acc = evaluate_model(model, test_set, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd034d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length=32, context_window=256):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Convert seed chords to tensor and preprocess\n",
    "        seed_tensor = torch.tensor(seed_chords, dtype=torch.long)\n",
    "        arpegio = preprocess(seed_tensor)\n",
    "        arpegio = arpegio.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        \n",
    "        # Generate new notes\n",
    "        for chord in range(length):\n",
    "            for note in range(4):\n",
    "                context = arpegio[:, -context_window:]\n",
    "            \n",
    "                # Get model prediction for the current sequence\n",
    "                outputs = model.predict(context)  # Shape: (1, seq_len, n_notes)\n",
    "                \n",
    "                \n",
    "                # Get the prediction for the last timestep\n",
    "                last_output = outputs[0, -1, :]  # Shape: (n_notes,)\n",
    "                \n",
    "                # Get the most likely next note\n",
    "                next_note = torch.argmax(last_output, dim=-1, keepdim=True)  # Shape: (1,)\n",
    "                \n",
    "                # Append the predicted note to the sequence\n",
    "                arpegio = torch.cat([arpegio, next_note.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # Convert back to original note range (reverse the preprocessing)\n",
    "        arpegio = torch.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "        \n",
    "        # Reshape to chord format (group every 4 notes)\n",
    "        arpegio_flat = arpegio.squeeze(0)  # Remove batch dimension\n",
    "        n_total_notes = len(arpegio_flat)\n",
    "        n_complete_chords = n_total_notes // 4\n",
    "        \n",
    "        # Take only complete chords and reshape\n",
    "        chorale = arpegio_flat[:n_complete_chords * 4].reshape(-1, 4)\n",
    "        \n",
    "        return chorale.cpu().numpy()  # Convert back to numpy for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afddb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:12]\n",
    "# baroque_synth.play_chorale(seed_chords)   \n",
    "print(\"Seed Chorale:\")\n",
    "print(seed_chords)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, length=32)\n",
    "baroque_synth.play_chorale(new_chorale)\n",
    "print(\"Generated Chorale:\")\n",
    "print(new_chorale[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ce91d",
   "metadata": {},
   "source": [
    "## Analyzing Transformer vs RNN Performance\n",
    "\n",
    "### Key Differences to Highlight:\n",
    "\n",
    "1. **Attention Mechanism**: \n",
    "   - Transformer can attend to any position in the sequence simultaneously\n",
    "   - RNN processes sequentially, potentially losing long-term dependencies\n",
    "\n",
    "2. **Parallelization**: \n",
    "   - Transformer training can be parallelized (all positions at once)\n",
    "   - RNN training is inherently sequential\n",
    "\n",
    "3. **Musical Structure**:\n",
    "   - Transformer might better capture harmonic relationships across time\n",
    "   - Can potentially learn chord progressions and voice leading patterns\n",
    "\n",
    "4. **Generation Quality**:\n",
    "   - Compare coherence of generated chorales\n",
    "   - Look at harmonic consistency and voice independence\n",
    "\n",
    "### Exercises for Students:\n",
    "\n",
    "1. **Experiment with attention heads**: Try different numbers of attention heads and see how it affects generation quality\n",
    "\n",
    "2. **Temperature sampling**: Adjust the temperature parameter to control randomness vs structure\n",
    "\n",
    "3. **Seed analysis**: Try different seed sequences and observe how the model continues them\n",
    "\n",
    "4. **Attention visualization**: Plot attention weights to see what the model focuses on (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcddd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
