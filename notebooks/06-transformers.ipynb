{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e4e7d3",
   "metadata": {},
   "source": [
    "# Lesson 5: Transformers\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Building our first transformer\n",
    "2. Visualizing the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb36e8f",
   "metadata": {},
   "source": [
    "## Bach Chorale Generation with Transformers\n",
    "\n",
    "We will use the same data set as last time and prepocess it the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd863ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset using urllib and extract with tarfile\n",
    "download_link = \"https://github.com/iCorv/jsb-chorales-dataset/raw/main/jsb_chorales.tar\"\n",
    "data_dir = Path('resources/_data/jsb_chorales')\n",
    "tar_path = data_dir / 'jsb_chorales.tar'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the file if it doesn't already exist\n",
    "if not tar_path.exists():\n",
    "    print(f\"Downloading dataset from {download_link}\")\n",
    "    urllib.request.urlretrieve(download_link, tar_path)\n",
    "    print(f\"Downloaded to {tar_path}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {tar_path}\")\n",
    "\n",
    "# Extract the tar file\n",
    "if tar_path.exists() and not (data_dir / 'jsb_chorales').exists():\n",
    "    print(f\"Extracting {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        tar.extractall(path=data_dir)\n",
    "    print(f\"Extracted to {data_dir}\")\n",
    "\n",
    "filepath = str(tar_path)\n",
    "print(f\"Dataset available at: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources._code.synthesizer import SimpleSynth\n",
    "\n",
    "baroque_synth = SimpleSynth(tempo=160, amplitude=0.1, sample_rate=44100, baroque_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def preprocess(window):\n",
    "    # Shift values: keep 0 as 0 (silence), shift other notes to start from 1\n",
    "    window = torch.where(window == 0, window, window - min_note + 1)\n",
    "    return window.reshape(-1)  # convert to arpeggio (flatten to 1D sequence)\n",
    "\n",
    "class BachDataset(Dataset):\n",
    "    def __init__(self, chorales, window_size=64, window_shift=32):\n",
    "        self.chorales = chorales\n",
    "        self.window_size = window_size\n",
    "        self.window_shift = window_shift\n",
    "        self.windows = self._create_windows()\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        for chorale in self.chorales:\n",
    "            chorale_tensor = torch.tensor(chorale, dtype=torch.long)\n",
    "            \n",
    "            # Create sliding windows\n",
    "            for i in range(0, len(chorale) - self.window_size, self.window_shift):\n",
    "                window = chorale_tensor[i:i + self.window_size + 1]\n",
    "                if len(window) == self.window_size + 1:  # Ensure full window\n",
    "                    windows.append(window)\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        # Preprocess: shift note values and flatten\n",
    "        preprocessed = preprocess(window)\n",
    "        \n",
    "        # Create input/target pairs \n",
    "        X = preprocessed[:-1]\n",
    "        Y = preprocessed[1:] # predict next note in each arpeggio, at each step\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "def bach_dataloader(chorales, batch_size=32, shuffle=False, window_size=32, window_shift=16):\n",
    "    \n",
    "    dataset = BachDataset(chorales, window_size, window_shift)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1995c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_set = bach_dataloader(train_chorales, shuffle=True)\n",
    "valid_set = bach_dataloader(valid_chorales)\n",
    "test_set = bach_dataloader(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947b231",
   "metadata": {},
   "source": [
    "Now we will build our Transformer from scratch. We will make a decoder only transformer (like a gpt model) and train our Transformer on Bach chorales and compare it with the RNN approach. We'll use the same JSB Chorales dataset but leverage the Transformer's attention mechanism for better long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c359832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention mechanism from \"Attention is All You Need\" (Vaswani et al., 2017).\n",
    "    \n",
    "    Splits the input into multiple attention heads, computes scaled dot-product attention\n",
    "    in parallel for each head, then concatenates and projects the results.\n",
    "    \n",
    "    The attention mechanism allows the model to focus on different positions in the sequence\n",
    "    when processing each position.  Multiple heads enable attending to different representation\n",
    "    subspaces simultaneously. \n",
    "    \n",
    "    Args:\n",
    "        d_model: Dimensionality of the model (embedding dimension)\n",
    "        num_heads: Number of parallel attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Ensure model dimension is divisible by number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False) # the Value parameters\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False) # the Key parameters\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False) # the Query parameters\n",
    "        self.Wo = nn.Linear(d_model, d_model, bias=False) # the output parameters\n",
    "\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, attention_mask=None):  \n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention. \n",
    "        \n",
    "        Attention formula: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V\n",
    "        \n",
    "        The scaling by sqrt(d_k) prevents the dot products from growing too large,\n",
    "        which would push the softmax into regions with very small gradients.\n",
    "        \n",
    "        Args:\n",
    "            query: Query tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "            key: Key tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "            value: Value tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "            attention_mask: Optional mask to prevent attention to certain positions\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (attention_output, attention_weights) where:\n",
    "            - attention_output: Weighted sum of values\n",
    "            - attention_weights: Attention probability distribution\n",
    "        \"\"\"      \n",
    "        d_k = query.size(-1)\n",
    "        tgt_len, src_len = query.size(-2), key.size(-2)\n",
    "\n",
    "        # logits = query * key^T / sqrt(d_k)\n",
    "        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # (batch_size, num_heads, tgt_len, src_len)\n",
    "\n",
    "\n",
    "        # Apply attention mask if provided \n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                # Verify mask dimensions match sequence lengths\n",
    "                assert attention_mask.size() == (tgt_len, src_len)\n",
    "                # Add batch dimension to broadcast across all batches\n",
    "                attention_mask = attention_mask.unsqueeze(0)\n",
    "                # Add mask to logits (positions with -inf will have ~0 probability after softmax)\n",
    "                logits = logits + attention_mask\n",
    "            else:\n",
    "                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n",
    "                \n",
    "        \n",
    "        attention = torch.softmax(logits, dim=-1)\n",
    "        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, head_dim)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "    \n",
    "    def split_into_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the input tensor into multiple attention heads.\n",
    "        \n",
    "        Reshapes from (batch_size, seq_len, d_model) to \n",
    "        (batch_size, num_heads, seq_len, head_dim) where head_dim = d_model / num_heads\n",
    "        \n",
    "        This allows parallel computation of attention for each head.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            \n",
    "        Returns:\n",
    "            Reshaped tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        # Reshape to separate heads: (batch_size, seq_len, num_heads, head_dim)\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2) # (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine multiple attention heads back into a single tensor.\n",
    "        \n",
    "        Inverse operation of split_into_heads.  Reshapes from \n",
    "        (batch_size, num_heads, seq_len, head_dim) to (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Combined tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    \n",
    "    def forward(self, q, k, v, attention_mask=None):\n",
    "\n",
    "        q = self.Wq(q) # Shape: (batch_size, seq_len, d_model)\n",
    "        k = self.Wk(k)\n",
    "        v = self.Wv(v)\n",
    "\n",
    "        q = self.split_into_heads(q) # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        k = self.split_into_heads(k)\n",
    "        v = self.split_into_heads(v)\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        attention_values, attention_weights  = self.scaled_dot_product_attention(query=q, key=k, value=v, attention_mask=attention_mask)\n",
    "\n",
    "        # Combine heads back into single tensor\n",
    "        grouped = self.combine_heads(attention_values)\n",
    "\n",
    "        # Apply output projection\n",
    "        output = self.Wo(grouped)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = attention_weights\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3ed09",
   "metadata": {},
   "source": [
    "Since the Transformer has no recurrence or convolution, it has no inherent notion of token position.  Positional encodings are added to the input embeddings to inject information about the relative or absolute position of tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses sinusoidal functions of different frequencies:\n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    These allow the model to learn to attend by relative positions, as for any fixed\n",
    "    offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
    "    \n",
    "    Args:\n",
    "        d_model: Dimensionality of the model embeddings\n",
    "        dropout: Dropout probability to apply after adding positional encodings\n",
    "        max_len: Maximum sequence length to pre-compute encodings for\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) to hold positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Create position indices: [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Create division term for the sinusoidal functions creating different frequencies for different dimensions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add batch dimension: shape becomes (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "            \n",
    "        self.register_buffer('pe', pe) # This ensures it moves to GPU with the model but is not trained\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af42832",
   "metadata": {},
   "source": [
    "Position-wise Feed-Forward Network applied after attention in each transformer block. Consists of two linear transformations with a ReLU activation in between:\n",
    "$$\n",
    "FFN(x) = ReLU(x W_1 + b_1)W_2 + b_2 \n",
    "$$\n",
    "\n",
    "This is applied independently to each position (hence \"position-wise\"), allowing the model to process and transform the attended information. Standard practice is to use a hidden dimension of 4 * d_model, providing additional model capacity for learning complex transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network from \"Attention is All You Need\" (Vaswani et al., 2017).\n",
    "    \n",
    "    Args:\n",
    "        d_model: Input and output dimensionality\n",
    "        hidden_dim: Hidden layer dimensionality (default: 4 * d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, hidden_dim=None):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * d_model \n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5744af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder block for the Transformer. \n",
    "    \n",
    "    Each sub-layer (attention and feed-forward) has:\n",
    "    - Pre-LayerNorm: normalization applied before the sub-layer\n",
    "    - Residual connection: input is added to sub-layer output\n",
    "    - Dropout: applied to sub-layer output for regularization\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimensionality\n",
    "        dropout: Dropout probability for regularization\n",
    "        num_heads: Number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.self_attention1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.self_attention2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.ff = PositionWiseFeedForward(d_model)\n",
    "        \n",
    "        # Dropout layers for regularization after each sub-layer\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout) \n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        masked_att_output = self.self_attention1(q=self.norm1(tgt), k=self.norm1(tgt), v=self.norm1(tgt), attention_mask=tgt_mask)\n",
    "        x = tgt + self.dropout1(masked_att_output)\n",
    "\n",
    "        masked_att_output = self.self_attention2(q=self.norm2(x), k=self.norm2(x), v=self.norm2(x), attention_mask=tgt_mask)\n",
    "        x = x + self.dropout2(masked_att_output)\n",
    "        \n",
    "        ff_output = self.ff(self.norm3(x))\n",
    "        output = x + self.dropout3(ff_output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module consisting of multiple stacked decoder blocks.\n",
    "    \n",
    "    Processes input sequences through:\n",
    "    1. Token embedding: maps token indices to dense vectors\n",
    "    2. Positional encoding: adds position information\n",
    "    3. Stack of decoder blocks: applies self-attention and feed-forward transformations\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimensionality\n",
    "        dropout: Dropout probability\n",
    "        num_decoder_blocks: Number of decoder blocks to stack\n",
    "        num_heads: Number of attention heads per block\n",
    "        shared_embedding: Shared embedding layer (shared with output layer for weight tying)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout, num_decoder_blocks, num_heads, shared_embedding):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = shared_embedding\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "          \n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(d_model, dropout, num_heads) for _ in range(num_decoder_blocks)])\n",
    "        \n",
    "        \n",
    "    def forward(self, tgt, tgt_mask=None):\n",
    "        x = self.embedding(tgt) \n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, tgt_mask=tgt_mask)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder-only Transformer model for autoregressive sequence generation.\n",
    "    \n",
    "    This architecture is similar to GPT (Generative Pre-trained Transformer), consisting\n",
    "    of only a decoder without an encoder. It generates sequences autoregressively by\n",
    "    predicting the next token given all previous tokens.\n",
    "    \n",
    "    Key features:\n",
    "    - Token embedding with weight sharing (embedding weights are shared with output projection)\n",
    "    - Positional encoding to provide sequence position information\n",
    "    - Stack of decoder blocks with multi-head self-attention\n",
    "    - Output projection to vocabulary size\n",
    "    \n",
    "    Args:\n",
    "        n_notes: Vocabulary size (number of possible note values)\n",
    "        d_model: Model dimensionality (embedding size)\n",
    "        dropout: Dropout probability for regularization\n",
    "        n_decoder_layers: Number of stacked decoder blocks\n",
    "        n_heads: Number of attention heads per block\n",
    "        batch_size: Batch size for training (used for organization, not computation)\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Extract hyperparameters from kwargs\n",
    "        self.n_notes = kwargs.get('n_notes')\n",
    "        self.d_model = kwargs.get('d_model') \n",
    "        self.dropout = kwargs.get('dropout')\n",
    "        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n",
    "        self.n_heads = kwargs.get('n_heads')\n",
    "        self.batch_size = kwargs.get('batch_size')\n",
    "\n",
    "        # Embedding layer maps token indices to dense vectors\n",
    "        self.shared_embedding = nn.Embedding(self.n_notes, self.d_model)\n",
    "\n",
    "        # Decoder processes the embedded and positionally encoded sequence\n",
    "        self.decoder = Decoder(self.d_model, self.dropout, self.n_decoder_layers, self.n_heads, self.shared_embedding)\n",
    "\n",
    "        # Output projection maps decoder output back to vocabulary size\n",
    "        self.fc = nn.Linear(self.d_model, self.n_notes)\n",
    "\n",
    "        # From the paper \"Using the Output Embedding to Improve Language Models\" (Press & Wolf, 2017)\n",
    "        self.fc.weight = self.shared_embedding.weight \n",
    "        \n",
    "\n",
    "    @staticmethod    \n",
    "    def generate_square_subsequent_mask(size, device=None):\n",
    "        \"\"\"\n",
    "        Generate a causal mask for autoregressive training.\n",
    "        \n",
    "        Creates a lower-triangular matrix where:\n",
    "        - mask[i, j] = 0 if j <= i (can attend to current and past positions)\n",
    "        - mask[i, j] = -inf if j > i (cannot attend to future positions)\n",
    "        \n",
    "        This prevents the model from \"cheating\" during training by looking at future tokens.\n",
    "        \n",
    "        Args:\n",
    "            size: Sequence length\n",
    "            device: Device to create mask on (CPU or GPU)\n",
    "            \n",
    "        Returns:\n",
    "            Causal mask of shape (size, size)\n",
    "        \"\"\"\n",
    "        mask = (1 - torch.triu(torch.ones(size, size, device=device), diagonal=1)).bool()\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "        \n",
    "    def forward(self, x ) -> torch.Tensor:\n",
    "        # Generate causal mask to prevent attending to future positions\n",
    "        tgt_mask = self.generate_square_subsequent_mask(x.size(1), device=x.device)\n",
    "\n",
    "        decoder_output = self.decoder(tgt=x, tgt_mask=tgt_mask)   # Shape: (batch_size, seq_len, d_model)\n",
    "        output = self.fc(decoder_output) # Shape: (batch_size, seq_len, n_notes)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x) -> torch.Tensor:\n",
    "        # Pass through decoder without mask (all positions can attend to all positions)\n",
    "        decoder_output = self.decoder(tgt=x, tgt_mask=None)  \n",
    "        output = self.fc(decoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9933fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Performs a complete pass through the training dataset, computing loss and gradients\n",
    "    for each batch and updating model parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model to train\n",
    "        train_loader: DataLoader providing batches of training data\n",
    "        optimizer: Optimizer for updating model parameters\n",
    "        lr_scheduler: Optional learning rate scheduler\n",
    "        criterion: Loss function (CrossEntropyLoss for classification)\n",
    "        device: Device to run computations on (CPU or GPU)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (average_loss, accuracy) for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(train_loader): \n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "       \n",
    "       # Clear gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass: compute model predictions\n",
    "        outputs = model(src)\n",
    "        \n",
    "        # Reshape for cross entropy loss\n",
    "        outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "        tgt_flat = tgt.reshape(-1)\n",
    "        \n",
    "        # Compute cross-entropy loss between predictions and targets\n",
    "        loss = criterion(outputs_flat, tgt_flat)\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()        \n",
    " \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs_flat.max(dim=1)\n",
    "        total += tgt_flat.size(0)\n",
    "        correct += predicted.eq(tgt_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, valid_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data.\n",
    "    \n",
    "    Performs inference on the validation set without updating model parameters,\n",
    "    used to monitor generalization during training.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model to evaluate\n",
    "        valid_loader: DataLoader providing batches of validation data\n",
    "        criterion: Loss function for computing validation loss\n",
    "        device: Device to run computations on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (average_loss, accuracy) on validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(valid_loader):  \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "            # Forward pass only (no gradient computation)\n",
    "            outputs = model(src)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs_flat = outputs.reshape(-1, outputs. size(-1))\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs_flat, tgt_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += tgt_flat.size(0)\n",
    "            correct += predicted.eq(tgt_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(valid_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop \n",
    "def train_model(model, train_loader, valid_loader, optimizer, lr_scheduler, criterion, epochs=15, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model for multiple epochs with validation and loss visualization.\n",
    "    \n",
    "    Performs the complete training loop: training for specified number of epochs,\n",
    "    validating after each epoch, and plotting the learning curves.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        valid_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer for parameter updates\n",
    "        lr_scheduler: Optional learning rate scheduler\n",
    "        criterion: Loss function\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "    \"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, lr_scheduler, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "        valid_losses.append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), valid_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "config = {\n",
    "    'n_notes': 47,\n",
    "    'd_model': 128, \n",
    "    'dropout': 0,\n",
    "    'n_decoder_layers': 4,\n",
    "    'n_heads': 8,\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(**config)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "   \n",
    "# Run training\n",
    "train_model(model=model, train_loader=train_set, valid_loader=valid_set,  optimizer=optimizer, lr_scheduler=None, criterion=criterion, epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73020c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test set.\n",
    "    \n",
    "    Computes final metrics on held-out test data to assess model performance\n",
    "    on unseen examples.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to run evaluation on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (test_loss, test_accuracy)\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(test_loader): \n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(src)\n",
    "            \n",
    "            # Reshape for cross entropy loss\n",
    "            outputs_flat = outputs.reshape(-1, outputs.size(-1))\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs_flat, tgt_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += tgt_flat.size(0)\n",
    "            correct += predicted.eq(tgt_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "        print(f'Test Results:')\n",
    "        print(f'Test Loss: {avg_loss:.4f}')\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "    \n",
    "# Evaluate the model on test set\n",
    "test_loss, test_acc = evaluate_model(model, test_set, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd034d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length=32, context_window=128):\n",
    "    \"\"\"\n",
    "    Generate a Bach chorale continuation using the trained model.\n",
    "    \n",
    "    Performs autoregressive generation: predicts one note at a time, appending each\n",
    "    prediction to the input sequence and using it to predict the next note.\n",
    "    \n",
    "    Uses a context window to limit the sequence length seen by the model during generation.\n",
    "    This prevents the model from seeing positions beyond its training range and maintains\n",
    "    stable generation quality.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Transformer model\n",
    "        seed_chords: Initial chords to condition generation on (list of 4-note chords)\n",
    "        length: Number of chords to generate (default: 32)\n",
    "        context_window: Maximum sequence length to feed to model (default: 128 tokens)\n",
    "                       Should match or be less than training window size\n",
    "        \n",
    "    Returns:\n",
    "        Generated chorale as numpy array of shape (num_chords, 4)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Convert seed chords to tensor and preprocess\n",
    "        seed_tensor = torch.tensor(seed_chords, dtype=torch.long)\n",
    "        arpeggio = preprocess(seed_tensor)\n",
    "        arpeggio = arpeggio.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        \n",
    "        # Generate new notes\n",
    "        for chord in range(length):\n",
    "            for note in range(4):\n",
    "                context = arpeggio[:, -context_window:]\n",
    "            \n",
    "                # Get model prediction for the current sequence\n",
    "                outputs = model.predict(context)  # Shape: (1, seq_len, n_notes)\n",
    "                \n",
    "                \n",
    "                # Get the prediction for the last timestep\n",
    "                last_output = outputs[0, -1, :]  # Shape: (n_notes,)\n",
    "                \n",
    "                # Get the most likely next note\n",
    "                next_note = torch.argmax(last_output, dim=-1, keepdim=True)  # Shape: (1,)\n",
    "                \n",
    "                # Append the predicted note to the sequence\n",
    "                arpeggio = torch.cat([arpeggio, next_note.unsqueeze(0)], dim=1)\n",
    "\n",
    "        # Convert back to original note range (reverse the preprocessing)\n",
    "        arpeggio = torch.where(arpeggio == 0, arpeggio, arpeggio + min_note - 1)\n",
    "        \n",
    "        # Reshape to chord format (group every 4 notes)\n",
    "        arpeggio_flat = arpeggio.squeeze(0)  # Remove batch dimension\n",
    "        n_total_notes = len(arpeggio_flat)\n",
    "        n_complete_chords = n_total_notes // 4\n",
    "        \n",
    "        # Take only complete chords and reshape\n",
    "        chorale = arpeggio_flat[:n_complete_chords * 4].reshape(-1, 4)\n",
    "        \n",
    "        return chorale.cpu().numpy()  # Convert back to numpy for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afddb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed from the test set\n",
    "seed_chords = test_chorales[2][:12]\n",
    "baroque_synth.play_chorale(seed_chords)   \n",
    "print(\"Seed Chorale:\")\n",
    "print(seed_chords)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new chorale continuation\n",
    "new_chorale = generate_chorale(model, seed_chords, length=32)\n",
    "baroque_synth.play_chorale(new_chorale)\n",
    "print(\"Generated Chorale:\")\n",
    "print(new_chorale[4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ce91d",
   "metadata": {},
   "source": [
    "## Analyzing Transformer vs RNN Performance\n",
    "\n",
    "### Key Differences to Highlight:\n",
    "\n",
    "1. **Attention Mechanism**: \n",
    "   - Transformer can attend to any position in the sequence simultaneously\n",
    "   - RNN processes sequentially, potentially losing long-term dependencies\n",
    "\n",
    "2. **Parallelization**: \n",
    "   - Transformer training can be parallelized (all positions at once)\n",
    "   - RNN training is inherently sequential\n",
    "\n",
    "3. **Musical Structure**:\n",
    "   - Transformer might better capture harmonic relationships across time\n",
    "   - Can potentially learn chord progressions and voice leading patterns\n",
    "\n",
    "4. **Generation Quality**:\n",
    "   - Compare coherence of generated chorales\n",
    "   - Look at harmonic consistency and voice independence\n",
    "\n",
    "### Exercises for Students:\n",
    "\n",
    "1. **Experiment with attention heads**: Try different numbers of attention heads and see how it affects generation quality\n",
    "\n",
    "2. **Temperature sampling**: Adjust the temperature parameter to control randomness vs structure\n",
    "\n",
    "3. **Seed analysis**: Try different seed sequences and observe how the model continues them\n",
    "\n",
    "4. **Attention visualization**: Plot attention weights to see what the model focuses on (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcddd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
