{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19298d61",
   "metadata": {},
   "source": [
    "# Lesson 10: Variational Auto Encoders\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Optimization techniques\n",
    "2. Regularization techniques\n",
    "3. Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"resources/_datasets/AudioMNIST/data\"  # Change this to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMNISTSpectrumExtractor:\n",
    "    def __init__(self, n_fft=510, hop_length=256, target_length=None):\n",
    "        self.n_fft = n_fft  # n_fft=510 gives 256 frequency bins (n_fft//2 + 1)\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = n_fft\n",
    "        self.target_length = target_length  \n",
    "        self.sr = None\n",
    "        self.spectrogram_transform = T.Spectrogram(n_fft=self.n_fft, win_length=self.win_length, hop_length=self.hop_length, power=2.0)\n",
    "        \n",
    "        \n",
    "    def get_spectrogram(self, audio_path):\n",
    "        # Load audio with librosa (works reliably on macOS without backend issues)\n",
    "        y, sr = librosa.load(audio_path)\n",
    "        self.sr = sr  # Store sample rate\n",
    "        # Convert to torch tensor and add channel dimension\n",
    "        waveform = torch.from_numpy(y).unsqueeze(0).float()\n",
    "        waveform = self.pad_or_crop_audio(waveform)\n",
    "        # Apply spectrogram transform\n",
    "        spectrogram = self.spectrogram_transform(waveform)\n",
    "        # Normalize spectrogram\n",
    "        spectrogram = torch.div(spectrogram, spectrogram.max())\n",
    "        return spectrogram\n",
    "    \n",
    "    def pad_or_crop_audio(self, waveform):\n",
    "        current_length = waveform.shape[-1]\n",
    "        if current_length < self.target_length:\n",
    "            # Pad with zeros\n",
    "            padding = self.target_length - current_length\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        elif current_length > self.target_length:\n",
    "            # Crop the waveform - need to index the time dimension correctly\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "\n",
    "        return waveform\n",
    "        \n",
    "\n",
    "def load_audiomnist_dataset(data_dir, max_samples_per_speaker=None):\n",
    "    \"\"\"\n",
    "    Load AudioMNIST dataset and extract features.\n",
    "    \n",
    "    Expected directory structure:\n",
    "    data_dir/\n",
    "        01/  (speaker 1)\n",
    "            0_01_0.wav\n",
    "            0_01_1.wav\n",
    "            ...\n",
    "            9_01_49.wav\n",
    "        02/  (speaker 2)\n",
    "            0_02_0.wav\n",
    "            ...\n",
    "        ...\n",
    "        60/  (speaker 60)\n",
    "    \n",
    "    Returns:\n",
    "        features: numpy array of shape (n_samples, n_features)\n",
    "        labels: numpy array of shape (n_samples,) - digit labels\n",
    "        speakers: numpy array of shape (n_samples,) - speaker IDs\n",
    "    \"\"\"\n",
    "    extractor = AudioMNISTSpectrumExtractor(target_length=16128)  # 16128 samples = 64 frames with hop_length=256 (since (16128-510)/256 + 1 ≈ 64)\n",
    "\n",
    "    \n",
    "    spectrogram_list = []\n",
    "    labels_list = []\n",
    "    speakers_list = []\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    print(\"Loading AudioMNIST dataset...\")\n",
    "    \n",
    "    # Iterate through all speaker directories (01-60)\n",
    "    for speaker_id in range(1, 6):  # AudioMNIST has 60 speakers\n",
    "        speaker_dir = data_path / str(speaker_id).zfill(2)\n",
    "        \n",
    "        if not speaker_dir.exists():\n",
    "            continue  # Skip missing speaker directories\n",
    "        \n",
    "        audio_files = sorted(speaker_dir.glob(\"*.wav\"))\n",
    "        \n",
    "        if max_samples_per_speaker:\n",
    "            audio_files = audio_files[:max_samples_per_speaker]\n",
    "        \n",
    "        samples_loaded = 0\n",
    "        for audio_file in audio_files:\n",
    "            try:\n",
    "                # Extract features\n",
    "                spectrogram = extractor.get_spectrogram(audio_file)\n",
    "                \n",
    "                # Parse filename to get digit (format: digit_speaker_repetition.wav)\n",
    "                parts = audio_file.stem.split('_')\n",
    "                digit = int(parts[0])\n",
    "\n",
    "                # Convert tensor to numpy before appending\n",
    "                spectrogram_list.append(spectrogram.numpy())\n",
    "                labels_list.append(digit)\n",
    "                speakers_list.append(speaker_id)\n",
    "                samples_loaded += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {audio_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if samples_loaded > 0:\n",
    "            print(f\"Speaker {speaker_id:02d}: {samples_loaded} samples loaded\")\n",
    "    \n",
    "    spectrogram = np.array(spectrogram_list)\n",
    "    labels = np.array(labels_list)\n",
    "    speakers = np.array(speakers_list)\n",
    "    \n",
    "    print(f\"\\nTotal samples: {len(spectrogram)}\")\n",
    "    print(f\"Feature shape: {spectrogram[0].shape}\")  # Now this will show (num_samples, 1, 257, 141)\n",
    "    print(f\"Unique digits: {sorted(np.unique(labels))}\")\n",
    "    print(f\"Unique speakers: {len(np.unique(speakers))}\")\n",
    "    \n",
    "    return spectrogram, labels, speakers\n",
    "\n",
    "X, y, _ = load_audiomnist_dataset(DATA_DIR, max_samples_per_speaker=None)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)   \n",
    "y = np.array(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec03055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create our data set with torch.utils.data.Dataset\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Channel dimension already exists from spectrogram\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "dataset = WaveformDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some spectrograms\n",
    "\n",
    "# Get a batch of spectrograms\n",
    "spectrograms, labels = next(iter(train_loader))\n",
    "spectrograms_to_show = spectrograms[:16]\n",
    "print(\"Shape of the spectrogram tensor:\", spectrograms_to_show.shape)\n",
    "\n",
    "# Create subplots to show individual spectrograms\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15, 12))\n",
    "fig.suptitle('Audio Spectrograms (Power)', fontsize=16)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < len(spectrograms_to_show):\n",
    "        # Get single spectrogram and squeeze channel dimension\n",
    "        spec = spectrograms_to_show[idx].squeeze().numpy()\n",
    "        \n",
    "        # Convert to dB scale for better visualization\n",
    "        spec_db = 10 * np.log10(spec + 1e-10)\n",
    "        \n",
    "        # Plot spectrogram\n",
    "        im = ax.imshow(spec_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "        ax.set_title(f'Sample {idx + 1}')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Frequency Bin')\n",
    "        \n",
    "        # Add colorbar for each subplot\n",
    "        plt.colorbar(im, ax=ax, label='Power (dB)')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01874381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "def spectrogram_to_audio(spectrogram, n_fft=510, hop_length=256, sr=22050):\n",
    "    \"\"\"\n",
    "    Convert a spectrogram back to audio using Griffin-Lim algorithm.\n",
    "    \n",
    "    Args:\n",
    "        spectrogram: tensor of shape (1, freq_bins, time_frames) or (freq_bins, time_frames)\n",
    "        n_fft: FFT size\n",
    "        hop_length: hop length\n",
    "        sr: sample rate\n",
    "    \n",
    "    Returns:\n",
    "        audio waveform as numpy array\n",
    "    \"\"\"\n",
    "    # Ensure spectrogram is on CPU and has correct shape\n",
    "    if len(spectrogram.shape) == 3 and spectrogram.shape[0] == 1:\n",
    "        spectrogram = spectrogram.squeeze(0)  # Remove batch/channel dim\n",
    "    \n",
    "    spectrogram = spectrogram.cpu()\n",
    "    \n",
    "    # Create Griffin-Lim transform\n",
    "    griffin_lim = T.GriffinLim(\n",
    "        n_fft=n_fft,\n",
    "        win_length=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_iter=32\n",
    "    )\n",
    "    \n",
    "    # Reconstruct audio\n",
    "    audio = griffin_lim(spectrogram)\n",
    "    \n",
    "    return audio.numpy()\n",
    "\n",
    "def play_audio_from_spectrogram(spectrogram, title=\"Audio\", sr=22050):\n",
    "    \"\"\"Convert spectrogram to audio and display player.\"\"\"\n",
    "    audio = spectrogram_to_audio(spectrogram, sr=sr)\n",
    "    print(f\"\\n{title}\")\n",
    "    display(Audio(audio, rate=sr))\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to original training data\n",
    "print(\"=== ORIGINAL TRAINING SAMPLES ===\")\n",
    "spectrograms, labels = next(iter(train_loader))\n",
    "\n",
    "# Play 3 original samples\n",
    "for i in range(3):\n",
    "    spec = spectrograms[i]\n",
    "    label = labels[i].item()\n",
    "    play_audio_from_spectrogram(spec, title=f\"Original Sample {i+1} - Digit: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45fb51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        h_channels = [32, 64]  # Number of filters in each conv layer\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "        self.net.append(nn.Conv2d(1, h_channels[0], kernel_size=3, stride=2, padding=1))  # 1 input channel -> 32 feature maps\n",
    "        self.net.append(nn.LeakyReLU())\n",
    "        self.net.append(nn.Conv2d(h_channels[0], h_channels[1], kernel_size=3, stride=2, padding=1))  # 32 -> 64 feature maps\n",
    "        self.net.append(nn.LeakyReLU())\n",
    "        self.net.append(nn.Flatten())\n",
    "        self.net.append(nn.Linear(h_channels[1]*64*16, latent_dim * 2))  # 64 channels × 64 height × 16 width = 65536\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        h_channels = [32, 64]  # Number of filters in each conv layer\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "        self.net.append(nn.Linear(latent_dim, h_channels[1]*64*16))\n",
    "        self.net.append(nn.Unflatten(1, (h_channels[1], 64, 16)))  # Reshape to (64 channels, 64 height, 16 width)\n",
    "        self.net.append(nn.LeakyReLU())\n",
    "        self.net.append(nn.ConvTranspose2d(h_channels[1], h_channels[0], kernel_size=3, stride=2, padding=1, output_padding=1))  # 64 -> 32 feature maps\n",
    "        self.net.append(nn.LeakyReLU())\n",
    "        self.net.append(nn.ConvTranspose2d(h_channels[0], 1, kernel_size=3, stride=2, padding=1, output_padding=1))  # 32 -> 1 output channel\n",
    "        self.net.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean_logvar = self.encoder(x)\n",
    "        mean = mean_logvar[:, :self.latent_dim]\n",
    "        logvar = mean_logvar[:, self.latent_dim:]\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        return x_reconstructed, mean, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_reconstructed, mean, logvar, beta):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = nn.functional.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    # nn.functional.elbo_loss\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + beta * kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model overview\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available else \"cpu\")\n",
    "latent_dim = 10\n",
    "model = VariationalAutoencoder(latent_dim)\n",
    "batch_size = 128\n",
    "\n",
    "# Output some shapes\n",
    "spectrograms, labels = next(iter(train_loader))\n",
    "print(\"Shape of the input tensor:\", spectrograms.shape)\n",
    "mean, logvar = model.encode(spectrograms)\n",
    "print(\"Shape of the mean tensor:\", mean.shape)\n",
    "print(\"Shape of the logvar tensor:\", logvar.shape)\n",
    "z = model.reparameterize(mean, logvar)\n",
    "print(\"Shape of the latent tensor:\", z.shape)\n",
    "x_reconstructed = model.decode(z)\n",
    "print(\"Shape of the output tensor:\", x_reconstructed.shape)\n",
    "\n",
    "# The model directly outputs the reconstructed spectrogram, the mean and the logvar\n",
    "x_reconstructed, mean, logvar = model(spectrograms)\n",
    "print(\"Shape of the output tensor:\", x_reconstructed.shape)\n",
    "print(\"Shape of the mean tensor:\", mean.shape)\n",
    "print(\"Shape of the logvar tensor:\", logvar.shape)\n",
    "\n",
    "# Print model summary with correct spectrogram dimensions (1 channel, 256 frequency bins, 64 time frames)\n",
    "torchinfo.summary(model, (batch_size, 1, 256, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf050483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, optimizer, criterion, beta):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, _ = data\n",
    "    x = x.to(device)\n",
    "    x_hat, mean, logvar = model(x)\n",
    "    loss = criterion(x, x_hat, mean, logvar, beta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test_step(model, data, criterion, beta):\n",
    "    model.eval()\n",
    "    x, _ = data\n",
    "    x = x.to(device)\n",
    "    x_hat, mean, logvar = model(x)\n",
    "    loss = criterion(x, x_hat, mean, logvar, beta)\n",
    "    return loss\n",
    "\n",
    "def generate_images(model, num_images):\n",
    "    model.eval()\n",
    "    random_input = torch.randn(num_images, latent_dim).to(device)\n",
    "    generated_images = model.decoder(random_input)\n",
    "    return generated_images\n",
    "\n",
    "def plot_images(images, labels):\n",
    "    # Show spectrograms in a grid (4x8 = 32 spectrograms)\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(20, 10))\n",
    "    fig.suptitle('Generated Spectrograms', fontsize=16)\n",
    "    \n",
    "    images_cpu = images.detach().cpu().numpy()\n",
    "    if labels is not None:\n",
    "        labels_cpu = labels.detach().cpu().numpy()\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(images_cpu):\n",
    "            # Get single spectrogram and squeeze channel dimension\n",
    "            spec = images_cpu[idx].squeeze()\n",
    "            \n",
    "            # Convert to dB scale for better visualization\n",
    "            spec_db = 10 * np.log10(spec + 1e-10)\n",
    "            \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(images_cpu):\n",
    "            # Get single spectrogram and squeeze channel dimension\n",
    "            spec = images_cpu[idx].squeeze()\n",
    "            \n",
    "            # Convert to dB scale for better visualization\n",
    "            spec_db = 10 * np.log10(spec + 1e-10)\n",
    "            \n",
    "            # Plot spectrogram\n",
    "            ax.imshow(spec_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "            \n",
    "            # Add label as title if provided\n",
    "            if labels is not None and idx < len(labels_cpu):\n",
    "                ax.set_title(f'Digit: {labels_cpu[idx]}', fontsize=10, color='black')\n",
    "            \n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    return\n",
    "\n",
    "def trainig_loop(model, train_loader, test_loader, optimizer, criterion, num_epochs, beta):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loader_tqdm = tqdm(train_loader,\n",
    "                                total=(len(train_loader)),\n",
    "                                desc=f\"Epoch {epoch} (training)\",\n",
    "                                bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\")\n",
    "\n",
    "        train_loss_epoch = 0\n",
    "        test_loss_epoch = 0\n",
    "\n",
    "        for x in train_loader_tqdm:\n",
    "            train_loss = train_step(model, x, optimizer, criterion, beta)\n",
    "            train_loss_epoch += train_loss\n",
    "            \n",
    "        for x in test_loader:\n",
    "            test_loss = test_step(model, x, criterion, beta)\n",
    "            test_loss_epoch += test_loss\n",
    "\n",
    "        train_loader_tqdm.write(f\"Epoch {epoch}, Average train Loss: {train_loss_epoch/len(train_loader)}, Average test Loss: {test_loss_epoch/len(test_loader)}\")\n",
    "        # Inside trainig_loop, change the last two lines to:\n",
    "        if epoch % 10 == 0:\n",
    "            x, y = next(iter(train_loader))\n",
    "            generated_images = generate_images(model, len(y))\n",
    "            plot_images(generated_images, y)\n",
    "    train_loader_tqdm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "latent_dim = 128\n",
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "beta = 0.5\n",
    "\n",
    "hidden_channels = [32, 64]  # Number of convolutional filters in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f54a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = VariationalAutoencoder(latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = loss_function\n",
    "\n",
    "trainig_loop(model, train_loader, test_loader, optimizer, criterion, epochs, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae5dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to reconstructed audio (VAE output)\n",
    "print(\"\\n=== RECONSTRUCTED SAMPLES (VAE) ===\")\n",
    "model.eval()\n",
    "spectrograms, labels = next(iter(test_loader))\n",
    "spectrograms_device = spectrograms[:3].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, _, _ = model(spectrograms_device)\n",
    "\n",
    "# Play 3 reconstructed samples\n",
    "for i in range(3):\n",
    "    original_spec = spectrograms[i]\n",
    "    reconstructed_spec = reconstructed[i]\n",
    "    label = labels[i].item()\n",
    "    \n",
    "    print(f\"\\n--- Sample {i+1} - Digit: {label} ---\")\n",
    "    play_audio_from_spectrogram(original_spec, title=\"  Original\")\n",
    "    play_audio_from_spectrogram(reconstructed_spec, title=\"  Reconstructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ddb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the model\n",
    "torch.save(model.state_dict(), \"variational_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b24ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = VariationalAutoencoder(latent_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"variational_autoencoder.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get the latent representation of the test data\n",
    "latent_representations = []\n",
    "latent_representations_labels = []\n",
    "\n",
    "test_loader_tqdm = tqdm(test_loader,\n",
    "                        total=(len(test_loader)),\n",
    "                        desc=f\"Getting latent representations\",\n",
    "                        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt}\")\n",
    "\n",
    "for x, y in test_loader_tqdm:\n",
    "    x = x.to(device)\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    latent_representations.append(z.detach().cpu().numpy())\n",
    "    latent_representations_labels.append(y.numpy())\n",
    "\n",
    "latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "latent_representations_labels = np.concatenate(latent_representations_labels, axis=0)\n",
    "print(\"Shape of latent representations:\", latent_representations.shape)\n",
    "print(\"Shape of latent representations labels:\", latent_representations_labels.shape)\n",
    "print(\"Maximum value in latent representations:\", np.max(latent_representations))\n",
    "print(\"Minimum value in latent representations:\", np.min(latent_representations))\n",
    "\n",
    "# Visualize the latent representations\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=42)\n",
    "latent_representations_tsne = tsne.fit_transform(latent_representations)\n",
    "print(\"Shape of latent representations tsne:\", latent_representations_tsne.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    indices = latent_representations_labels == i\n",
    "    plt.scatter(latent_representations_tsne[indices, 0], latent_representations_tsne[indices, 1], label=str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct images\n",
    "\n",
    "# Get a batch of spectrograms\n",
    "spectrograms, labels = next(iter(train_loader))\n",
    "spectrograms_to_show = spectrograms[:16]\n",
    "print(\"Shape of the spectrogram tensor:\", spectrograms_to_show.shape)\n",
    "\n",
    "spectrograms_to_show = spectrograms_to_show.to(device)\n",
    "x_reconstructed, _, _ = model(spectrograms_to_show)\n",
    "print(\"Shape of the reconstructed spectrogram tensor:\", x_reconstructed.shape)\n",
    "\n",
    "# Create subplots to show original and reconstructed spectrograms side by side\n",
    "# 2 rows (original, reconstructed) x 8 columns\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
    "fig.suptitle('Original vs Reconstructed Spectrograms', fontsize=16)\n",
    "\n",
    "num_samples = min(8, len(spectrograms_to_show))\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    # Get single spectrogram and squeeze channel dimension\n",
    "    spec = spectrograms_to_show[idx].squeeze().detach().cpu().numpy()\n",
    "    spec_reconstructed = x_reconstructed[idx].squeeze().detach().cpu().numpy()\n",
    "    \n",
    "    # Convert to dB scale for better visualization\n",
    "    spec_db = 10 * np.log10(spec + 1e-10)\n",
    "    spec_db_reconstructed = 10 * np.log10(spec_reconstructed + 1e-10)\n",
    "    \n",
    "    # Plot original spectrogram (top row)\n",
    "    im1 = axes[0, idx].imshow(spec_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[0, idx].set_title(f'Original {idx + 1}\\nDigit: {labels[idx].item()}', fontsize=9)\n",
    "    axes[0, idx].set_ylabel('Freq Bin', fontsize=8)\n",
    "    axes[0, idx].tick_params(labelsize=7)\n",
    "    \n",
    "    # Plot reconstructed spectrogram (bottom row)\n",
    "    im2 = axes[1, idx].imshow(spec_db_reconstructed, aspect='auto', origin='lower', cmap='viridis')\n",
    "    axes[1, idx].set_title(f'Reconstructed {idx + 1}', fontsize=9)\n",
    "    axes[1, idx].set_xlabel('Time', fontsize=8)\n",
    "    axes[1, idx].set_ylabel('Freq Bin', fontsize=8)\n",
    "    axes[1, idx].tick_params(labelsize=7)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(num_samples, 8):\n",
    "    axes[0, idx].axis('off')\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96709e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images\n",
    "\n",
    "generated_spec = generate_images(model, 16)\n",
    "plot_images(generated_spec, labels=None) \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        spec = generated_spec[i]\n",
    "        play_audio_from_spectrogram(spec, title=\"Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc51449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pca of the latent representations\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "latent_representations_pca = pca.fit_transform(latent_representations)\n",
    "print(\"Shape of latent representations pca:\", latent_representations_pca.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "print(\"Maximum value in latent representations pca:\", np.max(latent_representations_pca))\n",
    "print(\"Minimum value in latent representations pca:\", np.min(latent_representations_pca))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    indices = latent_representations_labels == i\n",
    "    plt.scatter(latent_representations_pca[indices, 0], latent_representations_pca[indices, 1], label=str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ae649",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transformed = pca.inverse_transform([[0, 0], [0, 0]])\n",
    "print(\"Inverse transformed:\", inverse_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use inverse transform to create maps\n",
    "\n",
    "n_x = 20\n",
    "digit_size = [256, 64]\n",
    "figure_size = 12\n",
    "scale = 3.\n",
    "\n",
    "figure = np.zeros((digit_size[0]*n_x, digit_size[1]*n_x))\n",
    "grid_x = np.linspace(-scale, scale, n_x)\n",
    "grid_y = np.linspace(scale, -scale, n_x)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        inverse_transformed = pca.inverse_transform([[-xi, -yi]])\n",
    "        z_sample = torch.tensor(inverse_transformed, dtype=torch.float32).to(device)\n",
    "        x_decoded = model.decoder(z_sample)\n",
    "        digit = x_decoded[0].reshape(256, 64).cpu().detach().numpy()\n",
    "        figure[i*digit_size[0]:(i+1)*digit_size[0],\n",
    "               j*digit_size[1]:(j+1)*digit_size[1]] = digit\n",
    "\n",
    "plt.figure(figsize=(figure_size, figure_size))\n",
    "start_range = digit_size[0] // 2\n",
    "end_range = n_x * digit_size[0] + start_range\n",
    "pixel_range = np.arange(start_range, end_range, digit_size[0])\n",
    "sample_range_x = np.round(grid_x, 1)\n",
    "sample_range_y = np.round(grid_y, 1)\n",
    "plt.xticks(pixel_range, sample_range_x)\n",
    "plt.yticks(pixel_range, sample_range_y)\n",
    "plt.xlabel(\"pca_0\")\n",
    "plt.ylabel(\"pca_1\")\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35af204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
