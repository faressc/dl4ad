{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lesson 2: Perceptrons\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover\n",
    "1. A quick introduction on the principles of neural networks\n",
    "2. An implementation for a single neuron in JAX\n",
    "3. An implementation on multi-layer perceptron (MLP) through manual derivation\n",
    "4. An implementation on MLP with JAX\n",
    "4. An introduction on using Pytorch for a multi-layer perceptron\n",
    "5. An exercise on multi-class classification using an MLP with JAX and Pytorch.\n",
    "\n",
    "This script was inspired by the Creative Machine Learning course of Philippe Esling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Single Neuron\n",
    "\n",
    "For the first parts of the tutorial, we will perform the simplest classification model possible in a neural network setting: a single neuron. Understanding how a single neuron works is fundamental to grasping how entire neural networks operate.\n",
    "\n",
    "### The Mathematical Model\n",
    "\n",
    "We can model this behavior mathematically. An artificial neuron receives multiple inputs $x_1, x_2, ..., x_N$, each representing a feature or signal. The neuron computes a weighted sum of these inputs and applies an activation function to produce an output:\n",
    "\n",
    "$$\n",
    "y=\\phi(\\sum_{i = 1}^{N}w_{i} \\cdot x_{i} + b)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Let's break down each component:\n",
    "\n",
    "- **Inputs ($x_i$)**: These are the features or signals coming into the neuron\n",
    "- **Weights ($w_i$)**: Each input has an associated weight that determines its importance. Positive weights strengthen the signal, while negative weights inhibit it\n",
    "- **Bias ($b$)**: This shifts the activation threshold, allowing the neuron to fire even when inputs are small, or requiring stronger inputs to fire\n",
    "- **Activation function ($\\phi$)**: This determines whether the neuron \"fires\" based on the weighted sum\n",
    "\n",
    "For a simple perceptron, we can use different activation function. It is important that when we stack layers our activation function is differentiable. \n",
    "\n",
    "### Building Blocks of Networks\n",
    "\n",
    "As we will see later on, a neural network is simply composed of layers of these neurons connected together. Each layer performs a transformation on its inputs, and by stacking multiple layers, we can learn increasingly complex patterns and representations. However, everything starts with understanding this single computational unit.\n",
    "\n",
    "Now let's implement a single neuron ourselves. We will do so using the binary cross-entropy loss function and gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, random, jit\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data set\n",
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 2\n",
    "\n",
    "# Input\n",
    "x_class_0_0 = np.linspace(0, 1.1, n_obs)\n",
    "x_class_1_0 = np.linspace(0.5, 2, n_obs)\n",
    "\n",
    "# Output \n",
    "x_class_0_1 = -3 * x_class_0_0**3 + 13 + epsilon\n",
    "x_class_1_1 = 2 * x_class_1_0**3 + 2 + epsilon\n",
    "\n",
    "X = np.vstack([\n",
    "    np.stack([x_class_0_0, x_class_0_1], axis=1),\n",
    "    np.stack([x_class_1_0, x_class_1_1], axis=1)\n",
    "])\n",
    "Y = np.concatenate([\n",
    "    np.zeros_like(x_class_0_0),\n",
    "    np.ones_like(x_class_1_0)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "\n",
    "### Binary Cross-Entropy Loss\n",
    "\n",
    "For binary classification, we will use **Binary Cross-Entropy (BCE)** loss, also known as log loss. The Loss function is calculated as follows:\n",
    "\n",
    "For a single sample:\n",
    "$$\n",
    "\\ell(y_i, \\hat{y_i}) = -[y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})]\n",
    "$$\n",
    "\n",
    "For a batch of $N$ samples:\n",
    "$$\n",
    "\\mathcal{L}(Y, \\hat{Y}) = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y \\in \\{0, 1\\}$ is the true label\n",
    "- $\\hat{y} \\in (0, 1)$ is the predicted probability (output of sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The funtion for the predictions\n",
    "def model(X, w, b):\n",
    "    z = b + w[0]*X[:,0] + w[1]*X[:,1]\n",
    "    return jax.nn.sigmoid(z)\n",
    "\n",
    "# Binary cross-entropy loss function  \n",
    "def binary_cross_entropy_loss(Y, Y_pred):\n",
    "    loss = -jnp.mean(Y*jnp.log(Y_pred)+(1-Y)*jnp.log(1 - Y_pred))\n",
    "    return loss\n",
    "\n",
    "def loss_with_params(X, Y, w, b):\n",
    "    Y_pred = model(X, w, b)\n",
    "    return binary_cross_entropy_loss(Y, Y_pred)\n",
    "\n",
    "@jit\n",
    "def gradient_descent(X, Y, w, b, lr):\n",
    "    grad_w, grad_b = grad(loss_with_params, argnums=[2, 3])(X, Y, w, b)\n",
    "    w = w - lr * grad_w\n",
    "    b = b - lr * grad_b\n",
    "    loss = loss_with_params(X, Y, w, b)\n",
    "    return w, b, loss\n",
    "\n",
    "# Optimization with gradient descent using jax's grad\n",
    "def train(key, X, Y, lr=0.05, n_iter=1000):\n",
    "    k_0, k_1 = random.split(key, 2)\n",
    "    w = random.normal(k_0, (2,))   # [w_0, w_1]\n",
    "    b = jnp.zeros(1)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        w, b, loss = gradient_descent(X, Y, w, b, lr)\n",
    "        if i % 50 == 0:  # nicht bei jedem Schritt speichern (leichter)\n",
    "            loss_history.append(float(loss))\n",
    "\n",
    "    return w, b, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 10000\n",
    "lr = 0.03\n",
    "\n",
    "w, b, loss_history = train(key, X, Y, lr, n_iters)\n",
    "\n",
    "w_0, w_1 = w\n",
    "\n",
    "x_line = jnp.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "y_line = -(b + w_0 * x_line) / w_1\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_class_0_0, x_class_0_1, color=\"C0\", label=\"Class 0\")\n",
    "plt.scatter(x_class_1_0, x_class_1_1, color=\"C1\", label=\"Class 1\")\n",
    "plt.plot(x_line, y_line, color=\"black\", lw=2, label=\"Decision boundary\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Binary Classification with SGD (JAX, {n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Hinge Loss\")\n",
    "plt.title(\"Training Loss (SGD, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Multi-layer networks\n",
    "\n",
    "### 2-layer XOR problem\n",
    "\n",
    "In most cases, classification problems are far from being linear. Therefore, we need more advanced methods to be able to compute non-linear class boundaries. The advantage of neural networks is that the same principle can be applied in a layer-wise fashion. This allows to further discriminate the space in sub-regions. We will try to implement the 2-layer perceptron that can provide a solution to the infamous XOR problem. The idea is now to have the output of the first neurons to be connected to a set of other neurons.\n",
    "\n",
    "### Backwardpropagation (Gradients via Chain Rule)\n",
    "\n",
    "We will compute the gradients with backpropagation. The algorithm computes gradients efficiently by applying the chain rule from calculus. The chain rule tells us how to compute the derivative of a composed function.\n",
    "\n",
    "If we have $\\hat{y} = f(g(x))$, then:\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "In neural networks, we have many layers composed together, so we need to apply the chain rule repeatedly to compute how the loss changes with respect to each parameter.\n",
    "\n",
    "The key insight is that we compute gradients backwards through the network:\n",
    "\n",
    "1. **Output layer**: Compute $\\delta^{(2)}$ using the loss gradient\n",
    "2. **Output weights/biases**: Use $\\delta^{(2)}$ and activations from previous layer\n",
    "3. **Propagate backwards**: Pass $\\delta^{(2)}$ through weights to get gradient at $\\mathbf{h}_1$\n",
    "4. **Hidden layer**: Compute $\\delta^{(1)}$ by multiplying with sigmoid derivative\n",
    "5. **Hidden weights/biases**: Use $\\delta^{(1)}$ and the inputs\n",
    "\n",
    "Each $\\delta$ tells us \"how much does the loss change if we change this pre-activation slightly?\" and we use these to compute how to adjust the weights.\n",
    "\n",
    "A detailed explaination and a step by step calculation of the backpropagation can be found in the slides. Here we will show only the code to calculate the gradients with backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def model(X, params):\n",
    "    W_1, b_1, W_2, b_2 = params\n",
    "    z_1 = X @ W_1 + b_1\n",
    "    h = np.tanh(z_1)  # Hidden layer activations\n",
    "    z_2 = h @ W_2 + b_2\n",
    "    Y_pred = sigmoid(z_2)  # Output layer activations\n",
    "    return h, Y_pred \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def binary_cross_entropy_loss(Y, Y_pred):\n",
    "    eps = 1e-7\n",
    "    Y_pred = np.clip(Y_pred, eps, 1 - eps)\n",
    "    return -np.mean(Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "\n",
    "def binary_cross_entropy_loss_grad(Y, X, Y_pred, h, W_2, n_samples):\n",
    "\n",
    "    # Step 1: Compute δ^(2) = ∂L/∂Z_2\n",
    "    # Using chain rule: ∂L/∂Z_2 = ∂L/∂Y_pred * ∂Y_pred/∂Z_2\n",
    "    # \n",
    "    # Where:\n",
    "    #   ∂L/∂y_pred = -(Y/y_pred - (1-Y)/(1-y_pred))  [derivative of BCE]\n",
    "    #   ∂y_pred/∂Z_2 = y_pred(1 - y_pred) [derivative of the sigmoid]\n",
    "    # Resulting in the simplified function:  δ^(2) = y_pred - y\n",
    "    delta_2 = (Y_pred - Y) \n",
    "\n",
    "    # Step 2: Compute ∂L/∂W_2\n",
    "    # Using chain rule: ∂L/∂W_2 = ∂L/∂Z_2 * ∂Z_2/∂W_2\n",
    "    #\n",
    "    # From forward pass: Z_2 = h @ W_2 + b_2\n",
    "    # Therefore: ∂Z_2/∂W_2 = h\n",
    "    #\n",
    "    # So: ∂L/∂W_2 = δ^(2) @ h\n",
    "\n",
    "    # Output layer weight gradient: ∂L/∂W^(L) = δ^(2) @ (h)\n",
    "    dW_2 = h.T @ delta_2 / n_samples\n",
    "    \n",
    "    # Step 3: Compute ∂L/∂b_2\n",
    "    # Using chain rule: ∂L/∂b_2 = ∂L/∂Z_2 * ∂Z_2/∂b_2\n",
    "    #\n",
    "    # From forward pass: Z_2 = h @ W_2 + b_2\n",
    "    # Therefore: ∂Z_2/∂b_2 = 1\n",
    "    #\n",
    "    # So: ∂L/∂b_2 = δ^(2) (summed over all samples)\n",
    "\n",
    "    # Output layer bias gradient: ∂L/∂b^(L) = δ^(2)\n",
    "    db_2 = np.mean(delta_2, axis=0)\n",
    "\n",
    "    # Step 4: Compute δ^(1) = ∂L/∂Z_1\n",
    "    # Using chain rule: ∂L/∂Z_1 = ∂L/Z_2 * ∂Z_2/∂h\n",
    "    # \n",
    "    # From forward pass: Z_2 = h_1 @ W_2 + b_2\n",
    "    #         Therefore: ∂Z_2/∂h_1 = W_2\n",
    "    #\n",
    "    # so: So: ∂L/∂h_1 = δ^(2) @ W_2^T\n",
    "    dh =  delta_2 @ W_2.T\n",
    "\n",
    "    # Step 5: Compute δ^(1) = ∂L/∂Z_1\n",
    "    # Using chain rule: ∂L/∂Z_1 = ∂L/∂h_1 * ∂h_1/∂Z_1\n",
    "    #\n",
    "    # Where: ∂h_1/∂Z_1 = 1 - tanh(z)^2  [derivative of tanh]\n",
    "    #\n",
    "    # So: δ^(1) = ∂L/∂h_1 * (1 - h^2)\n",
    "    delta_1 = dh * (1 - h**2)\n",
    "    \n",
    "    # Step 6: Compute ∂L/∂W_1\n",
    "    # Using chain rule: ∂L/∂W_1 = ∂L/∂Z_1 * ∂Z_1/∂W_1\n",
    "    #\n",
    "    # From forward pass: Z_1 = X @ W_1 + b_1\n",
    "    # Therefore: ∂Z_1/∂W_1 = X\n",
    "    #\n",
    "    # So: ∂L/∂W_1 = X @ δ^(1)\n",
    "    dW_1 = X.T @ delta_1 / n_samples\n",
    "    \n",
    "    # Step 7: Compute ∂L/∂b_1\n",
    "    # Using chain rule: ∂L/∂b_1 = ∂L/∂Z_1 * ∂Z_1/∂b_1\n",
    "    #\n",
    "    # From forward pass: Z_1 = X @ W_1 + b_1\n",
    "    # Therefore: ∂Z_1/∂b_1 = 1\n",
    "    #\n",
    "    # So: ∂L/∂b_1 = δ^(1) (summed over all samples)\n",
    "    db_1 = np.mean(delta_1, axis=0)\n",
    "    \n",
    "    return dW_2, db_2, dW_1, db_1\n",
    "\n",
    "def train(X, Y, n_hidden=2, lr=0.1, n_iter=10000):\n",
    "    np.random.seed(3)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    W_1 = np.random.randn(n_features, n_hidden)\n",
    "    b_1 = np.zeros((1, n_hidden))\n",
    "    W_2 = np.random.randn(n_hidden, 1)\n",
    "    b_2 = np.zeros((1, 1))\n",
    "\n",
    "    params = (W_1, b_1, W_2, b_2)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Forward pass\n",
    "        h, Y_pred = model(X, params)\n",
    "\n",
    "        # Loss\n",
    "        loss = binary_cross_entropy_loss(Y, Y_pred)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Backward pass\n",
    "        dW_2, db_2, dW_1, db_1 = binary_cross_entropy_loss_grad(Y, X, Y_pred, h, W_2, n_samples)\n",
    "\n",
    "        # Gradient update\n",
    "        W_1 -= lr * dW_1\n",
    "        b_1 -= lr * db_1\n",
    "        W_2 -= lr * dW_2\n",
    "        b_2 -= lr * db_2\n",
    "\n",
    "    return W_1, b_1, W_2, b_2, Y_pred, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1, b_1, W_2, b_2, Y_pred, loss_history = train(X, Y, n_hidden=2, lr=0.5, n_iter=20000)\n",
    "params = (W_1, b_1, W_2, b_2)\n",
    "\n",
    "preds = (Y_pred > 0.5).astype(int)\n",
    "\n",
    "print(\"Predictions:\", preds.ravel())\n",
    "print(\"True labels:\", Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "h = 0.01  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "# Create input array from mesh grid\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Reshape predictions to match mesh grid\n",
    "_, predictions = model(mesh_input, params) \n",
    "\n",
    "Z = predictions.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot decision boundary and regions\n",
    "contourf = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contourf, label='Prediction Probability')\n",
    "\n",
    "class_0 = X[Y.ravel() == 0]\n",
    "class_1 = X[Y.ravel() == 1]\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color=\"C1\", label=\"Class 0\")\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color=\"c\", label=\"Class 1\")\n",
    "\n",
    "plt.xlabel('Feature 1 (x₁)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (x₂)', fontsize=12)\n",
    "plt.title(\"XOR Classification\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (XOR - 2 Layer NN, NumPy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## XOR Problem with JAX\n",
    "\n",
    "We will solve the same problem with JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def model(X, params): \n",
    "    W_1, b_1, W_2, b_2 = params\n",
    "    z_1 = X @ W_1 + b_1\n",
    "    h = jnp.tanh(z_1)  # Hidden layer activations\n",
    "    z_2 = h @ W_2 + b_2\n",
    "    Y_pred = jax.nn.sigmoid(z_2)  # Output layer activations\n",
    "    return h, Y_pred \n",
    "\n",
    "def binary_cross_entropy_loss(Y, Y_pred):\n",
    "    eps = 1e-7\n",
    "    Y_pred = jnp.clip(Y_pred, eps, 1 - eps)\n",
    "    return -jnp.mean(Y*jnp.log(Y_pred) + (1 - Y)*jnp.log(1 - Y_pred))\n",
    "\n",
    "def loss_with_params(X, Y, params):\n",
    "    h,Y_pred = model(X, params)\n",
    "    return binary_cross_entropy_loss(Y, Y_pred)\n",
    "\n",
    "@jit\n",
    "def gradient_descent(params, X, Y, lr):\n",
    "    grads = grad(loss_with_params, argnums=2)(X, Y, params)\n",
    "    W_1, b_1, W_2, b_2 = params\n",
    "    dW1, db1, dW2, db2 = grads\n",
    "\n",
    "    W_2 -= lr * dW2\n",
    "    b_2 -= lr * db2\n",
    "    W_1 -= lr * dW1\n",
    "    b_1 -= lr * db1\n",
    "\n",
    "    new_params = (W_1, b_1, W_2, b_2)\n",
    "\n",
    "    loss = loss_with_params(X, Y, new_params)\n",
    "    return new_params, loss\n",
    "\n",
    "def train(key, X, Y, n_hidden=2, lr=0.5, n_iter=10000):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    k_0, k_1 = random.split(key, 2)\n",
    "    W_1 = random.normal(k_0, (n_features, n_hidden))\n",
    "    b_1 = jnp.zeros((1, n_hidden))\n",
    "    W_2 = random.normal(k_1, (n_hidden, 1))\n",
    "    b_2 = jnp.zeros((1, 1))\n",
    "\n",
    "    params = (W_1, b_1, W_2, b_2)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # Backpropagation\n",
    "        params, loss = gradient_descent(params, X, Y, lr)\n",
    "\n",
    "        loss_history.append(float(loss))\n",
    "\n",
    "    return params, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(3)\n",
    "\n",
    "params, loss_history = train(key, X, Y, n_hidden=2, lr=0.5, n_iter=20000)\n",
    "\n",
    "# Plot data\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "h = 0.01  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "# Create input array from mesh grid\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Reshape predictions to match mesh grid\n",
    "_, predictions = model(mesh_input, params) \n",
    "\n",
    "predictions_flat = predictions.flatten()\n",
    "\n",
    "Z = predictions.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot decision boundary and regions\n",
    "contourf = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contourf, label='Prediction Probability')\n",
    "3\n",
    "class_0 = X[Y.ravel() == 0]\n",
    "class_1 = X[Y.ravel() == 1]\n",
    "\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color=\"C1\", label=\"Class 0\")\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color=\"c\", label=\"Class 1\")\n",
    "\n",
    "plt.xlabel('Feature 1 (x₁)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (x₂)', fontsize=12)\n",
    "plt.title(\"XOR Classification\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (XOR - 2 Layer NN, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Implementation with JAX and two hidden layers\n",
    "\n",
    "The prediction unfortunately is not optimal using only one hidden layer with 2 neurons. We would expect that the prediction would be devided in squares, not a line. In the next exercise we will show how this is better approximated by a neural network with 2 hidden layers. Since we already showed how the chain rule works using numpy we will use JAX that already has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, params):\n",
    "    W_1, b_1, W_2, b_2, W_3, b_3 = params\n",
    "    z_1 = X @ W_1 + b_1\n",
    "    h_1 = jnp.tanh(z_1)\n",
    "    z_2 = h_1 @ W_2 + b_2\n",
    "    h_2 = jnp.tanh(z_2)\n",
    "    z_3 = h_2 @ W_3 + b_3\n",
    "    y_pred = jax.nn.sigmoid(z_3)\n",
    "    return h_1, h_2, y_pred\n",
    "\n",
    "def binary_cross_entropy_loss(Y, Y_pred):\n",
    "    eps = 1e-7\n",
    "    Y_pred = jnp.clip(Y_pred, eps, 1 - eps)\n",
    "    return -jnp.mean(Y*jnp.log(Y_pred) + (1 - Y)*jnp.log(1 - Y_pred))\n",
    "\n",
    "def loss_with_params(X, Y, params):\n",
    "    _,_,Y_pred = model(X, params)\n",
    "    return binary_cross_entropy_loss(Y, Y_pred)\n",
    "\n",
    "@jit\n",
    "def gradient_descent(params, X, Y, lr):\n",
    "    grads = grad(loss_with_params, argnums=2)(X, Y, params)\n",
    "    new_params = tuple(p - lr * g for p, g in zip(params, grads))\n",
    "    loss = loss_with_params(X, Y, new_params)\n",
    "    return new_params, loss\n",
    "\n",
    "def train(key, X, Y, n_hidden1=4, n_hidden2=4, lr=0.5, n_iter=10000):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    k_0, k_1, k_2 = random.split(key, 3)\n",
    "    W_1 = random.normal(k_0, (n_features, n_hidden1))\n",
    "    b_1 = jnp.zeros((1, n_hidden1))\n",
    "    W_2 = random.normal(k_1, (n_hidden1, n_hidden2))\n",
    "    b_2 = jnp.zeros((1, n_hidden2))\n",
    "    W_3 = random.normal(k_2, (n_hidden2, 1))\n",
    "    b_3 = jnp.zeros((1, 1))\n",
    "\n",
    "    params = (W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_iter):\n",
    "        params, loss = gradient_descent(params, X, Y, lr)\n",
    "\n",
    "        loss_history.append(float(loss))\n",
    "\n",
    "    return params, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(3)\n",
    "\n",
    "params, loss_history = train(key, X, Y, n_hidden1=6, n_hidden2=4, lr=0.3, n_iter=10000)\n",
    "\n",
    "# Plot data and fitted line\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "h = 0.01  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "# Create input array from mesh grid\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Reshape predictions to match mesh grid\n",
    "_, _, predictions = model(mesh_input, params)\n",
    "\n",
    "Z = predictions.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot decision boundary and regions\n",
    "contourf = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contourf, label='Prediction Probability')\n",
    "\n",
    "class_0 = X[Y.ravel() == 0]\n",
    "class_1 = X[Y.ravel() == 1]\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color=\"C1\", label=\"Class 0\")\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color=\"c\", label=\"Class 1\")\n",
    "\n",
    "plt.xlabel('Feature 1 (x₁)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (x₂)', fontsize=12)\n",
    "plt.title(\"XOR Classification\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (XOR - 3 Layer NN, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "PyTorch is a powerful and flexible library designed for machine learning and deep learning research and development. It provides a tensor-based computation framework similar to NumPy, but with the added ability to perform automatic differentiation and to leverage GPU acceleration seamlessly.\n",
    "\n",
    "It is built around the concept of dynamic computation graphs, which are created on the fly as operations are executed. This \"define-by-run\" paradigm makes PyTorch intuitive and flexible, allowing researchers to write and debug models in pure Python while still benefiting from high performance.\n",
    "\n",
    "PyTorch is widely used for implementing neural networks, training deep learning models, and experimenting with new architectures in both academia and industry. It supports a large ecosystem of tools and libraries, including torchaudio for audio processing, and torchtext for natural language processing.\n",
    "\n",
    "**Useful functions and modules:**\n",
    "- `torch.tensor`: creates a tensor, the fundamental data structure in PyTorch. Tensors can track gradients when requires_grad=True. [Documentation](https://docs.pytorch.org/docs/stable/tensors.html)\n",
    "- `torch.nn`: provides modules and classes for defining neural network layers, activation functions, and losses. [Documentation](https://docs.pytorch.org/docs/stable/nn.html)\n",
    "- `torch.optim`: implements common optimization algorithms such as SGD and Adam for training models. [Documentation](https://docs.pytorch.org/docs/stable/optim.html)\n",
    "- `loss.backward()`: automatically computes gradients of the loss with respect to model parameters.\n",
    "- `torch.no_grad()`: disables gradient tracking, typically used during model evaluation to improve efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Two hidden layers implemented in Pytorch\n",
    "\n",
    "We will now solve the same XOR Problem with 2 layers using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self, n_input=2, n_hidden1=4, n_hidden2=4, n_output=1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.layer2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.output = nn.Linear(n_hidden2, n_output)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z1 = self.activation(self.layer1(X))\n",
    "        z2 = self.activation(self.layer2(z1))\n",
    "        out = self.output(z2)  # logits (not softmaxed yet)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "n_iters = 20000\n",
    "lr = 0.3\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# Set random seed for reproducibility and use deterministic algorithms\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = XORNet()\n",
    "criterion = nn.BCEWithLogitsLoss()  # combines binary cross-entropy and sigmoid function\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    output = model(X_tensor)\n",
    "    loss = criterion(output, Y_tensor)\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad\n",
    "    \n",
    "    loss_history.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid over the input space\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "h = 0.01  # step size in the mesh\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Prepare input grid for model prediction\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "mesh_tensor = torch.tensor(mesh_input, dtype=torch.float32)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    logits = model(mesh_tensor)\n",
    "    preds = torch.sigmoid(logits).numpy()  # convert to probabilities\n",
    "\n",
    "# Reshape predictions to match grid shape\n",
    "Z = preds.reshape(xx.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot decision surface\n",
    "contourf = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contourf, label='Prediction Probability')\n",
    "\n",
    "# Scatter plot of data points\n",
    "class_0 = X[Y.ravel() == 0]\n",
    "class_1 = X[Y.ravel() == 1]\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color=\"C1\", label=\"Class 0\")\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color=\"c\", label=\"Class 1\")\n",
    "\n",
    "plt.xlabel('Feature 1 (x₁)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (x₂)', fontsize=12)\n",
    "plt.title(\"XOR Classification\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (XOR - 3 Layer NN, PyTorch)\", fontsize=12, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Initializing the parameters\n",
    "\n",
    "One way of optimizing the training is to start with certain initial values for the paramaters. Until now we always started with a random value. However there are other ways to initialize the weight with for example the Xavier initialization. There are two types of Xavier initialization, uniform and normal. For this exercize we will use the uniform Xavier initialization. It draws each weight, w, from a random uniform distribution in in $[-x,x]$ for \n",
    "\n",
    "$$ x = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$$\n",
    "\n",
    "\n",
    "\n",
    "We will solve the XOR Problem again with JAX but now with the Xavier initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(key, shape):\n",
    "    input, output = shape\n",
    "    std = 2 / (input + output)\n",
    "    return random.normal(key, shape) * std\n",
    "\n",
    "def model(X, params):\n",
    "    W_1, b_1, W_2, b_2, W_3, b_3 = params\n",
    "    z_1 = X @ W_1 + b_1\n",
    "    h_1 = jnp.tanh(z_1)\n",
    "    z_2 = h_1 @ W_2 + b_2\n",
    "    h_2 = jnp.tanh(z_2)\n",
    "    z_3 = h_2 @ W_3 + b_3\n",
    "    y_pred = jax.nn.sigmoid(z_3)\n",
    "    return h_1, h_2, y_pred\n",
    "\n",
    "def binary_cross_entropy_loss(Y, Y_pred):\n",
    "    eps = 1e-7\n",
    "    Y_pred = jnp.clip(Y_pred, eps, 1 - eps)\n",
    "    return -jnp.mean(Y*jnp.log(Y_pred) + (1 - Y)*jnp.log(1 - Y_pred))\n",
    "\n",
    "def loss_with_params(X, Y, params):\n",
    "    _,_,Y_pred = model(X, params)\n",
    "    return binary_cross_entropy_loss(Y, Y_pred)\n",
    "\n",
    "@jit\n",
    "def gradient_descent(params, X, Y, lr):\n",
    "    grads = grad(loss_with_params, argnums=2)(X, Y, params)\n",
    "    new_params = tuple(p - lr * g for p, g in zip(params, grads))\n",
    "    loss = loss_with_params(X, Y, new_params)\n",
    "    return new_params, loss\n",
    "\n",
    "def train_xavier_init(key, X, Y, n_hidden1=4, n_hidden2=4, lr=0.5, n_iter=10000):\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights\n",
    "    k_0, k_1, k_2,  = random.split(key, 3)\n",
    "    W_1 = xavier_init(k_0, (n_features, n_hidden1))\n",
    "    b_1 = jnp.zeros((1, n_hidden1))\n",
    "    W_2 = xavier_init(k_1, (n_hidden1, n_hidden2))\n",
    "    b_2 = jnp.zeros((1, n_hidden2))\n",
    "    W_3 = xavier_init(k_2, (n_hidden2, 1))\n",
    "    b_3 = jnp.zeros((1, 1))\n",
    "\n",
    "    params = (W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_iter):\n",
    "        params, loss = gradient_descent(params, X, Y, lr)\n",
    "\n",
    "        loss_history.append(float(loss))\n",
    "\n",
    "    return params, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(2)\n",
    "\n",
    "params, loss_history = train_xavier_init(key, X, Y, n_hidden1=2, n_hidden2=2, lr=0.08, n_iter=20000)\n",
    "\n",
    "# Plot data and fitted line\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "h = 0.01  # Step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "# Create input array from mesh grid\n",
    "mesh_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Reshape predictions to match mesh grid\n",
    "_, _, predictions = model(mesh_input, params)\n",
    "\n",
    "Z = predictions.reshape(xx.shape)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Plot decision boundary and regions\n",
    "contourf = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "plt.colorbar(contourf, label='Prediction Probability')\n",
    "\n",
    "class_0 = X[Y.ravel() == 0]\n",
    "class_1 = X[Y.ravel() == 1]\n",
    "\n",
    "\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], color=\"C1\", label=\"Class 0\")\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], color=\"c\", label=\"Class 1\")\n",
    "\n",
    "plt.xlabel('Feature 1 (x₁)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (x₂)', fontsize=12)\n",
    "plt.title(\"XOR Classification\", fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss (XOR - 2 Layer NN, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Exercise: Classification model for multi class \n",
    "\n",
    "In the next exersize you will build you own neural network to make a multi class classification model. \n",
    "\n",
    "### Implementation in JAX\n",
    "\n",
    "First try to make the model using JAX. The data is already given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_classes = 3\n",
    "n_points = 100\n",
    "noise = 0.2\n",
    "radius = [0.2, 0.7, 1.2]\n",
    "\n",
    "X_list, Y_list = [], []\n",
    "\n",
    "for i, r in enumerate(radius):\n",
    "    theta = np.linspace(0, 2*np.pi, n_points)\n",
    "    if r == 0:  # center dot\n",
    "        X_i = np.random.randn(n_points, 2) * noise\n",
    "    else:\n",
    "        X_i = np.stack([r*np.cos(theta), r*np.sin(theta)], axis=1)\n",
    "        X_i += np.random.randn(n_points, 2) * noise\n",
    "    y_i = np.ones(n_points, dtype=int) * i\n",
    "    X_list.append(X_i)\n",
    "    Y_list.append(y_i)\n",
    "\n",
    "X = np.concatenate(X_list)\n",
    "Y = np.concatenate(Y_list)\n",
    "\n",
    "# Hyperparameter\n",
    "n_iter = 10000\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the Softmax activation function.\n",
    "    Inputs:\n",
    "        z       : array of pre-activation values for the output layer (shape: N_samples, N_classes)\n",
    "    Returns:\n",
    "        a       : array of probabilities (shape: N_samples, N_classes)\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return a\n",
    "\n",
    "def forward_pass(params, X):\n",
    "    \"\"\"\n",
    "    Compute the forward pass for a two-layer neural network.\n",
    "    Inputs:\n",
    "        params  : a tuple containing parameters (W_1, b_1, W_2, b_2)\n",
    "        X       : array of input features (shape: N_samples, N_features)\n",
    "    Returns:\n",
    "        logits  : array of pre-softmax scores (shape: N_samples, N_classes)\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def cross_entropy_loss(thetas, X, Y_true):\n",
    "    \"\"\"\n",
    "    Compute the Categorical Cross-Entropy Loss.\n",
    "    Inputs:\n",
    "        params  : a tuple containing parameters [W_1, b_1, W_2, b_2]\n",
    "        X       : array of input features (shape: N_samples, N_features)\n",
    "        Y       : array of true target values (one-hot encoded, shape: N_samples, N_classes)\n",
    "    Returns:\n",
    "        loss    : float, mean cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "\n",
    "    return loss\n",
    "\n",
    "@jit\n",
    "def update_params(params, grads, lr):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "    Inputs:\n",
    "        params  : tuple with current parameters\n",
    "        grads   : gradients of the loss with respect to params\n",
    "        lr      : the learning rate\n",
    "    Returns:\n",
    "        new_params : updated parameters\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    \n",
    "    return new_params\n",
    "\n",
    "\n",
    "def train_nn(X, Y_true, n_iter, lr, loss_func):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent for the neural network.\n",
    "    Inputs:\n",
    "        X          : array of observations\n",
    "        Y_true     : array of true target values (one-hot encoded)\n",
    "        n_iter     : number of iterations\n",
    "        lr         : learning rate\n",
    "        cross_entropy_loss  : cross entropy loss function to compute loss \n",
    "    Returns:\n",
    "        params        : tuple of learned coefficients\n",
    "        loss_history  : list of loss values per iteration\n",
    "    \"\"\"\n",
    "    \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "    return params, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Implementation with Pytorch\n",
    "\n",
    "Now do the same using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data to tensors for PyTorch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassNet(nn.Module):\n",
    "    def __init__(self, n_input=2, n_hidden=2, n_output=3):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the network.\n",
    "        Inputs:\n",
    "            n_input  : number of input features\n",
    "            n_hidden : number of hidden units in the single hidden layer\n",
    "            n_output : number of output classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Defines the forward pass logic.\n",
    "        Inputs:\n",
    "            X : Tensor of input features (N_samples, N_features)\n",
    "        Returns:\n",
    "            out : Tensor of raw logits (N_samples, N_classes)\n",
    "        \"\"\"\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        return out\n",
    "    \n",
    "# Initialize model, loss, optimizer\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "# Training loop with optimization\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicton"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
