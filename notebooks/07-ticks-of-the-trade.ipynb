{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dce7f8",
   "metadata": {},
   "source": [
    "# Lesson 6: Tricks of the trade\n",
    "\n",
    "In this course we will cover:\n",
    "1. Learning rate scheduler/Dynamic learning rate\n",
    "2. Dropout (macht validation besser)\n",
    "3. Risidual connections (alles)\n",
    "4. batch normalisation (CNN), layer normalisation (Transformer) —> alle Arten von normalisation\n",
    "5. initialisation\n",
    "6. early stopping\n",
    "7. Transfer learning\n",
    "8. Augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Define device:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  \n",
    "elif  torch.mtia.is_available():\n",
    "    device = torch.device(\"mtia\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data\n",
    "def generate_waveform(wave_type, length, fs):\n",
    "    t = np.linspace(0, 1, length, endpoint=False)\n",
    "    freq = np.random.uniform(1, 10)      # random frequency 1-10Hz\n",
    "    amp = np.random.uniform(0.5, 1.5)    # random amplitude\n",
    "    phi = np.random.uniform(0, 2*np.pi)  # random phases\n",
    "\n",
    "    if wave_type == 'sine':\n",
    "        y = amp * np.sin(2 * np.pi * freq * t + phi)\n",
    "    elif wave_type == 'triangle':\n",
    "        y = amp * signal.sawtooth(2 * np.pi * freq * t + phi, 0.5)\n",
    "    elif wave_type == 'square':\n",
    "        y = amp * signal.square(2 * np.pi * freq * t + phi)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown wave type\")\n",
    "    \n",
    "    # optional noise\n",
    "    noise = np.random.normal(0, 0.05, length)\n",
    "    y += noise\n",
    "    return y\n",
    "\n",
    "# Parameters\n",
    "num_samples = 200      # samples per waveform type\n",
    "length = 128           # number of points per waveform\n",
    "fs = 128               # sampling frequency\n",
    "\n",
    "# Generate dataset in memory\n",
    "wave_types = ['sine', 'triangle', 'square']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, wave in enumerate(wave_types):\n",
    "    for _ in range(num_samples):\n",
    "        waveform = generate_waveform(wave, length, fs)\n",
    "        X.append(waveform)\n",
    "        y.append(idx)  # class label: 0=sine, 1=triangle, 2=square\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)   # shape: (600, 128)\n",
    "y = np.array(y)   # shape: (600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create our data set with torch.utils.data.Dataset\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # add channel dimension\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "dataset = WaveformDataset(X, y)\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our CNN model for waveform classification\n",
    "class Waveform_Classification_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5,  stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader,  criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy (no need to reshape for classification)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Step scheduler after epoch if provided\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "        print(f'Learning Rate after epoch: {lr_scheduler.get_last_lr()[0]:.6f}')\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:  \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass only (no gradient computation)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs_flat = outputs.reshape(-1, outputs. size(-1))\n",
    "            labels_flat = labels.reshape(-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += labels_flat.size(0)\n",
    "            correct += predicted.eq(labels_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(valid_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5630ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, lr_scheduler=None, n_epochs=15, device='cpu'):\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print progress every 100 epochs or on first/last epoch\n",
    "        if n_epochs <= 30:\n",
    "            if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "                print(f'Epoch [{epoch+1}/{n_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        elif n_epochs > 30:\n",
    "            if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "                print(f'Epoch [{epoch+1}/{n_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(n_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(n_epochs), valid_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a205c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with scheduler enabled\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b6240",
   "metadata": {},
   "source": [
    "## Using a Scheduler or Dynamic Learning Rate\n",
    "\n",
    "One way to improve the performance of the model is by using a scheduler for the learning rate or use a dynamic learning rate. \n",
    "\n",
    "### What is a Learning Rate Scheduler?\n",
    "\n",
    "A learning rate scheduler is a technique that adjusts the learning rate during training according to a predefined strategy. Instead of keeping the learning rate constant throughout the entire training process, the scheduler modifies it at specific intervals (e.g., per epoch or per batch).\n",
    "\n",
    "Common scheduling strategies include:\n",
    "- **Step Decay**: Reduces the learning rate by a factor every few epochs\n",
    "- **Exponential Decay**: Gradually decreases the learning rate exponentially\n",
    "- **Cosine Annealing**: Varies the learning rate following a cosine curve\n",
    "- **ReduceLROnPlateau**: Reduces the learning rate when a metric stops improving\n",
    "\n",
    "### Why Does It Make Training More Effective? \n",
    "\n",
    "1. **Better Convergence**: Starting with a higher learning rate allows the model to make large steps toward the optimal solution early in training.  As training progresses, a lower learning rate helps fine-tune the parameters and converge to a better minimum.\n",
    "\n",
    "2. **Escape Local Minima**: A dynamic learning rate can help the model escape shallow local minima in the early stages while settling into deeper, better minima as the rate decreases.\n",
    "\n",
    "3. **Prevents Overshooting**: A constant high learning rate might cause the optimizer to overshoot the optimal point. Reducing it over time ensures more precise updates near convergence.\n",
    "\n",
    "4. **Improved Generalization**: Gradually lowering the learning rate can lead to flatter minima, which often generalize better to unseen data. \n",
    "\n",
    "5. **Faster Training**: By adapting the learning rate to the training dynamics, schedulers can achieve better results in fewer epochs compared to a fixed learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "# Train the model with scheduler enabled\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "\n",
    "    n_epochs=n_epochs, \n",
    "\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_inputs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_inputs.extend(inputs.cpu().numpy()) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b939fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_types = ['sine', 'triangle', 'square']\n",
    "n_examples = 6\n",
    "idxs = np.random.choice(len(all_inputs), n_examples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(all_inputs[idx][0], color='black')\n",
    "    plt.title(f\"True: {wave_types[all_labels[idx]]}\\nPred: {wave_types[all_preds[idx]]}\",\n",
    "              color=\"green\" if all_labels[idx] == all_preds[idx] else \"red\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a97c9",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "One effective technique to prevent overfitting and improve model generalization is by using dropout during training. \n",
    "\n",
    "Dropout is a regularization technique where random neurons are temporarily \"dropped\" (set to zero) during training with a specified probability. This means that during each training iteration, a different subset of neurons is active, forcing the network to learn more robust features. Dropout is only applied during training. During evaluation/inference, all neurons are active, and their outputs are typically scaled to account for the dropout rate used during training. \n",
    "\n",
    "Common dropout rates range from 0.2 to 0.5. Higher rates provide stronger regularization but may hurt learning if too high. For example, with a dropout rate of 0.5, each neuron has a 50% chance of being deactivated during any given training step.\n",
    "\n",
    " \n",
    "### Why Does It Make Training More Effective?\n",
    "\n",
    "1.  **Prevents Overfitting**: By randomly dropping neurons, dropout prevents the network from relying too heavily on specific neurons or learning complex co-adaptations between neurons that only work on the training data.\n",
    "\n",
    "2. **Ensemble Effect**: Dropout can be viewed as training an ensemble of multiple sub-networks simultaneously. At inference time, using all neurons approximates averaging the predictions of all these sub-networks, leading to better generalization.\n",
    "\n",
    "3.  **Forces Redundancy**: Since any neuron can be dropped at any time, the network learns to distribute information across multiple neurons rather than concentrating it in a few.  This creates more robust and redundant representations.\n",
    "\n",
    "4. **Reduces Co-adaptation**: Without dropout, neurons can develop complex interdependencies that don't generalize well.  Dropout breaks these dependencies, forcing each neuron to learn more independently useful features.\n",
    "\n",
    "5. **Improves Generalization**: Models trained with dropout typically perform better on unseen data because they learn more general patterns rather than memorizing the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with dropout\n",
    "class CNN_with_dropout(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate_conv=0.2, dropout_rate_lin=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Add dropout layers\n",
    "        self.dropout_conv = nn.Dropout(dropout_rate_conv)  # Lower dropout for conv layers\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate_lin)  # Higher dropout for fully connected\n",
    "        \n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)  # Dropout after first conv block\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)  # Dropout after second conv block\n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "        x = F. relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)  # Dropout after first FC layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "dropout_rate_conv = 0\n",
    "dropout_rate_lin = 0\n",
    "\n",
    "model = CNN_with_dropout(num_classes=3, dropout_rate_conv=dropout_rate_conv, dropout_rate_lin=dropout_rate_lin).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model without\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "dropout_rate_conv = 0.1\n",
    "dropout_rate_lin = 0.3\n",
    "\n",
    "model = CNN_with_dropout(num_classes=3, dropout_rate_conv=dropout_rate_conv, dropout_rate_lin=dropout_rate_lin).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model without\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399bf71",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "\n",
    "One powerful technique to improve training stability and speed up convergence is by using batch normalization.  Batch normalization has become a standard component in modern deep neural networks. \n",
    "\n",
    "Batch normalization is a technique that normalizes the inputs of each layer by adjusting and scaling the activations.  For each mini-batch during training, it:\n",
    "\n",
    "1. Calculates the mean and variance of the activations\n",
    "2. Normalizes the activations using these statistics\n",
    "3. Scales and shifts the normalized values using learnable parameters (γ and β)\n",
    "\n",
    "The normalization formula for a batch:\n",
    "```\n",
    "x_norm = (x - μ_batch) / √(σ²_batch + ε)\n",
    "output = γ * x_norm + β\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **μ_batch**: Mean of the batch\n",
    "- **σ²_batch**: Variance of the batch\n",
    "- **ε**: Small constant for numerical stability (e.g., 1e-5)\n",
    "- **γ, β**: Learnable parameters for scale and shift\n",
    "\n",
    "Typically the normalization is placed **after** the linear/convolutional layer and **before** the activation function:\n",
    "   ```\n",
    "   Conv/Linear → BatchNorm → Activation (ReLU)\n",
    "   ```\n",
    "It works best with reasonably sized batches (≥16).  Very small batches can cause instability. \n",
    "\n",
    "\n",
    "### Why Does It Make Training More Effective?\n",
    "\n",
    "1. **Faster Training**: By normalizing activations, batch normalization allows for higher learning rates without the risk of divergence.  This can speed up training by 2-10x in some cases.\n",
    "\n",
    "2. **Reduces Internal Covariate Shift**: As the network learns, the distribution of inputs to each layer changes. Batch normalization stabilizes these distributions, making training more stable and predictable.\n",
    "\n",
    "3. **Acts as Regularization**: Batch normalization introduces a slight noise (because statistics are computed per batch), which has a mild regularization effect similar to dropout.  This can reduce the need for other regularization techniques.\n",
    "\n",
    "4. **Reduces Sensitivity to Initialization**: Networks with batch normalization are less sensitive to the initial weights, making training more robust and reproducible.\n",
    "\n",
    "5. **Helps Gradient Flow**: By keeping activations in a reasonable range, batch normalization prevents vanishing or exploding gradients, allowing gradients to flow more effectively through deep networks.\n",
    "\n",
    "6. **Enables Deeper Networks**: The stabilizing effect of batch normalization makes it possible to train much deeper networks that would otherwise be difficult to optimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c72702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with risidual connections\n",
    "class CNN_with_batch_normalization(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16, momentum=0.01)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32, momentum=0.01)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66882238",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_batch_normalization(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8637c",
   "metadata": {},
   "source": [
    "## Using Residual Connections\n",
    "\n",
    "One powerful architectural technique to improve deep neural network training is by using residual connections (also known as skip connections). \n",
    "\n",
    "Residual connections create shortcuts that allow the input of a layer (or block of layers) to bypass those layers and be added directly to the output.  Instead of learning a direct mapping H(x), the layers learn a residual mapping F(x), and the final output becomes F(x) + x. The input and output dimensions must match for the addition operation.  If they don't, use a projection (typically a 1x1 convolution) on the skip connection. Typically, the activation function (ReLU) is applied **after** the addition of the residual connection. Residual connections combine well with batch normalization, dropout, and other regularization techniques. \n",
    "\n",
    "This concept was introduced in ResNet (Residual Networks) and has become a fundamental building block in modern deep learning architectures.\n",
    "\n",
    "### Why Do They Make Training More Effective?\n",
    "\n",
    "1. **Solves Vanishing Gradient Problem**: In very deep networks, gradients can become extremely small during backpropagation, making it difficult for early layers to learn.  Residual connections provide direct gradient pathways, allowing gradients to flow backward through the network more easily.\n",
    "\n",
    "2. **Enables Deeper Networks**: Before residual connections, making networks deeper often degraded performance due to optimization difficulties. Residual connections make it possible to train networks with hundreds or even thousands of layers effectively.\n",
    "\n",
    "3. **Easier Optimization**: Learning the residual (the difference between input and desired output) is often easier than learning the complete transformation.  If the optimal function is close to an identity mapping, the network can simply learn to make F(x) ≈ 0.\n",
    "\n",
    "4. **Identity Mapping**: In the worst case, if additional layers aren't helpful, the network can learn to pass the input through unchanged (identity function) by setting the residual to zero.  This ensures deeper models perform at least as well as shallower ones.\n",
    "\n",
    "5. **Feature Reuse**: Skip connections allow the network to reuse features from earlier layers, combining low-level and high-level features for better representations.\n",
    "\n",
    "6. **Faster Convergence**: Networks with residual connections often converge faster during training because the gradient signal is stronger and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44990283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with risidual connections\n",
    "class CNN_with_Residual_Connections(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv_1x1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=5, stride=2),  # Match conv1\n",
    "            nn.MaxPool1d(2),                             # Match first pool\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2),  # Match conv2\n",
    "            nn.MaxPool1d(2)                              # Match second pool\n",
    "        )\n",
    "        \n",
    "        self.fc_projection = nn.Linear(224, 64)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual  = x\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        residual = self.conv_1x1(residual)  # Shape: [batch, 32, matching_length]\n",
    "        residual = self.pool(self.pool(residual)) \n",
    "\n",
    "        x = x + residual\n",
    "        x = F.relu(x) \n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "   \n",
    "        residual_fc = self.fc_projection(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x + residual_fc \n",
    "        x = F.relu(x)  \n",
    "\n",
    "        x = self.fc2(x)\n",
    "  \n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652173aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_batch_normalization(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_Residual_Connections(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baa70a",
   "metadata": {},
   "source": [
    "## Initialisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d440f",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8dffd9",
   "metadata": {},
   "source": [
    "## Transfer learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5422d6",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
