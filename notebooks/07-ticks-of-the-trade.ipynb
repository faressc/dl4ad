{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lesson 7: Tricks of the trade\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Optimization techniques\n",
    "2. Regularization techniques\n",
    "3. Transfer learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Waveform classifier\n",
    "\n",
    "We will work with the same data set and model as in notebook 4 and go through some optimization and regularizations techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define device:\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  \n",
    "elif  torch.mtia.is_available():\n",
    "    device = torch.device(\"mtia\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data\n",
    "def generate_waveform(wave_type, length, fs):\n",
    "    t = np.linspace(0, 1, length, endpoint=False)\n",
    "    freq = np.random.uniform(1, 10)      # random frequency 1-10Hz\n",
    "    amp = np.random.uniform(0.5, 1.5)    # random amplitude\n",
    "    phi = np.random.uniform(0, 2*np.pi)  # random phases\n",
    "\n",
    "    if wave_type == 'sine':\n",
    "        y = amp * np.sin(2 * np.pi * freq * t + phi)\n",
    "    elif wave_type == 'triangle':\n",
    "        y = amp * signal.sawtooth(2 * np.pi * freq * t + phi, 0.5)\n",
    "    elif wave_type == 'square':\n",
    "        y = amp * signal.square(2 * np.pi * freq * t + phi)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown wave type\")\n",
    "    \n",
    "    # optional noise\n",
    "    noise = np.random.normal(0, 0.05, length)\n",
    "    y += noise\n",
    "    return y\n",
    "\n",
    "# Parameters\n",
    "num_samples = 200      # samples per waveform type\n",
    "length = 128           # number of points per waveform\n",
    "fs = 128               # sampling frequency\n",
    "\n",
    "# Generate dataset in memory\n",
    "wave_types = ['sine', 'triangle', 'square']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, wave in enumerate(wave_types):\n",
    "    for _ in range(num_samples):\n",
    "        waveform = generate_waveform(wave, length, fs)\n",
    "        X.append(waveform)\n",
    "        y.append(idx)  # class label: 0=sine, 1=triangle, 2=square\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)  \n",
    "y = np.array(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create our data set with torch.utils.data.Dataset\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # add channel dimension\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "dataset = WaveformDataset(X, y)\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our CNN model for waveform classification\n",
    "class Waveform_Classification_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5,  stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader,  criterion, optimizer, lr_scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy (no need to reshape for classification)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Step scheduler after epoch if provided\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "       \n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:  \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass only (no gradient computation)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs_flat = outputs.reshape(-1, outputs. size(-1))\n",
    "            labels_flat = labels.reshape(-1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += labels_flat.size(0)\n",
    "            correct += predicted.eq(labels_flat).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(valid_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, lr_scheduler=None, early_stopping=None, n_epochs=15, device='cpu'):\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, lr_scheduler, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "        valid_losses.append(val_loss)\n",
    "        valid_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print progress every 100 epochs or on first/last epoch\n",
    "        if n_epochs <= 30:\n",
    "            if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "                print(f'Epoch [{epoch+1}/{n_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "                if lr_scheduler:\n",
    "                     print(f'Learning Rate after epoch: {lr_scheduler.get_last_lr()[0]:.6f}')   \n",
    "        elif n_epochs > 30:\n",
    "            if epoch % 10 == 0 or epoch == n_epochs - 1:\n",
    "                print(f'Epoch [{epoch+1}/{n_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "        if early_stopping:\n",
    "            if early_stopping.step(val_loss) == True:\n",
    "                print(\"Early stopping triggered\")\n",
    "                n_epochs = epoch + 1\n",
    "                model.load_state_dict(early_stopping.best_weights)  \n",
    "                break\n",
    "\n",
    "\n",
    "    print('-' * 50)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(range(n_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(n_epochs), valid_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "First we train the model without any optimization and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with scheduler enabled\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_inputs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_inputs.extend(inputs.cpu().numpy()) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_types = ['sine', 'triangle', 'square']\n",
    "n_examples = 6\n",
    "idxs = np.random.choice(len(all_inputs), n_examples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(all_inputs[idx][0], color='black')\n",
    "    plt.title(f\"True: {wave_types[all_labels[idx]]}\\nPred: {wave_types[all_preds[idx]]}\",\n",
    "              color=\"green\" if all_labels[idx] == all_preds[idx] else \"red\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "We will go discuss the following optimization techniques:\n",
    "1. Learning rate scheduler\n",
    "2. Batch normalization\n",
    "3. Weight Initialization\n",
    "4. Residual connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 1. Using a Learning Rate Scheduler\n",
    "\n",
    "One way to improve the performance of the model is by using a scheduler for the learning rate or use a dynamic learning rate. \n",
    "\n",
    "A learning rate scheduler is a technique that adjusts the learning rate during training according to a predefined strategy. Instead of keeping the learning rate constant throughout the entire training process, the scheduler modifies it at specific intervals (e.g., per epoch or per batch).\n",
    "\n",
    "Common scheduling strategies include:\n",
    "- **Step Decay**: Reduces the learning rate by a factor every few epochs\n",
    "- **Exponential Decay**: Gradually decreases the learning rate exponentially\n",
    "- **Cosine Annealing**: Varies the learning rate following a cosine curve\n",
    "- **ReduceLROnPlateau**: Reduces the learning rate when a metric stops improving\n",
    "\n",
    "### Why Does It Make Training More Effective? \n",
    "\n",
    "1. **Better Convergence**: Starting with a higher learning rate allows the model to make large steps toward the optimal solution early in training.  As training progresses, a lower learning rate helps fine-tune the parameters and converge to a better minimum.\n",
    "\n",
    "2. **Escape Local Minima**: A dynamic learning rate can help the model escape shallow local minima in the early stages while settling into deeper, better minima as the rate decreases.\n",
    "\n",
    "3. **Prevents Overshooting**: A constant high learning rate might cause the optimizer to overshoot the optimal point. Reducing it over time ensures more precise updates near convergence.\n",
    "\n",
    "4. **Improved Generalization**: Gradually lowering the learning rate can lead to flatter minima, which often generalize better to unseen data. \n",
    "\n",
    "5. **Faster Training**: By adapting the learning rate to the training dynamics, schedulers can achieve better results in fewer epochs compared to a fixed learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "# Train the model with scheduler enabled\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "\n",
    "    n_epochs=n_epochs, \n",
    "\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 2. Batch normalization\n",
    "\n",
    "One powerful technique to improve training stability and speed up convergence is by using batch normalization.  Batch normalization has become a standard component in modern deep neural networks. \n",
    "\n",
    "Batch normalization is a technique that normalizes the inputs of each layer by adjusting and scaling the activations.  For each mini-batch during training, it:\n",
    "\n",
    "1. Calculates the mean and variance of the activations\n",
    "2. Normalizes the activations using these statistics\n",
    "3. Scales and shifts the normalized values using learnable parameters (γ and β)\n",
    "\n",
    "\n",
    "Typically the normalization is placed **after** the linear/convolutional layer and **before** the activation function. It works best with reasonably sized batches (≥16).  Very small batches can cause instability. \n",
    "\n",
    "\n",
    "### Why Does It Make Training More Effective?\n",
    "\n",
    "1. **Faster Training**: By normalizing activations, batch normalization allows for higher learning rates without the risk of divergence.  This can speed up training by 2-10x in some cases.\n",
    "\n",
    "2. **Reduces Internal Covariate Shift**: As the network learns, the distribution of inputs to each layer changes. Batch normalization stabilizes these distributions, making training more stable and predictable.\n",
    "\n",
    "3. **Acts as Regularization**: Batch normalization introduces a slight noise (because statistics are computed per batch), which has a mild regularization effect similar to dropout.  This can reduce the need for other regularization techniques.\n",
    "\n",
    "4. **Reduces Sensitivity to Initialization**: Networks with batch normalization are less sensitive to the initial weights, making training more robust and reproducible.\n",
    "\n",
    "5. **Helps Gradient Flow**: By keeping activations in a reasonable range, batch normalization prevents vanishing or exploding gradients, allowing gradients to flow more effectively through deep networks.\n",
    "\n",
    "6. **Enables Deeper Networks**: The stabilizing effect of batch normalization makes it possible to train much deeper networks that would otherwise be difficult to optimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with risidual connections\n",
    "class CNN_with_batch_normalization(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16, momentum=0.1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32, momentum=0.1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_batch_normalization(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 3. Weight Initialization\n",
    "\n",
    "One often overlooked but important technique for successful training is proper weight initialization. Weight initialization refers to how we set the initial values of the network's parameters before training begins. \n",
    "\n",
    "Weight initialization is the process of setting initial values for the weights and biases in a neural network before training starts. Instead of starting with random values from a simple distribution (like uniform random between -1 and 1), we use carefully designed initialization strategies that account for the network's architecture. \n",
    "\n",
    "### Common Initialization Strategies\n",
    "\n",
    "- **Xavier/Glorot Initialization**: Designed for layers with sigmoid or tanh activations.  Scales weights based on the number of input and output neurons.\n",
    "- **Kaiming/He Initialization**: Designed for layers with ReLU activations. Accounts for the fact that ReLU zeros out half the neurons.\n",
    "- **Zero Initialization**: Setting all weights to zero, however neurons will all learn identically.\n",
    "- **Small Random Values**: Simple random initialization with small values (outdated approach).\n",
    "\n",
    "### Why Does It Matter?\n",
    "\n",
    "1. **Prevents Vanishing/Exploding Gradients**:  Proper initialization keeps the gradients in a reasonable range during the first few iterations, preventing them from becoming too small (vanishing) or too large (exploding).\n",
    "\n",
    "2. **Faster Convergence**: With good initialization, the network starts closer to a good solution, reducing the number of epochs needed to reach good performance.\n",
    "\n",
    "3. **Avoids Symmetry Breaking Issues**: If all weights start with the same value, all neurons in a layer will learn the same features.  Proper initialization breaks this symmetry.\n",
    "\n",
    "4. **Maintains Signal Variance**: Good initialization ensures that the variance of activations and gradients stays roughly constant across layers, making training more stable.\n",
    "\n",
    "5. **Activation-Specific**:  Different activation functions (ReLU, tanh, sigmoid) have different properties, and initialization should match the activation being used.\n",
    "\n",
    "6. **Reduces Sensitivity to Learning Rate**: Proper initialization makes the network less sensitive to the choice of learning rate, making hyperparameter tuning easier.\n",
    "\n",
    "### PyTorch Default Initialization\n",
    "\n",
    "PyTorch layers come with default initialization: \n",
    "- **Linear layers**: Uniform distribution scaled by layer size\n",
    "- **Conv layers**: Kaiming uniform initialization\n",
    "- **BatchNorm**:  Weights=1, Bias=0\n",
    "\n",
    "While these defaults often work well, custom initialization can improve performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# Our CNN model for waveform classification with initialization\n",
    "class CNN_with_initialization(nn.Module):\n",
    "    def __init__(self, num_classes=3, init=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5,  stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        if init == 0:\n",
    "            self.init_zeros()\n",
    "        elif init == 1:\n",
    "            self.init_too_high()\n",
    "        elif init == 2:\n",
    "            self.init_too_low()\n",
    "\n",
    "    def init_zeros(self):\n",
    "        # Option 1: All zeros (symmetry problem - all neurons learn the same)\n",
    "        print(\"Initializing weights to zeros\")\n",
    "        init.zeros_(self.conv1.weight)\n",
    "        init.zeros_(self.conv2.weight)    \n",
    "        \n",
    "    def init_too_high(self):\n",
    "        # Option 2: Too large values (exploding gradients)\n",
    "        print(\"Initializing weights too high\")\n",
    "        init.normal_(self.conv1.weight, mean=0, std=10.0) \n",
    "        init.normal_(self.conv2.weight, mean=0, std=10.0)\n",
    "    \n",
    "    def init_too_low(self):\n",
    "        # Option 3: Too small values (vanishing gradients)\n",
    "        print(\"Initializing weights too low\")\n",
    "        init.normal_(self.conv1.weight, mean=0, std=0.0001)  # Way too small!\n",
    "        init. normal_(self.conv2.weight, mean=0, std=0.0001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_initialization(num_classes=3, init=0).to(device) # init=0 for zeros, init=1 for too high, init=2 for too low\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 4. Using Residual Connections\n",
    "\n",
    "One powerful architectural technique to improve deep neural network training is by using residual connections (also known as skip connections). \n",
    "\n",
    "Residual connections create shortcuts that allow the input of a layer (or block of layers) to bypass those layers and be added directly to the output.  Instead of learning a direct mapping H(x), the layers learn a residual mapping F(x), and the final output becomes F(x) + x. The input and output dimensions must match for the addition operation. Typically, the activation function (ReLU) is applied **after** the addition of the residual connection. Residual connections combine well with batch normalization, dropout, and other regularization techniques. \n",
    "\n",
    "This concept was introduced in ResNet (Residual Networks) and has become a fundamental building block in modern deep learning architectures.\n",
    "\n",
    "### Why Do They Make Training More Effective?\n",
    "\n",
    "1. **Solves Vanishing Gradient Problem**: In very deep networks, gradients can become extremely small during backpropagation, making it difficult for early layers to learn.  Residual connections provide direct gradient pathways, allowing gradients to flow backward through the network more easily.\n",
    "\n",
    "2. **Enables Deeper Networks**: Before residual connections, making networks deeper often degraded performance due to optimization difficulties. Residual connections make it possible to train networks with hundreds or even thousands of layers effectively.\n",
    "\n",
    "3. **Easier Optimization**: Learning the residual (the difference between input and desired output) is often easier than learning the complete transformation.  If the optimal function is close to an identity mapping, the network can simply learn to make F(x) ≈ 0.\n",
    "\n",
    "4. **Identity Mapping**: In the worst case, if additional layers aren't helpful, the network can learn to pass the input through unchanged (identity function) by setting the residual to zero.  This ensures deeper models perform at least as well as shallower ones.\n",
    "\n",
    "5. **Feature Reuse**: Skip connections allow the network to reuse features from earlier layers, combining low-level and high-level features for better representations.\n",
    "\n",
    "6. **Faster Convergence**: Networks with residual connections often converge faster during training because the gradient signal is stronger and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with risidual connections\n",
    "class CNN_with_Residual_Connections(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        self.fc_projection = nn.Linear(224, 64)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual  = x\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        residual = residual[...,:7]  # Adjust residual to match output shape\n",
    "\n",
    "        x = x + residual\n",
    "        x = F.relu(x) \n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "        # print(f\"x shape before fc1: {x.shape}\")  # Debugging line\n",
    "   \n",
    "        residual_fc = x\n",
    "        residual_fc = residual_fc[...,:64]  # Adjust residual to match output shape\n",
    "        # print(f\"residual_fc shape: {residual_fc.shape}\")  # Debugging line\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print(f\"x shape after fc1: {x.shape}\")  # Debugging line\n",
    "        x = x + residual_fc \n",
    "        x = F.relu(x)  \n",
    "\n",
    "        x = self.fc2(x)\n",
    "  \n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "model = CNN_with_Residual_Connections(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Overfitting Prevention Techniques (regularization)\n",
    "\n",
    "Overfitting is a big problem when training a model. We can see that when we train the model for too long, the validation error starts increasing again which suggests that the model is overfitting. There are a few regularization that can be implemented to prevent the overfitting when training. We will discuss the following regularization:\n",
    "1. Dropout\n",
    "2. Early stopping\n",
    "3. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model without\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 1. Dropout\n",
    "\n",
    "One effective technique to prevent overfitting and improve model generalization is by using dropout during training. \n",
    "\n",
    "Dropout is a regularization technique where random neurons are temporarily \"dropped\" (set to zero) during training with a specified probability. This means that during each training iteration, a different subset of neurons is active, forcing the network to learn more robust features. Dropout is only applied during training. During evaluation/inference, all neurons are active, and their outputs are typically scaled to account for the dropout rate used during training. \n",
    "\n",
    "Common dropout rates range from 0.2 to 0.5. Higher rates provide stronger regularization but may hurt learning if too high. For example, with a dropout rate of 0.5, each neuron has a 50% chance of being deactivated during any given training step.\n",
    "\n",
    " \n",
    "### Why Does It Make Training More Effective?\n",
    "\n",
    "1.  **Prevents Overfitting**: By randomly dropping neurons, dropout prevents the network from relying too heavily on specific neurons or learning complex co-adaptations between neurons that only work on the training data.\n",
    "\n",
    "2. **Ensemble Effect**: Dropout can be viewed as training an ensemble of multiple sub-networks simultaneously. At inference time, using all neurons approximates averaging the predictions of all these sub-networks, leading to better generalization.\n",
    "\n",
    "3.  **Forces Redundancy**: Since any neuron can be dropped at any time, the network learns to distribute information across multiple neurons rather than concentrating it in a few.  This creates more robust and redundant representations.\n",
    "\n",
    "4. **Reduces Co-adaptation**: Without dropout, neurons can develop complex interdependencies that don't generalize well.  Dropout breaks these dependencies, forcing each neuron to learn more independently useful features.\n",
    "\n",
    "5. **Improves Generalization**: Models trained with dropout typically perform better on unseen data because they learn more general patterns rather than memorizing the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our CNN model for waveform classification with dropout\n",
    "class CNN_with_dropout(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate_conv=0.2, dropout_rate_lin=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Add dropout layers\n",
    "        self.dropout_conv = nn.Dropout(dropout_rate_conv)  # Lower dropout for conv layers\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate_lin)  # Higher dropout for fully connected\n",
    "        \n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)  # Dropout after first conv block\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)  # Dropout after second conv block\n",
    "        \n",
    "        x = x.view(x. size(0), -1)  # flatten\n",
    "        x = F. relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)  # Dropout after first FC layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "dropout_rate_conv = 0.1\n",
    "dropout_rate_lin = 0.3\n",
    "\n",
    "model = CNN_with_dropout(num_classes=3, dropout_rate_conv=dropout_rate_conv, dropout_rate_lin=dropout_rate_lin).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model without\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 2. Early stopping\n",
    "\n",
    "One practical technique to prevent overfitting and save training time is early stopping. It automatically stops training when the model stops improving on the validation set. Instead of training for a fixed number of epochs, we monitor the validation loss and stop training if it doesn't improve for a certain number of epochs (called **patience**). The patience should be set appropriately, too low (2-3) might stop too early, too high (>20) defeats the purpose.\n",
    "\n",
    "It monitors validation loss after each epoch. Then it track the best validation loss seen so far and counts epochs without improvement (patience counter). It stop training if patience is exceeded and restores best weights from the epoch with lowest validation loss.\n",
    "\n",
    "### Why Does It Help?\n",
    "\n",
    "1. **Prevents Overfitting**: Stops training before the model starts memorizing the training data instead of learning general patterns. \n",
    "\n",
    "2. **Saves Time**: No need to train for hundreds of epochs if the model stopped improving after 50 epochs.\n",
    "\n",
    "3. **Automatic**:  You don't have to guess the optimal number of epochs in advance - the algorithm decides for you.\n",
    "\n",
    "4. **Better Generalization**: Returns the model from the epoch where it performed best on validation data, not the last epoch.\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "- **Patience**: How many epochs to wait without improvement before stopping (typical: 5-15 epochs)\n",
    "- **Min Delta**: Minimum change in validation loss to count as improvement (typical: 0.001)\n",
    "- **Restore Best Weights**: Whether to load the best model weights when stopping (usually:  True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def step(self, current_loss):\n",
    "        if self.best_loss is None or current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.best_weights = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    early_stopping=EarlyStopping(patience=10, min_delta=0.001),\n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 3. Data augmentation\n",
    "\n",
    "Another way to get better results and prevent overfitting is by augmenting the training data. Data augmentation artificially increases the size and diversity of the training dataset by applying random transformations to existing samples. It is important to **only augment training data**, Never augment validation or test sets! These should represent real-world data to give honest performance metrics. Furthermore, don't distort the data so much that it no longer represents the task.\n",
    "\n",
    "For audio data augmentation the following can be done: \n",
    "\n",
    "- Noise injection: add gaussian or random noise to the audio dataset to improve the model performance. \n",
    "- Shifting: shift audio left (fast forward) or right with random seconds.\n",
    "- Changing the speed: stretches times series by a fixed rate.\n",
    "- Changing the pitch: randomly change the pitch of the audio. \n",
    "\n",
    "### Why Use Data Augmentation?\n",
    "\n",
    "1. **Increases Dataset Size**: Turns limited data into a much larger, more diverse training set\n",
    "2. **Prevents Overfitting**: Model can't memorize the data when it looks different each time\n",
    "3. **Improves Robustness**: Helps the model handle noisy or varied real-world data during inference\n",
    "4. **Better Generalization**: Model learns the underlying patterns rather than specific examples\n",
    "5. **Acts as Regularization**: Similar to dropout, reduces the gap between training and validation performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_waveform(waveform):\n",
    "    \"\"\"Apply random augmentations to waveform\"\"\"\n",
    "    augmented = waveform.copy()\n",
    "    \n",
    "    # Time shift (50% chance)\n",
    "    if np.random.random() < 0.5:\n",
    "        shift = np.random.randint(-10, 10)\n",
    "        augmented = np.roll(augmented, shift)\n",
    "    \n",
    "    # Add noise (70% chance)\n",
    "    if np.random.random() < 0.7:\n",
    "        noise = np. random.normal(0, 0.02, len(augmented))\n",
    "        augmented += noise\n",
    "    \n",
    "    # Amplitude scaling (50% chance)\n",
    "    if np.random.random() < 0.5:\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "        augmented *= scale\n",
    "    \n",
    "    # Invert (20% chance)\n",
    "    if np.random.random() < 0.2:\n",
    "        augmented = -augmented\n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedAugmentedDataset(Dataset):\n",
    "    def __init__(self, X, y, expansion_factor=3, apply_random_aug=True):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.apply_random_aug = apply_random_aug\n",
    "        self.original_size = len(X)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Dataset is expansion_factor times larger\n",
    "        return self.original_size * self.expansion_factor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Map to original index\n",
    "        original_idx = idx % self.original_size\n",
    "        \n",
    "        waveform = self.X[original_idx]\n",
    "        label = self.y[original_idx]\n",
    "        \n",
    "        # If not the first copy (idx >= original_size), apply augmentation\n",
    "        if idx >= self.original_size and self.apply_random_aug:\n",
    "            wave_np = waveform.squeeze(0).numpy()\n",
    "            wave_np = augment_waveform(wave_np)\n",
    "            waveform = torch.tensor(wave_np, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        return waveform, label\n",
    "\n",
    "# Extract training indices\n",
    "train_indices = train_dataset.indices\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "# Create expanded training dataset (3x larger with random augmentation)\n",
    "train_dataset_expanded = ExpandedAugmentedDataset(\n",
    "    X_train, y_train, \n",
    "    expansion_factor=2,      # 2x larger (original + 1x augmented)\n",
    "    apply_random_aug=True    # Apply random augmentation\n",
    ")\n",
    "\n",
    "\n",
    "# Validation/test unchanged\n",
    "train_loader_expanded = DataLoader(train_dataset_expanded, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader= train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model with residual connections\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=model, \n",
    "    train_loader= train_loader_expanded, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "Transfer learning is a powerful technique where a model trained on one task is reused as the starting point for a model on a different but related task. Instead of training a neural network from scratch with randomly initialized weights, we start with weights that have already learned useful features from a large dataset. \n",
    "\n",
    "Transfer learning involves taking a pre-trained model (trained on a large dataset like ImageNet with millions of images) and adapting it to your specific task. The key idea is that the features learned on the original task are generalizable and useful for the new task.\n",
    "\n",
    "**Common workflow:**\n",
    "1. Start with a model pre-trained on a large dataset (e.g., ResNet trained on ImageNet)\n",
    "2. Remove the final classification layer(s)\n",
    "3. Add new layer(s) suited to your task\n",
    "4. Fine-tune the model on your dataset\n",
    "\n",
    "There are two main approaches:\n",
    "1. **Feature Extraction (Frozen Base)**: Use the pre-trained model as a fixed feature extractor. Freeze the pre-trained layers and only train the new layers you added.\n",
    "\n",
    "2. **Fine-Tuning**: Unfreeze some or all of the pre-trained layers and train them with a low learning rate, along with the new layers.\n",
    "\n",
    "### Why Does Transfer Learning Work?\n",
    "\n",
    "1. **Hierarchical Feature Learning**: Neural networks learn features in a hierarchy: \n",
    "   - **Early layers**: Learn general, low-level features (edges, textures, simple patterns)\n",
    "   - **Middle layers**: Learn mid-level features (shapes, object parts)\n",
    "   - **Late layers**:  Learn high-level, task-specific features (specific object classes)\n",
    "\n",
    "2. **Knowledge Transfer**: Features learned on one task (e.g., recognizing cats and dogs) are often useful for related tasks (e.g., recognizing wildlife)\n",
    "\n",
    "3. **Less Data Required**: Pre-trained models have already learned useful representations, so you need less data to train for your specific task\n",
    "\n",
    "4. **Faster Training**: Starting from good weights means faster convergence than random initialization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "dropout_rate_conv = 0.1\n",
    "dropout_rate_lin = 0.3\n",
    "\n",
    "base_model = CNN_with_dropout(num_classes=3, dropout_rate_conv=dropout_rate_conv, dropout_rate_lin=dropout_rate_lin).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.01)\n",
    "scheduler = None\n",
    "\n",
    "# Train the model without\n",
    "train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
    "    model=base_model, \n",
    "    train_loader=train_loader, \n",
    "    valid_loader=valid_loader, \n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler, \n",
    "    criterion=criterion, \n",
    "    n_epochs=n_epochs, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the pre-trained model\n",
    "torch.save(base_model.state_dict(), 'base_model_3_classes.pth')\n",
    "print(\"\\n✓ Base model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sawtooth(length, fs):\n",
    "    t = np.linspace(0, 1, length, endpoint=False)\n",
    "    freq = np.random.uniform(1, 10)\n",
    "    amp = np.random.uniform(0.5, 1.5)\n",
    "    phi = np.random.uniform(0, 2*np.pi)\n",
    "    y = amp * signal.sawtooth(2 * np.pi * freq * t + phi, 0)  # width=0 for sawtooth\n",
    "    noise = np.random.normal(0, 0.05, length)\n",
    "    return y + noise\n",
    "\n",
    "def generate_pulse(length, fs):\n",
    "    t = np.linspace(0, 1, length, endpoint=False)\n",
    "    freq = np.random. uniform(1, 10)\n",
    "    amp = np.random.uniform(0.5, 1.5)\n",
    "    phi = np.random.uniform(0, 2*np.pi)\n",
    "    y = amp * signal.square(2 * np.pi * freq * t + phi, duty=0.25)  # 25% duty cycle\n",
    "    noise = np.random.normal(0, 0.05, length)\n",
    "    return y + noise\n",
    "\n",
    "def generate_extended_waveform(wave_type, length, fs):\n",
    "    if wave_type == 'sine':\n",
    "        return generate_waveform('sine', length, fs)\n",
    "    elif wave_type == 'triangle':\n",
    "        return generate_waveform('triangle', length, fs)\n",
    "    elif wave_type == 'square':\n",
    "        return generate_waveform('square', length, fs)\n",
    "    elif wave_type == 'sawtooth': \n",
    "        return generate_sawtooth(length, fs)\n",
    "    elif wave_type == 'pulse': \n",
    "        return generate_pulse(length, fs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown wave type: {wave_type}\")\n",
    "\n",
    "# Generate extended dataset (5 classes)\n",
    "wave_types_extended = ['sine', 'triangle', 'square', 'sawtooth', 'pulse']\n",
    "X_extended = []\n",
    "y_extended = []\n",
    "\n",
    "for idx, wave in enumerate(wave_types_extended):\n",
    "    for _ in range(200):  # 200 samples per class\n",
    "        waveform = generate_extended_waveform(wave, length=128, fs=128)\n",
    "        X_extended.append(waveform)\n",
    "        y_extended.append(idx)\n",
    "\n",
    "X_extended = np.array(X_extended)\n",
    "y_extended = np.array(y_extended)\n",
    "\n",
    "print(f\"\\nExtended dataset:  {len(X_extended)} samples, {len(wave_types_extended)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare extended dataset\n",
    "dataset_extended = WaveformDataset(X_extended, y_extended)\n",
    "train_size_ext = int(0.6 * len(dataset_extended))\n",
    "valid_size_ext = int(0.2 * len(dataset_extended))\n",
    "test_size_ext = len(dataset_extended) - train_size_ext - valid_size_ext\n",
    "\n",
    "train_dataset_ext, valid_dataset_ext, test_dataset_ext = random_split(\n",
    "    dataset_extended, [train_size_ext, valid_size_ext, test_size_ext]\n",
    ")\n",
    "\n",
    "train_loader_ext = DataLoader(train_dataset_ext, batch_size=32, shuffle=True)\n",
    "valid_loader_ext = DataLoader(valid_dataset_ext, batch_size=32)\n",
    "test_loader_ext = DataLoader(test_dataset_ext, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningCNN(nn.Module):\n",
    "    def __init__(self, base_model, num_new_classes=5):\n",
    "        super().__init__()\n",
    "        # Copy layers from base model\n",
    "        self.conv1 = base_model.conv1\n",
    "        self.conv2 = base_model.conv2\n",
    "        self.pool = base_model.pool\n",
    "        self.fc1 = base_model.fc1\n",
    "        \n",
    "        # Replace final layer for new number of classes\n",
    "        self.fc2 = nn.Linear(64, num_new_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x. size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze_feature_layers(self):\n",
    "        for param in self. conv1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.conv2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.fc1.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Load base model and create transfer learning model\n",
    "base_model_loaded = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "base_model_loaded.load_state_dict(torch.load('base_model_3_classes.pth'))\n",
    "\n",
    "transfer_model = TransferLearningCNN(base_model_loaded, num_new_classes=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze feature layers\n",
    "transfer_model.freeze_feature_layers()\n",
    "\n",
    "# Train only the new final layer\n",
    "optimizer_transfer = optim.Adam(filter(lambda p: p.requires_grad, transfer_model.parameters()), lr=0.01)\n",
    "\n",
    "train_losses_transfer, valid_losses_transfer, _, _ = train_model(\n",
    "    model=transfer_model,\n",
    "    train_loader=train_loader_ext,\n",
    "    valid_loader=valid_loader_ext,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer_transfer,\n",
    "    n_epochs=50,\n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
