{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d238cf5a",
   "metadata": {},
   "source": [
    "# Lesson 3: Convolutional layers\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. An introduction to convolution\n",
    "2. Defining a convolutional neural network in Pytorch for audio classification\n",
    "3. An explanation on how to use different devices (CPU, CUDA, MPS, etc.) for computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad1a63",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "In purely mathematical terms, convolution is an operation derived from two functions through integration, which expresses how the shape of one function is modified by the other. In discrete terms, the convolution of a larger signal with a smaller one can be understood as a form of _filtering_. The smaller signal, often called a _kernel_ or _filter_, slides over the larger signal, and at each position, we compute a local weighted sum of the overlapping elements. Formally, for a discrete 1D signal, the convolution operator $\\star$ is defined as\n",
    "\n",
    "$$\n",
    "(f \\star g)[n] = \\sum_{m=-M}^{M} f[n - m] \\, g[m].\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b316a",
   "metadata": {},
   "source": [
    "## A simple convolution in Numpy\n",
    "\n",
    "In the context of audio processing, convolution can be used to _filter_ a sound signal, modifying its frequency content. For example, a low-pass finite impulse response (FIR) filter allows low-frequency components of a signal to pass through while attenuating higher frequencies. By convolving an audio signal with the FIR filter coefficients, we effectively smooth the signal, removing rapid fluctuations corresponding to high-frequency noise.\n",
    "\n",
    "\n",
    "The behavior of the convolution is determined by several parameters:\n",
    "- **filter length (M)**: the number of coefficients in the FIR filter, which determines the amount of smoothing  \n",
    "- **padding**: zeros can be added at the start and end of the signal to control the output length\n",
    "\n",
    "Convolution is fundamental in signal processing and machine learning, allowing us to extract features, remove noise, or apply effects. In neural networks for audio, convolutional layers operate on raw waveforms or spectrograms, learning patterns in time or frequency automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c84ea",
   "metadata": {},
   "source": [
    "In the first example we will convolve an audio sample with a low pass filter using numpy. For that we will go through the following three steps:\n",
    "1. Pad the signal to control the output size (using mode='same' in `np.convolve` achieves this automatically).\n",
    "2. Convolve the (padded) signal with the filter\n",
    "3. Downsample the output using a stride to reduce its length and control computational cost, a concept that becomes especially important when working with CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748902f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sounddevice as sd\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1592bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate, audio = wavfile.read(\"recources/_audio/foot_steps.wav\")\n",
    "\n",
    "# In case the audio is stereo or multichannel only the first channel is used\n",
    "if audio.ndim > 1:\n",
    "    audio = audio[:,0]\n",
    "    \n",
    "# Normalize the audio\n",
    "audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "sd.play(audio,sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6e7e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple FIR filter (low-pass)\n",
    "M = 11\n",
    "filter_IR = signal.firwin(M, 200, fs=sample_rate)\n",
    "\n",
    "down_factor = 2 # downsampling factor\n",
    "\n",
    "# To avoid ambiguity\n",
    "audio = audio[:len(audio) // down_factor] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd4d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero padding\n",
    "pad = M - 1\n",
    "\n",
    "# Manual zero padding (same as np.pad)\n",
    "audio_padded = np.concatenate([np.zeros(int(pad)), audio, np.zeros(int(pad))])\n",
    "\n",
    "# Convolution in numpy\n",
    "y_conv = np.convolve(audio_padded, filter_IR, mode='valid')\n",
    "\n",
    "# Downsampling\n",
    "y_down = y_conv[::down_factor]\n",
    "\n",
    "sd.play(y_down,sample_rate/down_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2314b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "x = np.arange(len(audio)) / sample_rate \n",
    "x_conv = np.arange(len(y_down)) / (sample_rate / down_factor)\n",
    "\n",
    "plt.figure(figsize=(8, 5), dpi=120)\n",
    "plt.plot(x[7000:9000], audio[7000:9000], color='C0', lw=1.2, alpha=0.8, label=\"Original audio\")\n",
    "plt.plot(x_conv[3500:4500], y_down[3500:4500], color='C1', lw=2, alpha=0.8, label=\"Audio after convolution\")\n",
    "\n",
    "plt.title(\"Audio Signal vs. Audio Signal after Convolution (Filtered)\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384278fc",
   "metadata": {},
   "source": [
    "## A transposed convolution in Numpy\n",
    "\n",
    "The transposed convolution is a complementary operation that allows us to increase the length of a signal or upsample it. In neural networks, it is commonly used in generative models or audio synthesis, where we want to reconstruct or expand a signal from a lower-resolution representation.\n",
    "\n",
    "Conceptually, a transposed convolution can be thought of as “reversing” the effect of a normal convolution. Instead of sliding a filter over an input and producing a smaller (or same-size) output, we insert zeros between the input samples (according to the upsampling factor) and then apply a convolution-like operation to spread the input values over a larger output. This effectively increases the signal length while applying the learned filter weights to produce a smooth, meaningful output.\n",
    "\n",
    "In audio applications, a simple example would be upsampling a low-resolution waveform. In this example the downsampled audio signal from before will be upsampled again to have the same samplerate as the original audio signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aece61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposed convolution\n",
    "up_factor = 2 # upsampling factor\n",
    "\n",
    "# Upsampling\n",
    "y_up = np.zeros(len(y_down) * up_factor)\n",
    "y_up[::up_factor] = y_down  # insert zeros\n",
    "\n",
    "# Zero padding\n",
    "pad = M - 1\n",
    "y_up_padded = np.concatenate([np.zeros(int(pad)), y_up, np.zeros(int(pad))])\n",
    "\n",
    "# Convolve with same filter\n",
    "y_trans = np.convolve(y_up_padded, filter_IR, mode='valid')\n",
    "y_trans = y_trans / np.max(np.abs(y_trans))\n",
    "\n",
    "sd.play(y_trans,sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8045eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "x_conv = np.arange(len(y_down)) / (sample_rate / down_factor)  \n",
    "x_trans = np.arange(len(y_trans)) / sample_rate\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(8, 5), dpi=120)\n",
    "plt.plot(x[7000:9000], audio[7000:9000], color='C0', lw=1.2, alpha=0.8, label=\"Original Audio\")\n",
    "plt.plot(x_conv[3500:4500], y_down[3500:4500], color='C1', lw=1.2, alpha=0.8, label=\"convolved Audio\")\n",
    "plt.plot(x_trans[7000:9000], y_trans[7000:9000], color='C3', lw=2, alpha=0.8, label=\"Audio after transposed convolution\")\n",
    "\n",
    "\n",
    "plt.title(\"Audio Signal vs. Audio Signal after Transposed Convolution\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba8ebe",
   "metadata": {},
   "source": [
    "Of course, it will be impossible to reconstruct the signal exactly, since data is lost in the downsampling process after convolution and cannot be recovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffaf244",
   "metadata": {},
   "source": [
    "## Convolution with PyTorch\n",
    "\n",
    "Now we will do the same using PyTorch. In this example we will do the convolution with the same audio file and the same filter using PyTorchs `torch.nn.functional.conv1d`.\n",
    "\n",
    "One of the advantages of PyTorch is that it can run computations on different hardware devices. This allows us to take advantage of faster processors when available.\n",
    "\n",
    "On machines with an NVIDIA GPU, PyTorch can use **CUDA** to perform operations on the graphics card, which is highly efficient for large numerical computations and deep learning. On Apple Silicon (M1/M2) or other Macs with newer GPUs, PyTorch can use **MPS** to run the same operations on the GPU. If no compatible GPU is available, PyTorch will automatically fall back to using the **CPU**, which is slower but always available.\n",
    "\n",
    "We can easily check which device is available and move our data and models to that device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cf391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check which device is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"MPS is available: {torch.backends.mps.is_available()}\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(f\"Cuda is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "print(rf\"Device that will be trained on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d614f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_tensor = torch.tensor(audio, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "filter_IR = torch.tensor(signal.firwin(12, 200, fs=sample_rate), dtype=torch.float32, device=device)\n",
    "kernel = filter_IR.flip(0).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(f\"kernel is on device: {kernel.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73499d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_y_down = F.conv1d(audio_tensor, kernel, stride=down_factor, padding=(M-1,))\n",
    "\n",
    "print(f\"tensor_y_down is on device {tensor_y_down.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd04970",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_y_down_squeezed = tensor_y_down.squeeze(0).squeeze(0)\n",
    "y_down_np = tensor_y_down_squeezed.detach().cpu().numpy()\n",
    "\n",
    "y_dif = np.mean((y_down - y_down_np)**2)\n",
    "print(rf\"The Mean Squared Error of the difference between the convolution with numpy and with pytorch is: y_dif = {np.round(y_dif,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(audio)) / sample_rate\n",
    "x_conv_np = np.arange(len(y_down_np)) / (sample_rate / down_factor)\n",
    "\n",
    "plt.figure(figsize=(8, 5), dpi=120)\n",
    "plt.plot(x[7000:9000], audio[7000:9000], color='C0', lw=1.2, alpha=0.8, label=\"Original Audio\")\n",
    "plt.plot(x_conv_np[3500:4500], y_down_np[3500:4500], color='C1', lw=2, alpha=0.8, label=\"Filtered Audio (Low-pass)\")\n",
    "\n",
    "plt.title(\"Audio Signal vs. Audio Signal after Convolution (Filtered)\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Time (seconds)\", fontsize=12)\n",
    "plt.ylabel(\"Amplitude\", fontsize=12)\n",
    "plt.legend(frameon=False, fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a391131",
   "metadata": {},
   "source": [
    "## Transposed convolution with PyTorch\n",
    "\n",
    "Now we will implement the trnasposed convolution in PyTorch too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356518ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling with pytorch\n",
    "tensor_y_up = F.conv_transpose1d(tensor_y_down, kernel, stride=up_factor)\n",
    "\n",
    "tensor_y_up_squeezed = tensor_y_up.squeeze(0).squeeze(0)\n",
    "y_up_np = tensor_y_up_squeezed.detach().cpu().numpy()\n",
    "\n",
    "y_dif = np.mean( (y_trans - y_up_np)**2)\n",
    "print(rf\"The Mean Squared Error of the difference between the transposed convolution with numpy and with pytorch is: y_dif = {np.round(y_dif,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff351d1",
   "metadata": {},
   "source": [
    "# Waveform classification with a convolutional neureal network\n",
    "\n",
    "In this exercise, we build and train a simple 1D Convolutional Neural Network (CNN) to classify different waveform types, such as sine, triangle, and square waves. Each waveform is represented as a one-dimensional signal of length 128.\n",
    "\n",
    "We will:\n",
    "1. Generate a synthetic dataset of waveforms.\n",
    "2. Prepare the data for PyTorch using custom Dataset and DataLoader classes.\n",
    "3. Define and train a 1D CNN for waveform classification.\n",
    "4. Evaluate the model performance with a test set and visualize the training process and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac85c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define devices:\n",
    "if torch.backends.mps.is_available():\n",
    "    device_mps = torch.device(\"mps\")  \n",
    "if torch.cuda.is_available():\n",
    "    device_cuda = torch.device(\"cuda\")\n",
    "\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74c93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def generate_waveform(wave_type, length, fs):\n",
    "    t = np.linspace(0, 1, length, endpoint=False)\n",
    "    freq = np.random.uniform(1, 10)      # random frequency 1-10Hz\n",
    "    amp = np.random.uniform(0.5, 1.5)    # random amplitude\n",
    "    phi = np.random.uniform(0, 2*np.pi)  # random phases\n",
    "\n",
    "    if wave_type == 'sine':\n",
    "        y = amp * np.sin(2 * np.pi * freq * t + phi)\n",
    "    elif wave_type == 'triangle':\n",
    "        y = amp * signal.sawtooth(2 * np.pi * freq * t + phi, 0.5)\n",
    "    elif wave_type == 'square':\n",
    "        y = amp * signal.square(2 * np.pi * freq * t + phi)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown wave type\")\n",
    "    \n",
    "    # optional noise\n",
    "    noise = np.random.normal(0, 0.05, length)\n",
    "    y += noise\n",
    "    return y\n",
    "\n",
    "# Parameters\n",
    "num_samples = 200      # samples per waveform type\n",
    "length = 128           # number of points per waveform\n",
    "fs = 128               # sampling frequency\n",
    "\n",
    "# Generate dataset in memory\n",
    "wave_types = ['sine', 'triangle', 'square']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for idx, wave in enumerate(wave_types):\n",
    "    for _ in range(num_samples):\n",
    "        waveform = generate_waveform(wave, length, fs)\n",
    "        X.append(waveform)\n",
    "        y.append(idx)  # class label: 0=sine, 1=triangle, 2=square\n",
    "\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)   # shape: (600, 128)\n",
    "y = np.array(y)   # shape: (600,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create our data set with torch.utils.data.Dataset\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # add channel dimension\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "dataset = WaveformDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea55b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our CNN model for waveform classification\n",
    "class Waveform_Classification_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5,  stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(224, 64) \n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "device = device_cpu\n",
    "\n",
    "model = Waveform_Classification_CNN(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(model.parameters(),lr=0.01)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "n_epochs = 1000\n",
    "loss_history = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Training time on {device}: {elapsed:.2f} seconds\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, label=\"Training loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f075c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_inputs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_inputs.extend(inputs.cpu().numpy()) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wave_types = ['sine', 'triangle', 'square']\n",
    "n_examples = 6\n",
    "idxs = np.random.choice(len(all_inputs), n_examples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, idx in enumerate(idxs):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.plot(all_inputs[idx][0], color='black')\n",
    "    plt.title(f\"True: {wave_types[all_labels[idx]]}\\nPred: {wave_types[all_preds[idx]]}\",\n",
    "              color=\"green\" if all_labels[idx] == all_preds[idx] else \"red\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d7edb5",
   "metadata": {},
   "source": [
    "We could also implement this task using a fully linear model (a simple feed-forward network)\n",
    "instead of a convolutional one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdf476bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "\n",
    "class Waveform_Classification_Linear(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        # Input: 1 × 128 waveform → flatten to 128 features\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten from (batch, 1, 128) → (batch, 128)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)   # output logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device_cpu  \n",
    "model = Waveform_Classification_Linear(num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.ASGD(model.parameters(), lr=0.01)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 1000\n",
    "loss_history = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Training time on {device}: {elapsed:.2f} seconds\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(loss_history, label=\"Training loss\")\n",
    "plt.title(\"Training Loss (Linear Model)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_inputs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_inputs.extend(inputs.cpu().numpy())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b3596",
   "metadata": {},
   "source": [
    "However, such a model would not achieve the same accuracy as the CNN, because it treats\n",
    "every input value as an independent feature and ignores the local structure of the waveform.\n",
    "Convolutional layers, in contrast, learn spatial and temporal relationships — for example,\n",
    "recurring shapes, edges, or transitions in the signal, by applying shared filters across\n",
    "the entire sequence. So if there is a phase shift in the waveform the linear model won't recognise it anymore.\n",
    "\n",
    "As a result, while a fully linear network can technically classify the data,\n",
    "it usually performs worse and requires many more parameters to capture the same patterns.\n",
    "CNNs are therefore much better suited for waveform or time-series classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311e838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
