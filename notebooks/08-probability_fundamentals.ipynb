{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lesson 7: Probability Fundamentals\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Probability distributions\n",
    "2. Sampling from distributions\n",
    "3. Bayes' Theorem\n",
    "4. Maximum Likelihood\n",
    "\n",
    "This script was inspired by the Creative Machine Learning course of Philippe Esling and the CS Build Week 1 for Data Science by John Dailey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.distributions as distribution\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Coin toss\n",
    "\n",
    "Let's start with the most intuitive random process: a coin flip. In the example below, we simulate multiple coin tosses using Python's random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = ['heads', 'tails']\n",
    "trials = 5\n",
    "for t in range(trials):\n",
    "    # Get a random 0 or 1\n",
    "    toss = random.randint(0,1)\n",
    "    # Print the result of our toss\n",
    "    print('Coin is tossed on ' + sample_space[toss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Each time we run this code, we get a different outcome due to randomness. But what happens when we repeat the experiment many times? By sampling thousands of coin flips, patterns emerge. We can observe the **distribution** that underlies the random process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10.5, 2.5), dpi=500, tight_layout=True)\n",
    "# loop through 5,10,50,100,1000,10000 trials\n",
    "for t_id, trials in enumerate([5,10,50,100,1000,10000]):\n",
    "    # Keep track\n",
    "    heads_or_tails = [0, 0]\n",
    "    for t in range(trials):\n",
    "        # Get a random 0 or 1\n",
    "        toss = random.randint(0,1)\n",
    "        # Increment the list element corresponding to the toss result\n",
    "        heads_or_tails[toss] += 1\n",
    "    # Show a pie chart of the results\n",
    "    plt.subplot(1, 6, t_id+1)\n",
    "    plt.pie(heads_or_tails, colors=['turquoise', 'purple'], labels=['heads', 'tails'])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "### Bernoulli distribution\n",
    "\n",
    "A very simple example of discrete distribution is the **Bernoulli** distribution. With this distribution, we can model a coin flip, if it has equal probability. More formally, a Bernoulli distribution is defined as\n",
    "\n",
    "$$ \n",
    "Bernoulli(x)= p^x (1-p)^{(1-x)} \n",
    "$$\n",
    "\n",
    "with $p$ controlling the probability of the two classes. That means that fair coin should have $p=0.5$. If we throw the coin a very large number of times, we hope to see on average an equal amount of _heads_ and _tails_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = distribution.Bernoulli(0.5)\n",
    "# bernoulli = distribution.Bernoulli(0.2)\n",
    "samples = bernoulli.sample((10000,))\n",
    "print(samples[:10])\n",
    "# figure(640, 480)\n",
    "sns.displot(samples)\n",
    "plt.title(\"Samples from a Bernoulli (coin toss)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_space = ['heads', 'tails']\n",
    "samples = bernoulli.sample((10, ))\n",
    "for s in samples:\n",
    "    print('Coin is tossed on ' + sample_space[int(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Normal distribution\n",
    "\n",
    "The same ideas apply to *continuous* random variables. The Normal (or Gaussian) distribution is perhaps the most important distribution in statistics and machine learning. Below, two normal distibutions are plottet using `pytorch.distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 6, 1000)\n",
    "\n",
    "# Based on a normal\n",
    "n_1 = distribution.Normal(0, 1)\n",
    "n_2 = distribution.Normal(1, 1.5)\n",
    "\n",
    "# Obtain some samples\n",
    "samples = n_1.sample((1000, ))\n",
    "\n",
    "# PyTorch's distribution module only provides log_prob(), not a direct probability density function. To get the density, we exponentiate the log_prob.\n",
    "density_1 = torch.exp(n_1.log_prob(torch.Tensor(x))).numpy()\n",
    "density_2 = torch.exp(n_2.log_prob(torch.Tensor(x))).numpy()\n",
    "\n",
    "# Plot both samples and density\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Compare both distributions\n",
    "plt.plot(x, density_1)\n",
    "plt.fill_between(x, density_1, 0, alpha=0.5, label='N(0,1)')\n",
    "plt.plot(x, density_2)\n",
    "plt.fill_between(x, density_2, 0, alpha=0.5, label='N(1,1.5)')\n",
    "plt.legend()\n",
    "plt.title(\"Normal Distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Here, we rely on the [PyTorch distributions module](https://pytorch.org/docs/stable/_modules/torch/distributions/), which is defined in `torch.distributions`. Inside this toolbox, we can already find some of the major probability distributions that we are used to deal with:\n",
    "\n",
    "- ``distribution.Normal``\n",
    "- ``distribution.Bernoulli``\n",
    "- ``distribution.Beta``\n",
    "- ``distribution.Gamma``\n",
    "- ``distribution.LogNormal``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Sampling from a distribution\n",
    "\n",
    "Once we know a probability distribution, we can sample from it. For example, sampling repeatedly from a Gaussian distribution produces values that perfectly match the Gaussian shape. The trick is this: to sample from any distribution, we need to invert the Cumulative Distribution Function (CDF) of this distribution. The CDF can be obtained by integrating the probability density function (PDF) and tells us the cumulative probability up to any point x; inverting it lets us map uniform random numbers directly to samples from our target distribution.\n",
    "\n",
    "Below, we demonstrate this with the exponential distribution, comparing both the standard NumPy sampling method and the inverse CDF approach.\n",
    "\n",
    "The formula for the probability density function of the exponential distribution is:\n",
    "\n",
    "$$\n",
    "p(x; \\lambda) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0, \\lambda > 0\n",
    "$$\n",
    "\n",
    "We can get the CDF by integrating the PDF:\n",
    "\n",
    "$$\n",
    "F(x; \\lambda) = \\int_0^{x} \\lambda e^{-\\lambda x} dx = 1 - e^{-\\lambda x}\n",
    "$$\n",
    "\n",
    "From here, we can derive the inverse CDF:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F(x; \\lambda) & =  1 - e^{-\\lambda x} \\\\\n",
    "e^{-\\lambda x} & = 1 - F(x; \\lambda) \\\\\n",
    "-\\lambda x & = \\ln(1 - F(x; \\lambda)) \\\\\n",
    "x & = -\\frac{1}{\\lambda} \\ln(1 - F(x; \\lambda))\\\\\n",
    "F(y; \\lambda) & = -\\frac{1}{\\lambda} \\ln(1 - y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now use a uniform random variable `y` to generate samples from the exponential distribution using the inverse CDF formula above.\n",
    "\n",
    "$$\n",
    "u \\sim \\text{Uniform}(0, 1) \\\\\n",
    "x = F(u; \\lambda) = -\\frac{1}{\\lambda} \\ln(1 - u)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "nb_samples = 500\n",
    "nb_bins = 50\n",
    "\n",
    "def exponential_pdf(x, lambda_):\n",
    "    return lambda_ * np.exp(-lambda_ * x)\n",
    "\n",
    "def sample_exponential(lambda_, n):\n",
    "    samples = []\n",
    "    u = np.random.uniform(0, 1, n)  # Generate uniform random number\n",
    "    samples = - (1/lambda_) * np.log(1 - u)      # Apply inverse CDF\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Exponential distribution\n",
    "lambda_ = 0.5\n",
    "exp_dist = distribution.Exponential(lambda_)\n",
    "\n",
    "# Because numpy defines the exponential distribution using the scale parameter B = 1/lambda -> 1/B * exp(-x/B)\n",
    "samples = exp_dist.sample((nb_samples, )).numpy()\n",
    "samples_manual = sample_exponential(lambda_, nb_samples)\n",
    "\n",
    "# Compute the PDF\n",
    "X = np.linspace(0, 15, 1000)\n",
    "X_manual = np.linspace(0, 15, 1000)\n",
    "y = exp_dist.log_prob(torch.Tensor(X)).exp().numpy()\n",
    "y_manual = exponential_pdf(X_manual, lambda_)\n",
    "\n",
    "# Define consistent bin edges for both histograms\n",
    "bin_edges_fixed = np.linspace(0, 15, 51)  # 50 bins from 0 to 15\n",
    "\n",
    "# Display both\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2,1,1)\n",
    "counts, bin_edges, _ = plt.hist(samples, bins=bin_edges_fixed, label='Samples')\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "plt.plot(X, y * bin_width * nb_samples,ls='--',c='r',linewidth=2, label='Exponential PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.xlim(0, 15)\n",
    "plt.ylim(0, 80)\n",
    "plt.xlabel('sampled value')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Exponential distribution sampling using PyTorch')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "counts, bin_edges, _ = plt.hist(samples_manual, bins=bin_edges_fixed, label='Our exponential samples')\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "plt.plot(X_manual, y_manual * bin_width * nb_samples,ls='--',c='r',linewidth=2, label='Exponential PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('sampled value')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xlim(0, 15)\n",
    "plt.ylim(0, 80)\n",
    "plt.title('Exponential distribution sampling using inverse CDF')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Bayes' Theorem\n",
    "\n",
    "Bayes' theorem is a fundamental formula that describes how to update our beliefs based on new evidence. It allows us to calculate the probability of a hypothesis given observed data, expressed as:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
    "\n",
    "where:\n",
    "- $P(A|B)$ is the posterior probability (probability of A given B)\n",
    "- $P(B|A)$ is the likelihood (probability of observing B given A)\n",
    "- $P(A)$ is the prior probability (initial belief about A)\n",
    "- $P(B)$ is the marginal probability (total probability of observing B)\n",
    "\n",
    "This theorem is the foundation for many machine learning algorithms, including naive Bayes classifiers, which we'll explore next.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution. We have the following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(200, 2, centers=2, random_state=2, cluster_std=3.2)\n",
    "\n",
    "train_idx = np.random.choice(len(X), int(0.9 * len(X)), replace=False)\n",
    "test_idx = np.array([i for i in range(len(X)) if i not in train_idx])\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "\n",
    "plt.title(\"Training set\")\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=80, cmap='RdBu', edgecolor='w'); plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We'll build a Gaussian Naive Bayes model from scratch to understand what's happening under the hood. Afterwards we'll use the the scikit-learn implementation to compare our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Bayes' Theorem in Naive Bayes\n",
    "\n",
    "We need an algorithm that learns the mean and variance of each feature for each class, then uses Bayes' theorem to make predictions.\n",
    "\n",
    "As shown before, Bayes' theorem tells us how to classify:\n",
    "\n",
    "$$P(\\text{class} | \\text{data}) = \\frac{P(\\text{data} | \\text{class}) \\cdot P(\\text{class})}{P(\\text{data})}$$\n",
    "\n",
    "This is used in the `predict` method to find the most probable class.\n",
    "\n",
    "## MLE for Gaussian Parameters\n",
    "\n",
    "The mean and std in `stat_info` come from **maximizing the likelihood function**. Here's the derivation:\n",
    "\n",
    "For Gaussian data $x_1, x_2, ..., x_n$ with unknown mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "**Likelihood function:**\n",
    "$$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "**Log-likelihood:**\n",
    "$$\\ln L = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2$$\n",
    "\n",
    "**Maximizing by taking derivatives:**\n",
    "\n",
    "$$\\frac{\\partial \\ln L}{\\partial \\mu} = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu) = 0 \\implies \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$\n",
    "\n",
    "$$\\frac{\\partial \\ln L}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\sum_{i=1}^{n}(x_i - \\mu)^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\hat{\\mu})^2$$\n",
    "\n",
    "This is exactly what `np.mean(feature)` and `np.std(feature)` compute.\n",
    "\n",
    "In conclusion, the Bayes' theorem is used for classification and MLE is used to estimate the Gaussian parameters from training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier Class\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Separate the dataset into a subset of data for each class\n",
    "    def separate_classes(self, X, y):\n",
    "        \"\"\"\n",
    "        Separates the dataset in to a subset of data for each class.\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X - array, list of features\n",
    "        y - list, target\n",
    "        Returns:\n",
    "        A dictionary with y as keys and assigned X as values.\n",
    "        \"\"\"\n",
    "        separated_classes = {}\n",
    "        for i in range(len(X)):\n",
    "            feature_values = X[i]\n",
    "            class_name = y[i]\n",
    "            if class_name not in separated_classes:\n",
    "                separated_classes[class_name] = []\n",
    "            separated_classes[class_name].append(feature_values)\n",
    "        return separated_classes\n",
    "\n",
    "    # Standard deviation and mean are required for the (Gaussian) distribution function\n",
    "\n",
    "    def stat_info(self, X):\n",
    "        \"\"\"\n",
    "        Calculates standard deviation and mean of features.\n",
    "        Parameters:\n",
    "        ------------\n",
    "        X- array , list of features\n",
    "        Returns:\n",
    "        A dictionary with STD and Mean as keys and assigned features STD and Mean as values.\n",
    "        \"\"\"\n",
    "        for feature in zip(*X):\n",
    "            yield {\n",
    "                'std' : np.std(feature),\n",
    "                'mean' : np.mean(feature)\n",
    "            }\n",
    "            \n",
    "    # Required fit method, to train the model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the model.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: array-like, training features\n",
    "        y: list, target variable\n",
    "        Returns:\n",
    "        Dictionary with the prior probability, mean, and standard deviation of each class\n",
    "        \"\"\"\n",
    "\n",
    "        separated_classes = self.separate_classes(X, y)\n",
    "        self.class_summary = {}\n",
    "\n",
    "        for class_name, feature_values in separated_classes.items():\n",
    "            self.class_summary[class_name] = {\n",
    "                'prior_proba': len(feature_values)/len(X),\n",
    "                'summary': [i for i in self.stat_info(feature_values)],\n",
    "            }\n",
    "        return self.class_summary\n",
    "\n",
    "    # Gaussian distribution function\n",
    "\n",
    "    def distribution(self, x, mean, std):\n",
    "        \"\"\"\n",
    "        Gaussian Distribution Function\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x: float, value of feature\n",
    "        mean: float, the average value of feature\n",
    "        stdev: float, the standard deviation of feature\n",
    "        Returns:\n",
    "        A value of Normal Probability\n",
    "        \"\"\"\n",
    "\n",
    "        exponent = np.exp(-(((x-mean)/std)**2)/2)\n",
    "\n",
    "        return exponent/(np.sqrt(2*np.pi)*std)\n",
    "\n",
    "    # Required predict method, to predict the class\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X: array-like, test data set\n",
    "        Returns:\n",
    "        -----------\n",
    "        List of predicted class for each row of data set\n",
    "        \"\"\"\n",
    "\n",
    "        # Maximum a posteriori (MAP)\n",
    "        \n",
    "        MAPs = []\n",
    "\n",
    "        for row in X:\n",
    "            joint_proba = {}\n",
    "            \n",
    "            for class_name, features in self.class_summary.items():\n",
    "                total_features = len(features['summary'])\n",
    "                likelihood = 1\n",
    "\n",
    "                for idx in range(total_features):\n",
    "                    feature = row[idx]\n",
    "                    mean = features['summary'][idx]['mean']\n",
    "                    stdev = features['summary'][idx]['std']\n",
    "                    normal_proba = self.distribution(feature, mean, stdev)\n",
    "                    likelihood *= normal_proba\n",
    "                prior_proba = features['prior_proba']\n",
    "                joint_proba[class_name] = prior_proba * likelihood\n",
    "\n",
    "            MAP = max(joint_proba, key=joint_proba.get)\n",
    "            MAPs.append(MAP)\n",
    "\n",
    "        return np.array(MAPs)\n",
    "    \n",
    "    # equivalent function to scikit-learns predict_proba\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Returns probabilities for each class\"\"\"\n",
    "        probas = []\n",
    "        for row in X:\n",
    "            joint_proba = {}\n",
    "            for class_name, features in self.class_summary.items():\n",
    "                likelihood = 1\n",
    "                for idx in range(len(features['summary'])):\n",
    "                    feature = row[idx]\n",
    "                    mean = features['summary'][idx]['mean']\n",
    "                    stdev = features['summary'][idx]['std']\n",
    "                    likelihood *= self.distribution(feature, mean, stdev)\n",
    "                joint_proba[class_name] = features['prior_proba'] * likelihood\n",
    "            \n",
    "            total = sum(joint_proba.values())\n",
    "            prob_class_0 = joint_proba.get(0) / total\n",
    "            prob_class_1 = joint_proba.get(1) / total\n",
    "            probas.append([prob_class_0, prob_class_1])\n",
    "    \n",
    "        return np.array(probas)\n",
    "    \n",
    "    # Calculate the model's accuracy\n",
    "\n",
    "    def accuracy(self, y_test, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates model's accuracy.\n",
    "        Parameters:\n",
    "        ------------\n",
    "        y_test: actual values\n",
    "        y_pred: predicted values\n",
    "        Returns:\n",
    "        ------------\n",
    "        A number between 0-1, representing the percentage of correct predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        true_true = 0\n",
    "\n",
    "        for y_t, y_p in zip(y_test, y_pred):\n",
    "            if y_t == y_p:\n",
    "                true_true += 1 \n",
    "        return true_true / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Before we build the classifier, let's visualize the distribution of each class. We'll look at the distribution of feature 0 (x-axis) for both classes to see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "seperate_classes = model.separate_classes(X_train, y_train)\n",
    "\n",
    "# Convert lists to NumPy arrays for indexing\n",
    "class_0_data = np.asarray(seperate_classes[0])\n",
    "class_1_data = np.asarray(seperate_classes[1])\n",
    "\n",
    "# Create a 1D visualization using feature 0 (x-coordinate)\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot 1: Histogram + fitted Gaussian for feature 0\n",
    "feature_idx = 0\n",
    "plt.hist(class_0_data[:, feature_idx], bins=20, alpha=0.5, color='blue', density=True, label='Class 0')\n",
    "plt.hist(class_1_data[:, feature_idx], bins=20, alpha=0.5, color='red', density=True, label='Class 1')\n",
    "\n",
    "# Fit Gaussians to each class\n",
    "mean_0 = np.mean(class_0_data[:, feature_idx])\n",
    "std_0 = np.std(class_0_data[:, feature_idx])\n",
    "mean_1 = np.mean(class_1_data[:, feature_idx])\n",
    "std_1 = np.std(class_1_data[:, feature_idx])\n",
    "\n",
    "# Plot the Gaussian curves\n",
    "x_min = min(class_0_data[:, feature_idx].min(), class_1_data[:, feature_idx].min()) - 2\n",
    "x_max = max(class_0_data[:, feature_idx].max(), class_1_data[:, feature_idx].max()) + 2\n",
    "x_range = np.linspace(x_min, x_max, 200)\n",
    "\n",
    "gaussian_0 = norm.pdf(x_range, mean_0, std_0)\n",
    "gaussian_1 = norm.pdf(x_range, mean_1, std_1)\n",
    "\n",
    "plt.plot(x_range, gaussian_0, 'b-', linewidth=2, label=f'Class 0 Gaussian\\nμ={mean_0:.1f}, σ={std_0:.1f}')\n",
    "plt.plot(x_range, gaussian_1, 'r-', linewidth=2, label=f'Class 1 Gaussian\\nμ={mean_1:.1f}, σ={std_1:.1f}')\n",
    "plt.xlabel('Feature 0 value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Gaussian Models for Each Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Next, we plot the data and see where the decision boundary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "\n",
    "# predict the classification probabilities on a grid\n",
    "xlim = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "ylim = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 71), np.linspace(ylim[0], ylim[1], 81))\n",
    "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Take the probability of class 1\n",
    "Z = Z[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "# Switch from red to blue with red being 0. and blue being 1.\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu', edgecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The decision boundary is slightly curved rather than straight. This happens because Gaussian Naive Bayes compares two Gaussian probability distributions, and the math results in a quadratic equation—producing parabolas, ellipses, or hyperbolas as boundaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We can also try to predict the test values and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "model.accuracy(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "To verify our model we will compare it to the `GaussianNB` provided by scikit-learn to see if our results match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Another advantage of this Bayesian approach is that it provides **probability estimates** for each prediction. Instead of just saying \"this is class 0,\" it can say \"85% confident it's class 0, 15% confident it's class 1.\" We can access these probabilities using the ``predict_proba`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[:10])\n",
    "yprob = model.predict_proba(X_test)\n",
    "yprob[:10].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "We'll use gradient descent to recover the true Bernoulli parameter $p$ from samples alone. Starting with a random guess, we minimize the negative log-likelihood until the estimated parameter converges to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli = distribution.Bernoulli(0.2)\n",
    "\n",
    "sample = bernoulli.sample((10000, ))\n",
    "\n",
    "sns.displot(sample)\n",
    "plt.title(\"Samples from a Bernoulli (p = .2)\")\n",
    "torch.mean(sample)\n",
    "print(sample[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x = torch.Tensor(sample)\n",
    "p = Variable(torch.rand(1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Now we can use our estimator to gradually compute the Maximum Likelihood, in order to uncover the estimated probability of the underlying distribution, solely based on observing the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "for t in range(50):\n",
    "\n",
    "    # Negative log-likelihood\n",
    "    NLL = -torch.sum(torch.log(x*p + (1-x)*(1-p) + 1e-10) )\n",
    "\n",
    "    # Automatic differentiation\n",
    "    NLL.backward()\n",
    "    p.data\n",
    "    p.grad.data\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print(\"loglik = %.4f - p = %.4f - dL/dp = %.4f\"%(NLL.data.numpy(), p.data.numpy(), p.grad.data.numpy()))   \n",
    "    p.data -= learning_rate * p.grad.data\n",
    "    p.grad.data.zero_()\n",
    "    \n",
    "print('Final probability p =', p.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Try to sample manually from the beta distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "nb_samples = 500\n",
    "nb_bins = 50\n",
    "\n",
    "def sample_beta(a, b, N):\n",
    "\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "a = 0.6\n",
    "b = 1.5\n",
    "\n",
    "samples = np.random.beta(a, b, nb_samples)\n",
    "samples_manual = sample_beta(a, b, nb_samples)\n",
    "\n",
    "# Compute the PDF\n",
    "X = np.linspace(0, 1, 100)\n",
    "y_1 = beta.pdf(X, a, b) * (nb_samples / nb_bins) * (np.max(samples) * 1.5)\n",
    "\n",
    "## calculate the pdf of beta manually (X_manual and y_manual) ##\n",
    "\n",
    "# Display both\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2,1,1)\n",
    "counts, bin_edges, _ = plt.hist(samples, 50, label='Samples')\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "plt.plot(X, y_1, ls='--',c='r',linewidth=2, label='Exponential PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.xlim(0, max(samples))\n",
    "plt.xlabel('sampled value')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Exponential distribution sampling using numpy')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "counts, bin_edges, _ = plt.hist(samples_manual, 50, label='Our exponential samples')\n",
    "bin_width = bin_edges[1] - bin_edges[0]\n",
    "plt.plot(X_manual, y_manual,ls='--',c='r',linewidth=2, label='Exponential PDF')\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('sampled value')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.xlim(0, max(samples_manual))\n",
    "plt.title('Exponential distribution sampling using inverse CDF')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
