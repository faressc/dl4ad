{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d43353d",
   "metadata": {},
   "source": [
    "# Lesson 9: Latent Models\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Gaussian Mixture Models\n",
    "2. EM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.distributions as distribution\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ccf9",
   "metadata": {},
   "source": [
    "## Create data\n",
    "\n",
    "We begin by generating a synthetic dataset with 4 clusters using `make_blobs` from scikit-learn. The data points are scattered across two features, forming distinct groups that we'll try to model using latent variables. But for now we don't know how to classify the points in the best way yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.80, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # ✅ Better\n",
    "ax.scatter(X[:, 0], X[:, 1], c='coral', s=50, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.grid(color='black', linewidth=0.5)\n",
    "ax.set_title('Dataset with 4 unknown classes', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-4, 4.5)\n",
    "ax.set_ylim(-2, 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0538a4",
   "metadata": {},
   "source": [
    "## Fitting with a simple Gaussian\n",
    "\n",
    "We can try to fit the data with one gaussian distribution, but this won't give us a lot of insight, since we actually want to classify the data into 4 clusters. A single Gaussian distribution is insufficient for modeling multi-cluster data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Manually compute mean and covariance\n",
    "mu = X.mean(axis=0)\n",
    "sigma = np.cov(X.T)\n",
    "\n",
    "# Create grid for plotting\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 2\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Compute Gaussian PDF on grid\n",
    "pos = np.dstack((xx, yy))\n",
    "rv = multivariate_normal(mu, sigma)\n",
    "z = rv.pdf(pos)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], c='coral', s=40, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.contourf(xx, yy, z, cmap='Oranges', alpha=1)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title('Simple Gaussian Distribution Fit to Data', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-4, 4.5)\n",
    "ax.set_ylim(-2, 11)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f008f",
   "metadata": {},
   "source": [
    "By fitting one multivariate Gaussian to the entire dataset and visualizing its probability density function as a contour plot, we can see that it fails to capture the distinct clusters. The single Gaussian averages over all clusters, resulting in a poor representation of the data's true structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0303340",
   "metadata": {},
   "source": [
    "## Gaussian mixture models\n",
    "\n",
    "The solution is to use multiple gaussian distributions to model complex data. Each Gaussian component can capture one cluster. First we have to initialize 4 gaussian distributions and then we have to optimize the mean and the variance of these distributions to correctly fit the clusters. The good thing about GMMs is that they allow for distribution functions to be shaped elliptical (with K-Means, this is not possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gmm_step(X, means, covariances, weights, responsibilities=None, title=\"\", classes=False):\n",
    "    \"\"\"\n",
    "    Visualize GMM components and optionally responsibilities\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Create grid for plotting\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    pos = np.dstack((xx, yy))\n",
    "    \n",
    "    # # Compute mixture PDF\n",
    "    z = np.zeros_like(xx)\n",
    "    \n",
    "    for k in range(len(means)):\n",
    "        rv = multivariate_normal(means[k], covariances[k])\n",
    "        z += weights[k] * rv.pdf(pos)\n",
    "    \n",
    "    colors = ['gold', 'darkorange', 'orangered', 'red']\n",
    "\n",
    "    if classes is True:\n",
    "        labels = np.argmax(responsibilities, axis=1)\n",
    "        # Scatter plot each cluster with size based on responsibility\n",
    "        for i in range(len(means)): \n",
    "            mask = labels == i\n",
    "            size = 50 * (responsibilities[mask, :].max(1) ** 2)\n",
    "            ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}', zorder=3)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c='coral', s=40, \n",
    "                    edgecolor='w', alpha=0.8, zorder=3)\n",
    "    \n",
    "    # Plot contours\n",
    "    ax.contourf(xx, yy, z, cmap='Oranges', alpha=1)\n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "# Initialize GMM parameters randomly\n",
    "np.random.seed(42)\n",
    "n_components = 4\n",
    "\n",
    "# Random initialization (poor fit initially)\n",
    "initial_means = np.random.randn(n_components, 2) \n",
    "initial_covariances = np.array([np.eye(2) * 2 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "print(\"Step 0: Random Initialization\")\n",
    "plot_gmm_step(X, initial_means, initial_covariances, initial_weights, \n",
    "              title=\"Step 0: Random Initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26239e0",
   "metadata": {},
   "source": [
    "## Optimizing the GMM with EM-Algorithm\n",
    "The Expectation-Maximization (EM) algorithm is an iterative method used to find maximum likelihood estimates of parameters in statistical models, particularly when the model depends on unobserved latent variables. In the context of Gaussian Mixture Models (GMMs), the EM algorithm helps optimize the parameters of the Gaussian components to best fit the data.\n",
    "\n",
    "### E-step\n",
    "In the E-step (Expectation step), we compute the expected value of the latent variables given the current estimates of the parameters. For GMMs, this involves calculating the responsibilities, which represent the probability that each data point belongs to each Gaussian component. The responsibilities are computed using Bayes' theorem, taking into account the current means, covariances, and mixture weights of the Gaussian components.\n",
    "\n",
    "$$\n",
    "\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Expectation step: compute responsibilities (soft assignments)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of data points\n",
    "    K = len(means) # number of components\n",
    "    responsibilities = np.zeros((N, K))\n",
    "    \n",
    "    # Compute probability of each point under each component\n",
    "    for k in range(K):\n",
    "        rv = multivariate_normal(means[k], covariances[k])\n",
    "        responsibilities[:, k] = weights[k] * rv.pdf(X)\n",
    "    \n",
    "    # Normalize to get responsibilities (posterior probabilities)\n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "# Run E-step\n",
    "responsibilities = e_step(X, initial_means, initial_covariances, initial_weights)\n",
    "\n",
    "print(\"\\nStep 1: E-step - Assign points to clusters (soft assignment)\")\n",
    "plot_gmm_step(X, initial_means, initial_covariances, initial_weights, \n",
    "              responsibilities, \n",
    "              title=\"Step 1: E-step - Compute Responsibilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f1aaf",
   "metadata": {},
   "source": [
    "### M-step\n",
    "In the M-step (Maximization step), we update the parameters of the Gaussian components to maximize the expected log-likelihood found in the E-step. This involves recalculating the means, covariances, and mixture weights based on the responsibilities computed in the E-step.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "N_k &= \\sum_{i=1}^{N} \\gamma_{ik} \\\\\n",
    "\\mu_k &= \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} x_i \\\\\n",
    "\\Sigma_k &= \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T \\\\\n",
    "\\pi_k &= \\frac{N_k}{N}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    \"\"\"\n",
    "    Maximization step: update means, covariances, and weights\n",
    "    \"\"\"\n",
    "    N, d = X.shape # number of data points and dimensions\n",
    "    K = responsibilities.shape[1] # number of components\n",
    "    \n",
    "    # Effective number of points assigned to each component\n",
    "    Nk = responsibilities.sum(axis=0)\n",
    "    \n",
    "    # Update weights\n",
    "    weights = Nk / N\n",
    "    \n",
    "    # Update means\n",
    "    means = np.zeros((K, d))\n",
    "    for k in range(K):\n",
    "        means[k] = (responsibilities[:, k:k+1] * X).sum(axis=0) / Nk[k]\n",
    "    \n",
    "    # Update covariances\n",
    "    covariances = np.zeros((K, d, d))\n",
    "    for k in range(K):\n",
    "        diff = X - means[k]\n",
    "        covariances[k] = (responsibilities[:, k:k+1] * diff).T @ diff / Nk[k]\n",
    "    \n",
    "    return means, covariances, weights\n",
    "\n",
    "# Run M-step\n",
    "new_means, new_covariances, new_weights = m_step(X, responsibilities)\n",
    "\n",
    "print(\"\\nStep 2: M-step - Update parameters based on responsibilities\")\n",
    "plot_gmm_step(X, new_means, new_covariances, new_weights, \n",
    "              responsibilities,\n",
    "              title=\"Step 2: M-step - Updated Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690f918",
   "metadata": {},
   "source": [
    "After iterating between the E-step and M-step until convergence, the parameters of the Gaussian components will be optimized to best fit the data, allowing us to effectively model the underlying clusters in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gmm(X, n_iterations=100, means=None, covariances=None, weights=None, classes=False):\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # E-step\n",
    "        responsibilities = e_step(X, means, covariances, weights)\n",
    "        \n",
    "        # M-step\n",
    "        means, covariances, weights = m_step(X, responsibilities)\n",
    "        \n",
    "    plot_gmm_step(X, means, covariances, weights, responsibilities, classes=classes,\n",
    "                        title=f\"Gaussian Mixture Model with {len(means)} components, {i+2} iterations\")\n",
    "            \n",
    "    return means, covariances, weights\n",
    "\n",
    "# Train GMM for a few iterations\n",
    "final_means, final_covariances, final_weights = train_gmm(X, 99, new_means, new_covariances, new_weights, classes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24efdab",
   "metadata": {},
   "source": [
    "## Predicting the class labels\n",
    "To predict the class labels for new data points using the fitted Gaussian Mixture Model (GMM), we calculate the probabilties of each data point belonging to each Gaussian component and assign the class based on the highest probability. In this plot the predicted class labels are shown with different colors and sizes. The size of the points indicates the confidence of the prediction, with larger points representing higher confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gmm(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Predict cluster assignments based on highest responsibility\n",
    "    \"\"\"\n",
    "    responsibilities = e_step(X, means, covariances, weights)\n",
    "    return np.argmax(responsibilities, axis=1), responsibilities\n",
    "\n",
    "z = np.zeros_like(xx)\n",
    "colors = ['gold', 'darkorange', 'orangered', 'red' ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot each cluster with size based on responsibility\n",
    "for i in range(4):\n",
    "    labels, responsibilities = predict_gmm(X, final_means, final_covariances, final_weights)\n",
    "    mask = labels == i\n",
    "    size = 80 * (responsibilities[mask, :].max(1) ** 2)\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}')\n",
    "    \n",
    "# Plot contours\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Predicted GMM Clusters', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(['Class 1', 'Class 2', 'Class 3', 'Class 4'], loc='upper right')\n",
    "ax.set_xlim(-4,4.5)\n",
    "ax.set_ylim(-2,11)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2e466",
   "metadata": {},
   "source": [
    "### Scikit-learn's GaussianMixture function\n",
    "Now that we have understood how the EM algorithm works to fit a Gaussian Mixture Model (GMM), we can compare it to existing libraries to simplify the implementation. Skit-learn provides a convenient `GaussianMixture` class that encapsulates the EM algorithm for fitting GMMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4).fit(X) # uses EM algorithm and 100 iterations by default\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "colors = ['gold', 'darkorange', 'orangered', 'red' ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = labels == i\n",
    "    responsibilities = gmm.predict_proba(X)\n",
    "    size = 80 * (responsibilities[mask, :].max(1) ** 2)\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}')\n",
    "\n",
    "# ax.scatter(X[:, 0], X[:, 1], c=[colors[label] for label in labels], s=40, edgecolor='w', alpha=0.8)\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Predicted GMM Clusters with Scikit-learn', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend( loc='upper right')\n",
    "ax.set_xlim(-4,4.5)\n",
    "ax.set_ylim(-2,11)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791976cc",
   "metadata": {},
   "source": [
    "### Robustness of GMMs with full covariance matrices\n",
    "\n",
    "To test the robustness of GMMs, we apply a random linear transformation to stretch and rotate the data. This creates elliptical clusters with different orientations and shapes. Unlike spherical Gaussians with diagonal covariances, GMMs with full covariance matrices can adapt to these elongated clusters, demonstrating the flexibility of the model in capturing various cluster geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # ✅ Better\n",
    "ax.scatter(X_stretched[:, 0], X_stretched[:, 1], c='coral', s=60, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.grid(color='black', linewidth=0.5)\n",
    "ax.set_title('Dataset with 4 unknown classes', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-2, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e941742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GMM for more elliptical data\n",
    "np.random.seed(0) \n",
    "initial_means = np.random.randn(n_components, 2)\n",
    "initial_covariances = np.array([np.eye(2) * 3 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "final_means, final_covariances, final_weights = train_gmm(X_stretched, 99, initial_means, initial_covariances, initial_weights, classes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f81de",
   "metadata": {},
   "source": [
    "## ELBO\n",
    "\n",
    "The Evidence Lower Bound (ELBO) is a fundamental concept in variational inference and latent variable models. It provides a lower bound on the log-likelihood of the data and consists of two terms: the expected log-likelihood under the posterior and the KL divergence between the approximate and true posterior. In this exercise the ELBO is maximized during the EM algorithm to optimize the parameters of the Gaussian Mixture Model (GMM). The E-step minimizes the KL divergence by updating the responsibilities, while the M-step maximizes the expected log-likelihood by updating the model parameters. Monitoring the ELBO over iterations helps ensure convergence and assess model fit.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "ELBO &= \\mathbb{E}_{q(z|x)}[\\log p(x, z|\\Theta)] - \\mathbb{E}_{q(z|x)}[\\log q(z|x)] \\\\\n",
    "&= \\log p(x|\\Theta) - D_{KL}(q(z|x) || p(z|x,\\Theta))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### KL Divergence Derivation\n",
    "\n",
    "The KL divergence between our approximate posterior $q(z|x)$ and the true posterior $p(z|x)$ is defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(q(z|x) || p(z|x)) = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{q(z|x)}{p(z|x)}\\right]\n",
    "$$\n",
    "\n",
    "For a Gaussian Mixture Model, we expand this for each data point $x_i$ and component $k$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "D_{KL}(q(z|x) || p(z|x)) &= \\sum_{i=1}^{N} \\sum_{k=1}^{K} q(z_i = k) \\log \\frac{q(z_i = k)}{p(z_i = k | x_i, \\theta)} \\\\\n",
    "&= \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\gamma_{ik} \\log \\frac{\\gamma_{ik}}{p(z_i = k | x_i, \\theta)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\gamma_{ik} = q(z_i = k)$ are the responsibilities computed in the E-step.\n",
    "\n",
    "Using Bayes' rule, the true posterior is:\n",
    "\n",
    "$$\n",
    "p(z_i = k | x_i, \\theta) = \\frac{p(x_i | z_i = k, \\theta) \\cdot p(z_i = k)}{\\sum_{j=1}^{K} p(x_i | z_i = j, \\theta) \\cdot p(z_i = j)} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "where $\\pi_k$ are the mixture weights, $\\mu_k$ are the means, and $\\Sigma_k$ are the covariances.\n",
    "\n",
    "Substituting this into the KL divergence formula:\n",
    "\n",
    "$$\n",
    "D_{KL}(q(z) || p(z|x)) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\gamma_{ik} \\left(\\log \\gamma_{ik} - \\log \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a34361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single-cluster data\n",
    "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Fit with single Gaussian (no latent variables needed!)\n",
    "n_components = 2\n",
    "initial_means = np.array([[0.0, 0.0]])\n",
    "initial_covariances = np.array([np.eye(2)])\n",
    "initial_weights = np.array([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbo = log(p(x|theta))-D_KL(q(z)||p(z|x,theta))\n",
    "def log_likelihood(X, means, covariances, weights):\n",
    "    \"\"\"Compute log-likelihood of data under GMM\"\"\"\n",
    "    N = X.shape[0]\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        prob = 0\n",
    "        for k in range(len(means)):\n",
    "            rv = multivariate_normal(means[k], covariances[k])\n",
    "            prob += weights[k] * rv.pdf(X[i])\n",
    "        log_likelihood += np.log(prob)\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "def compute_elbo(X, means, covariances, weights, responsibilities, ll):\n",
    "    \"\"\"\n",
    "    Compute ELBO as: log p(x|θ) - KL(q||p)\n",
    "    More intuitive decomposition!\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    \n",
    "    # 2. Compute KL divergence: KL(q(z) || p(z|x,θ))\n",
    "    kl_div = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        denominator = 0\n",
    "        for j in range(len(means)):\n",
    "            rv_j = multivariate_normal(means[j], covariances[j])\n",
    "            denominator += weights[j] * rv_j.pdf(X[i])\n",
    "            \n",
    "        for k in range(len(means)):\n",
    "            if responsibilities[i, k] > 1e-10:\n",
    "                # q(z_i = k)\n",
    "                q_z = responsibilities[i, k]\n",
    "                \n",
    "                # p(z_i = k | x_i, θ) using Bayes rule\n",
    "                rv = multivariate_normal(means[k], covariances[k])\n",
    "                numerator = weights[k] * rv.pdf(X[i])\n",
    "                \n",
    "                # Denominator: sum over all components\n",
    "                p_z_given_x = numerator / (denominator + 1e-10)\n",
    "                \n",
    "                # KL divergence: sum_z q(z) log(q(z) / p(z|x))\n",
    "                if p_z_given_x > 1e-10:\n",
    "                    kl_div += q_z * (np.log(q_z) - np.log(p_z_given_x))\n",
    "    \n",
    "    # ELBO = log p(x|θ) - KL(q||p)\n",
    "    elbo = ll - kl_div\n",
    "    \n",
    "    return elbo, ll, kl_div\n",
    "\n",
    "\n",
    "# Visualization: sweep over mean of first component\n",
    "np.random.seed(42)\n",
    "n_components = 1\n",
    "initial_means = np.random.randn(n_components,2) \n",
    "print(\"Initial means:\\n\", initial_means)\n",
    "initial_covariances = np.array([np.eye(2) * 2 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "# Run one E-step and one M-step\n",
    "responsibilities_0 = e_step(X, initial_means, initial_covariances, initial_weights)\n",
    "means_1, covariances_1, weights_1 = m_step(X, responsibilities_0)\n",
    "responsibilities_1 = e_step(X, means_1, covariances_1, weights_1)\n",
    "means_2, covariances_2, weights_2 = m_step(X, responsibilities_1)\n",
    "responsibilities_2 = e_step(X, means_2, covariances_2, weights_2)\n",
    "means_3, covariances_3, weights_3 = m_step(X, responsibilities_2)\n",
    "\n",
    "# Sweep mean of first component along x-axis\n",
    "mean_range = np.linspace(-3, 3, 50)\n",
    "# elbo_range = np.linspace(0, 1, 50)\n",
    "log_likelihoods_0 = []\n",
    "log_likelihoods_1 = []\n",
    "elbos_0 = []\n",
    "elbos_1 = []\n",
    "elbos_2 = []\n",
    "elbos_3 = []\n",
    "\n",
    "test_means_0 = initial_means.copy()\n",
    "test_means_1 = means_1.copy()\n",
    "test_means_2 = means_2.copy()\n",
    "\n",
    "for mean_val in mean_range:\n",
    "    # Test with varying first component mean\n",
    "    test_means_0[0, 0] = mean_val    \n",
    "    test_means_1[0, 0] = mean_val\n",
    "    test_means_2[0, 0] = mean_val\n",
    "    \n",
    "    # Compute log-likelihood for EACH parameter set\n",
    "    ll_0 = log_likelihood(X, test_means_0, initial_covariances, initial_weights)\n",
    "    log_likelihoods_0.append(ll_0)\n",
    "    \n",
    "    # ELBO after E-step 0: tight bound at θ^[0] (KL=0 at initial_means[0,0])\n",
    "    elbo_0, _, _= compute_elbo(X, test_means_0, initial_covariances, initial_weights, responsibilities_0, ll_0)\n",
    "    elbos_0.append(elbo_0)\n",
    "    \n",
    "    # ELBO after M-step 1 but keeping q^[0] fixed (shows how ELBO varies when we change θ but keep q fixed)\n",
    "    elbo_1, _, _= compute_elbo(X, test_means_0, initial_covariances, initial_weights, responsibilities_1, ll_0)\n",
    "    elbos_1.append(elbo_1)\n",
    "    \n",
    "    # ELBO after E-step 1: update q to make bound tight again at new θ^[1]\n",
    "    elbo_2, _, _= compute_elbo(X, test_means_1, covariances_1, weights_1, responsibilities_1, ll_0)\n",
    "    elbos_2.append(elbo_2)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "# First plot - initialization: show log-likelihood and ELBO with initial q and θ\n",
    "ax1.plot(mean_range, log_likelihoods_0, 'k-', linewidth=2.5, label=r'$\\log p(\\mathbf{x}|\\theta^{[0]})$')\n",
    "ax1.plot(mean_range, elbos_0, 'coral', linewidth=2, label=r'ELBO[$q^{[0]}, \\theta^{[0]}$]')\n",
    "ax1.axvline(initial_means[0, 0], color='coral', linestyle='--', alpha=0.5, label=r'$\\theta^{[0]}$ (KL=0 here)')\n",
    "ax1.scatter([initial_means[0, 0]], [elbos_0[np.argmin(np.abs(mean_range - initial_means[0, 0]))]], \n",
    "           color='coral', s=100, zorder=5)\n",
    "ax1.set_xlabel(r'$\\theta$ (mean of component 1)', fontsize=12)\n",
    "ax1.set_ylabel(r'Value', fontsize=12)\n",
    "ax1.set_title('Initialization', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(-3800,-3600)\n",
    "ax1.set_xlim(-2,2)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# E-step - minimize KL divergence (update q to make ELBO tight)\n",
    "ax2.plot(mean_range, log_likelihoods_0, 'k-', linewidth=2.5, label=r'$\\log p(\\mathbf{x}|\\theta)$')\n",
    "ax2.plot(mean_range, elbos_0, 'coral', linewidth=2, alpha=0.5, label=r'ELBO[$q^{[0]}, \\theta^{[0]}$]')\n",
    "ax2.plot(mean_range, elbos_1, 'turquoise', linewidth=2, label=r'ELBO[$q^{[1]}, \\theta^{[0]}$] (after E-step)')\n",
    "ax2.axvline(initial_means[0, 0], color='coral', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(means_1[0, 0], color='turquoise', linestyle='--', alpha=0.5)\n",
    "ax2.scatter([initial_means[0, 0]], [elbos_0[np.argmin(np.abs(mean_range - initial_means[0, 0]))]], color='coral', s=100, zorder=5)\n",
    "ax2.scatter([means_1[0, 0]], [elbos_1[np.argmin(np.abs(mean_range - means_1[0, 0]))]], color='turquoise', s=100, zorder=5)\n",
    "ax2.set_xlabel(r'$\\theta$ (mean of component 1)', fontsize=12)\n",
    "ax2.set_ylabel(r'Value', fontsize=12)\n",
    "ax2.set_title('E-step: Minimize KL (choose q=p)', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim(-3800,-3600)\n",
    "ax2.set_xlim(-2,2)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# M-step plot - maximize ELBO w.r.t. θ (keeping q fixed)\n",
    "ax3.plot(mean_range, log_likelihoods_0, 'k-', linewidth=2.5, label=r'$\\log p(\\mathbf{x}|\\theta^{[0]})$')\n",
    "ax3.plot(mean_range, elbos_1, 'turquoise', linewidth=2, label=r'ELBO[$q^{[1]}, \\theta^{[0]}$] (after E-step)')\n",
    "ax3.plot(mean_range, elbos_2, 'purple', linewidth=2, label=r'ELBO[$q^{[1]}, \\theta^{[1]}$] (after M-step)')\n",
    "ax3.axvline(means_2[0, 0], color='purple', linestyle='--', alpha=0.5)\n",
    "ax3.axvline(means_1[0, 0], color='turquoise', linestyle='--', alpha=0.5)\n",
    "ax3.scatter([means_1[0, 0]], [elbos_1[np.argmin(np.abs(mean_range - means_1[0, 0]))]], color='turquoise', s=100, zorder=5)\n",
    "ax3.scatter([means_2[0, 0]], [elbos_2[np.argmin(np.abs(mean_range - means_2[0, 0]))]], color='purple', s=100, zorder=5)\n",
    "ax3.set_xlabel(r'$\\theta$ (mean of component 1)', fontsize=12)\n",
    "ax3.set_ylabel(r'Value', fontsize=12)\n",
    "ax3.set_title('M-step: Maximize ELBO w.r.t. θ (fix q)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylim(-3800,-3600)\n",
    "ax3.set_xlim(-2,2)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bec13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
