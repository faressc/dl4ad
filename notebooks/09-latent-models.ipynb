{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d43353d",
   "metadata": {},
   "source": [
    "# Lesson 9: Latent Models\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.distributions as distribution\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2ccf9",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=400, centers=4, cluster_std=0.80, random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # ✅ Better\n",
    "ax.scatter(X[:, 0], X[:, 1], c='coral', s=50, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.grid(color='black', linewidth=0.5)\n",
    "ax.set_title('Dataset with 4 unknown classes', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-4, 4.5)\n",
    "ax.set_ylim(-2, 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0538a4",
   "metadata": {},
   "source": [
    "## Simple Gaussian does not fit well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Manually compute mean and covariance\n",
    "mu = X.mean(axis=0)\n",
    "sigma = np.cov(X.T)\n",
    "\n",
    "# Create grid for plotting\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 2\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Compute Gaussian PDF on grid\n",
    "pos = np.dstack((xx, yy))\n",
    "rv = multivariate_normal(mu, sigma)\n",
    "z = rv.pdf(pos)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], c='coral', s=40, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.contourf(xx, yy, z, cmap='Oranges', alpha=1)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title('Simple Gaussian Distribution Fit to Data', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-4, 4.5)\n",
    "ax.set_ylim(-2, 11)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0303340",
   "metadata": {},
   "source": [
    "## Instead of one Gaussian, use a mixture of Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gmm_step(X, means, covariances, weights, responsibilities=None, title=\"\", classes=False):\n",
    "    \"\"\"\n",
    "    Visualize GMM components and optionally responsibilities\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Create grid for plotting\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    pos = np.dstack((xx, yy))\n",
    "    \n",
    "    # # Compute mixture PDF\n",
    "    z = np.zeros_like(xx)\n",
    "    \n",
    "    for k in range(len(means)):\n",
    "        rv = multivariate_normal(means[k], covariances[k])\n",
    "        z += weights[k] * rv.pdf(pos)\n",
    "    \n",
    "    colors = ['gold', 'darkorange', 'orangered', 'red']\n",
    "\n",
    "    if classes is True:\n",
    "        labels = np.argmax(responsibilities, axis=1)\n",
    "        # Scatter plot each cluster with size based on responsibility\n",
    "        for i in range(len(means)): \n",
    "            mask = labels == i\n",
    "            size = 50 * (responsibilities[mask, :].max(1) ** 2)\n",
    "            ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}', zorder=3)\n",
    "    else:\n",
    "        ax.scatter(X[:, 0], X[:, 1], c='coral', s=40, \n",
    "                    edgecolor='w', alpha=0.8, zorder=3)\n",
    "    \n",
    "    # Plot contours\n",
    "    ax.contourf(xx, yy, z, cmap='Oranges', alpha=1)\n",
    "    ax.set_xlabel('Feature 1', fontsize=12)\n",
    "    ax.set_ylabel('Feature 2', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "# Initialize GMM parameters randomly\n",
    "np.random.seed(42)\n",
    "n_components = 4\n",
    "\n",
    "# Random initialization (poor fit initially)\n",
    "initial_means = np.random.randn(n_components, 2) \n",
    "initial_covariances = np.array([np.eye(2) * 2 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "print(\"Step 0: Random Initialization\")\n",
    "plot_gmm_step(X, initial_means, initial_covariances, initial_weights, \n",
    "              title=\"Step 0: Random Initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Expectation step: compute responsibilities (soft assignments)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of data points\n",
    "    C = len(means) # number of components\n",
    "    responsibilities = np.zeros((N, C))\n",
    "    \n",
    "    # Compute probability of each point under each component\n",
    "    for c in range(C):\n",
    "        rv = multivariate_normal(means[c], covariances[c])\n",
    "        responsibilities[:, c] = weights[c] * rv.pdf(X)\n",
    "    \n",
    "    # Normalize to get responsibilities (posterior probabilities)\n",
    "    responsibilities /= responsibilities.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "# Run E-step\n",
    "responsibilities = e_step(X, initial_means, initial_covariances, initial_weights)\n",
    "\n",
    "print(\"\\nStep 1: E-step - Assign points to clusters (soft assignment)\")\n",
    "plot_gmm_step(X, initial_means, initial_covariances, initial_weights, \n",
    "              responsibilities, \n",
    "              title=\"Step 1: E-step - Compute Responsibilities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities):\n",
    "    \"\"\"\n",
    "    Maximization step: update means, covariances, and weights\n",
    "    \"\"\"\n",
    "    N, d = X.shape # number of data points and dimensions\n",
    "    C = responsibilities.shape[1] # number of components\n",
    "    \n",
    "    # Effective number of points assigned to each component\n",
    "    Nk = responsibilities.sum(axis=0)\n",
    "    \n",
    "    # Update weights\n",
    "    weights = Nk / N\n",
    "    \n",
    "    # Update means\n",
    "    means = np.zeros((C, d))\n",
    "    for c in range(C):\n",
    "        means[c] = (responsibilities[:, c:c+1] * X).sum(axis=0) / Nk[c]\n",
    "    \n",
    "    # Update covariances\n",
    "    covariances = np.zeros((C, d, d))\n",
    "    for c in range(C):\n",
    "        diff = X - means[c]\n",
    "        covariances[c] = (responsibilities[:, c:c+1] * diff).T @ diff / Nk[c]\n",
    "    \n",
    "    return means, covariances, weights\n",
    "\n",
    "# Run M-step\n",
    "new_means, new_covariances, new_weights = m_step(X, responsibilities)\n",
    "\n",
    "print(\"\\nStep 2: M-step - Update parameters based on responsibilities\")\n",
    "plot_gmm_step(X, new_means, new_covariances, new_weights, \n",
    "              responsibilities,\n",
    "              title=\"Step 2: M-step - Updated Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gmm(X, n_iterations=100, means=None, covariances=None, weights=None, classes=False):\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # E-step\n",
    "        responsibilities = e_step(X, means, covariances, weights)\n",
    "        \n",
    "        # M-step\n",
    "        means, covariances, weights = m_step(X, responsibilities)\n",
    "        \n",
    "    plot_gmm_step(X, means, covariances, weights, responsibilities, classes=classes,\n",
    "                        title=f\"Gaussian Mixture Model with {len(means)} components, {i+2} iterations\")\n",
    "            \n",
    "    return means, covariances, weights\n",
    "\n",
    "# Train GMM for a few iterations\n",
    "final_means, final_covariances, final_weights = train_gmm(X, 99, new_means, new_covariances, new_weights, classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gmm(X, means, covariances, weights):\n",
    "    \"\"\"\n",
    "    Predict cluster assignments based on highest responsibility\n",
    "    \"\"\"\n",
    "    responsibilities = e_step(X, means, covariances, weights)\n",
    "    return np.argmax(responsibilities, axis=1), responsibilities\n",
    "\n",
    "z = np.zeros_like(xx)\n",
    "colors = ['gold', 'darkorange', 'orangered', 'red' ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot each cluster with size based on responsibility\n",
    "for i in range(4):\n",
    "    labels, responsibilities = predict_gmm(X, final_means, final_covariances, final_weights)\n",
    "    mask = labels == i\n",
    "    size = 80 * (responsibilities[mask, :].max(1) ** 2)\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}')\n",
    "    \n",
    "# Plot contours\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Predicted GMM Clusters', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(['Class 1', 'Class 2', 'Class 3', 'Class 4'], loc='upper right')\n",
    "ax.set_xlim(-4,4.5)\n",
    "ax.set_ylim(-2,11)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf2e466",
   "metadata": {},
   "source": [
    "## Also possible with Skicit-learn's GaussianMixture function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4).fit(X) # uses EM algorithm and 100 iterations by default\n",
    "labels = gmm.predict(X)\n",
    "\n",
    "colors = ['gold', 'darkorange', 'orangered', 'red' ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "for i in range(4):\n",
    "    mask = labels == i\n",
    "    responsibilities = gmm.predict_proba(X)\n",
    "    size = 80 * (responsibilities[mask, :].max(1) ** 2)\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[i], s=size, edgecolor='w', alpha=0.8, label=f'Class {i+1}')\n",
    "\n",
    "# ax.scatter(X[:, 0], X[:, 1], c=[colors[label] for label in labels], s=40, edgecolor='w', alpha=0.8)\n",
    "ax.set_xlabel('Feature 1', fontsize=12)\n",
    "ax.set_ylabel('Feature 2', fontsize=12)\n",
    "ax.set_title('Predicted GMM Clusters with Scikit-learn', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend( loc='upper right')\n",
    "ax.set_xlim(-4,4.5)\n",
    "ax.set_ylim(-2,11)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791976cc",
   "metadata": {},
   "source": [
    "## Fit with streched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(13)\n",
    "X_stretched = np.dot(X, rng.randn(2, 2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # ✅ Better\n",
    "ax.scatter(X_stretched[:, 0], X_stretched[:, 1], c='coral', s=60, edgecolor='w', alpha=0.8, zorder=2)\n",
    "ax.grid(color='black', linewidth=0.5)\n",
    "ax.set_title('Dataset with 4 unknown classes', color='black')\n",
    "ax.set_xlabel('Feature 1', color='black')\n",
    "ax.set_ylabel('Feature 2', color='black')\n",
    "ax.tick_params(colors='black')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-2, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e941742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GMM for more elliptical data\n",
    "np.random.seed(0) \n",
    "initial_means = np.random.randn(n_components, 2)\n",
    "initial_covariances = np.array([np.eye(2) * 3 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "final_means, final_covariances, final_weights = train_gmm(X_stretched, 99, initial_means, initial_covariances, initial_weights, classes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f81de",
   "metadata": {},
   "source": [
    "## ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be99dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbo = log(p(x|theta))-D_KL(q(z)||p(z|x,theta))\n",
    "def log_likelihood(X, means, covariances, weights):\n",
    "    \"\"\"Compute log-likelihood of data under GMM\"\"\"\n",
    "    N = X.shape[0]\n",
    "    log_likelihood = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        prob = 0\n",
    "        for k in range(len(means)):\n",
    "            rv = multivariate_normal(means[k], covariances[k])\n",
    "            prob += weights[k] * rv.pdf(X[i])\n",
    "        log_likelihood += np.log(prob)\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "def compute_elbo(X, means, covariances, weights, responsibilities):\n",
    "    \"\"\"Compute ELBO (Evidence Lower Bound)\"\"\"\n",
    "    N = X.shape[0]\n",
    "    elbo = 0\n",
    "    \n",
    "    # E[log p(x,z|theta)]\n",
    "    for i in range(N):\n",
    "        for k in range(len(means)):\n",
    "            if responsibilities[i, k] > 1e-10:  # Avoid log(0)\n",
    "                rv = multivariate_normal(means[k], covariances[k])\n",
    "                elbo += responsibilities[i, k] * (\n",
    "                    np.log(weights[k]) + np.log(rv.pdf(X[i]) + 1e-10)\n",
    "                )\n",
    "    \n",
    "    # - E[log q(z)]\n",
    "    for i in range(N):\n",
    "        for k in range(len(means)):\n",
    "            if responsibilities[i, k] > 1e-10:\n",
    "                elbo -= responsibilities[i, k] * np.log(responsibilities[i, k])\n",
    "    \n",
    "    return elbo\n",
    "\n",
    "# Visualization: sweep over mean of first component\n",
    "np.random.seed(42)\n",
    "n_components = 1\n",
    "initial_means = np.random.randn(n_components, 2) \n",
    "initial_covariances = np.array([np.eye(2) * 2 for _ in range(n_components)])\n",
    "initial_weights = np.ones(n_components) / n_components\n",
    "\n",
    "# Run one E-step and one M-step\n",
    "responsibilities_0 = e_step(X, initial_means, initial_covariances, initial_weights)\n",
    "means_1, covariances_1, weights_1 = m_step(X, responsibilities_0)\n",
    "responsibilities_1 = e_step(X, means_1, covariances_1, weights_1)\n",
    "\n",
    "# Sweep mean of first component along x-axis\n",
    "mean_range = np.linspace(-3, 3, 50)\n",
    "log_likelihoods = []\n",
    "elbos_0 = []\n",
    "elbos_1 = []\n",
    "\n",
    "for mean_val in mean_range:\n",
    "    # Test with varying first component mean\n",
    "    test_means_0 = initial_means.copy()\n",
    "    test_means_0[0, 0] = mean_val\n",
    "    \n",
    "    test_means_1 = means_1.copy()\n",
    "    test_means_1[0, 0] = mean_val\n",
    "    \n",
    "    # Compute log-likelihood (same for both since it's objective)\n",
    "    ll = log_likelihood(X, test_means_0, initial_covariances, initial_weights)\n",
    "    log_likelihoods.append(ll)\n",
    "    \n",
    "    # ELBO with initial q (E-step 0)\n",
    "    elbo_0 = compute_elbo(X, test_means_0, initial_covariances, initial_weights, responsibilities_0)\n",
    "    elbos_0.append(elbo_0)\n",
    "    \n",
    "    # ELBO with updated q (E-step 1)\n",
    "    elbo_1 = compute_elbo(X, test_means_1, covariances_1, weights_1, responsibilities_1)\n",
    "    elbos_1.append(elbo_1)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Initial iteration\n",
    "ax1.plot(mean_range, log_likelihoods, 'k-', linewidth=2.5, label=r'$\\log p(\\mathbf{x}|\\theta)$')\n",
    "ax1.plot(mean_range, elbos_0, 'coral', linewidth=2, label=r'ELBO[$q^{[0]}, \\theta^{[0]}$]')\n",
    "ax1.axvline(initial_means[0, 0], color='coral', linestyle='--', alpha=0.5)\n",
    "ax1.scatter([initial_means[0, 0]], [elbos_0[np.argmin(np.abs(mean_range - initial_means[0, 0]))]], \n",
    "           color='coral', s=100, zorder=5)\n",
    "ax1.set_xlabel(r'$\\theta$ (mean of component 1)', fontsize=12)\n",
    "ax1.set_ylabel(r'$\\log [ p(\\mathbf{x}|\\theta)]$', fontsize=12)\n",
    "ax1.set_title('E-step: Fix θ, optimize q', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Right plot: After M-step\n",
    "ax2.plot(mean_range, log_likelihoods, 'k-', linewidth=2.5, label=r'$\\log p(\\mathbf{x}|\\theta)$')\n",
    "ax2.plot(mean_range, elbos_0, 'coral', linewidth=2, alpha=0.5, label=r'ELBO[$q^{[0]}, \\theta^{[0]}$]')\n",
    "ax2.plot(mean_range, elbos_1, 'turquoise', linewidth=2, label=r'ELBO[$q^{[1]}, \\theta^{[1]}$]')\n",
    "ax2.axvline(initial_means[0, 0], color='coral', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(means_1[0, 0], color='turquoise', linestyle='--', alpha=0.5)\n",
    "ax2.scatter([initial_means[0, 0]], [elbos_0[np.argmin(np.abs(mean_range - initial_means[0, 0]))]], \n",
    "           color='coral', s=100, zorder=5)\n",
    "ax2.scatter([means_1[0, 0]], [elbos_1[np.argmin(np.abs(mean_range - means_1[0, 0]))]], \n",
    "           color='turquoise', s=100, zorder=5)\n",
    "ax2.set_xlabel(r'$\\theta$ (mean of component 1)', fontsize=12)\n",
    "ax2.set_ylabel(r'$\\log [ p(\\mathbf{x}|\\theta)]$', fontsize=12)\n",
    "ax2.set_title('M-step: Fix q, optimize θ', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82aa665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
