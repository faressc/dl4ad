{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3698e7d5",
   "metadata": {},
   "source": [
    "# Lesson 1: Machine learning fundamentals\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\\\n",
    "*Date:* 16.10.2025\n",
    "\n",
    "\n",
    "In this course we will cover \n",
    "1. A recap on the concept of machine learning\n",
    "2. Creating our own dataset\n",
    "3. A detailed implementation of simple linear regression\n",
    "4. A detailed implementation of simple binary classification\n",
    "5. An explanation on Train–Test Split and Model Evaluation\n",
    "6. Two exercises to explore polynomial regression and Normal Equations for linear regression\n",
    "\n",
    "This script was inspired by the Creative Machine Learning course of Philippe Esling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9020ae",
   "metadata": {},
   "source": [
    "# Defining Machine Learning\n",
    "\n",
    "Machine learning aims to approximate an unknown mapping between input and output spaces, $\\mathcal{X} \\mapsto \\mathcal{Y}$,\n",
    "given only a finite set of Data\n",
    "$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N},\n",
    "$\n",
    "where $(x_i \\in \\mathcal{X})$ and $(y_i \\in \\mathcal{Y})$.\n",
    "\n",
    "We define a **parametric model**\n",
    "$ f_{\\boldsymbol{\\theta}} \\in \\mathcal{F}$,\n",
    "with parameters $(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta})$, chosen such that\n",
    "$\n",
    "f_{\\boldsymbol{\\theta}}(x) \\approx y$.\n",
    "\n",
    "\n",
    "Any machine learning problem involves these core components:\n",
    "1. **Dataset** $(\\mathcal{D})$: A collection of input-output pairs $ \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1} $, The dataset must be representative of the true underlying function $(f^*: \\mathcal{X} \\mapsto \\mathcal{Y})$  \n",
    "2. **Function or Model** $(f_{\\boldsymbol{\\theta}})$: A parameterized function that maps inputs $\\mathbf{x}_i$ to predicted outputs $\\mathbf{\\hat{y}}_i = f_{{\\theta}}(\\mathbf{x_i})$ The choice of function defines the function space $\\mathcal{F}_{\\Theta}$\n",
    "3. **Parameters ($\\theta$)**: The set of parameters that define the specific function within the function space. These parameters are adjusted during training to minimize the empirical risk. come from the parameter space $\\boldsymbol{\\Theta}$\n",
    "4. **Lossfunction**: A function that quantifies the difference between the predicted outputs $\\mathbf{\\hat{y}}_i$ and the true labels $\\mathbf{y}_i$. The choice of loss function depends on the task (e.g., regression vs. classification).\n",
    "5. **Optimization Algorithm**: A method for adjusting the parameters $\\boldsymbol{\\theta}$ to minimize the empirical risk $\\hat{R}(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1557ba4",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "To understand how a machine learning model works mathematically, we start with the case of simple linear regression, where $\\mathbf{x}\\in\\mathbb{R}$. This implies that our model will follow\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "We will measure the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N} ( y_{i} - f_{\\theta}(x_i))^{2} = \\frac{1}{N} \\sum_{i=1}^{N}  ( y_{i} - (\\theta_{0} + \\theta_{1} x_{i}) )^{2}$$\n",
    "\n",
    "Then our goal is to find the most adequate set of parameters $\\theta = \\{\\theta_{0}, \\theta_{1}\\}$, which are those that minimize the MSE loss defined previously. To do so, we will implement the gradient descent algorithm.\n",
    "\n",
    "### Manual implementation in NumPy\n",
    "\n",
    "We start by performing a full manual implementation, in the sense that we need to manually derive the gradient in order to apply the gradient descent updates. To do so, we will rely on Numpy.\n",
    "\n",
    "We start by importing libraries and set a random seed to ensure that the random number generator produces always a reproducible series of random numbers.\n",
    "\n",
    "**Used functions**\n",
    "- `np.random.seed`: sets the seed for the NumPy random number generator. [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41625c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2a87f",
   "metadata": {},
   "source": [
    "### Generate a synthetic dataset.\n",
    "\n",
    "For the sake of this exercise, we will generate the data ourselves, so that we know the true values that we are looking for in advance. As in most real-world cases, our measurements are not perfect, they contain some amount of **observation noise** (also called *residual error*) $\\epsilon$ . Hence, we define a linear relationship following\n",
    "$$y_i = f^*(\\mathbf{x}_i) + \\epsilon_i$$\n",
    "\n",
    "**Useful functions**\n",
    "- `np.random.rand`: generates an array of random numbers uniformly distributed over [0, 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
    "- `np.random.randn`: generates an array of random numbers from the standard normal distribution (mean 0, variance 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)\n",
    "- `np.random.uniform`\" draws samples from a uniform distribution. [Documentation](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.uniform.html)\n",
    "\n",
    "Below, we will visualize synthetic data with generated noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "404558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset\n",
    "\n",
    "true_theta0 = 4\n",
    "true_theta1 = -3\n",
    "n_obs = 100\n",
    "\n",
    "epsilon = np.random.randn(n_obs) * 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(eps=0.7):\n",
    "    x = np.linspace(0, 1, n_obs) \n",
    "    y =  true_theta1 * x + true_theta0 +  eps * epsilon\n",
    "    return x, y\n",
    "\n",
    "def plot_data(eps=0.7):\n",
    "    x, y = generate_data(eps)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.scatter(x, y, color=\"C0\", label=\"Created data\")\n",
    "    plt.plot(x, (true_theta1*x + true_theta0), color=\"C1\", label=\"True function\" )\n",
    "    plt.title(rf\"$y = {true_theta1:.2f}*x + {true_theta0:.2f} + ϵ, ϵ \\in[-{eps:.2f},{eps:.2f}]$\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.xlim(0, 1)  \n",
    "    plt.ylim(0, 5)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', lw=1.5) \n",
    "    plt.axvline(0, color='black', lw=1.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with interactive slider for epsilon\n",
    "interact(\n",
    "    plot_data,\n",
    "    eps=widgets.FloatSlider(value=0.3, min=0.0, max=2, step=0.05, description=\"ϵ range\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set for the linear regression with set epsilon\n",
    "true_theta0 = 2\n",
    "true_theta1 = 3\n",
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 0.2\n",
    "\n",
    "x = np.linspace(0, 1, n_obs)\n",
    "y = (true_theta1 * x + true_theta0) + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa98deb",
   "metadata": {},
   "source": [
    "### Computing the MSE loss\n",
    "\n",
    "The next step is to calculate the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}\\left( \\hat{\\mathbf{y}},\\theta \\right) = \\sum_{i=1}^{n} ( \\hat{y}_{i} - y_{i} )^{2} = \\sum_{i=1}^{n} ( (\\theta_{0} + \\theta_{1} x_{i}) -  y_{i} )^{2}$$\n",
    "\n",
    "This will allow us to evaluate the performances of our model, but is also the basis for the gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6885d97",
   "metadata": {},
   "source": [
    "### Implement the gradient descent algorithm.\n",
    "\n",
    "First we have to Iinitialize the parameters $\\theta_1$ and $\\theta_0$ to small random values\n",
    "1. Evaluate the predictions made by our model \n",
    "$$\\hat{y} = \\theta_{1}x + \\theta_0$$\n",
    "2. Compute the mean squared error (MSE) loss between the predicted values and the ground truth labels: \n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i} - y_i)^2$$\n",
    "3. Compute the gradients of the loss with respect to the parameters: \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = \\sum_{i} 2 * (\\hat{y}_{i} - y_{i})*x_{i}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_0} = \\sum_{i} 2 * (\\hat{y}_{i} - y_{i})$$\n",
    "4. Update the parameters using the gradients and a learning rate $\\eta$: \n",
    "$$\\theta_1 \\leftarrow \\theta_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}$$\n",
    "\n",
    "$$\\theta_0 \\leftarrow \\theta_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def mse_loss_np(y, y_pred):\n",
    "    return   np.sum((y_pred - y) ** 2)/n_obs\n",
    "\n",
    "# Manually calculated gradients of the MSE loss function\n",
    "def mse_loss_gradient_np(y,y_pred, x):\n",
    "    dLdtheta1 = 2 * np.sum((y_pred - y) * x)\n",
    "    dLdtheta0 = 2 * np.sum((y_pred - y))\n",
    "    return dLdtheta0, dLdtheta1\n",
    "\n",
    "# Optimization with gradient descent\n",
    "def gradient_descent_np(x, y, lr, n_iter):\n",
    "    #  Initializing the parameters\n",
    "    theta1 = np.random.randn()\n",
    "    theta0 = np.random.randn()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Gradient Descent Loop \n",
    "    for i in range(n_iter):\n",
    "        # Predictions\n",
    "        y_pred = theta1 * x + theta0\n",
    "        \n",
    "        # Compute gradients\n",
    "        dLdtheta0, dLdtheta1 = mse_loss_gradient_np(y,y_pred,x)\n",
    "\n",
    "        # Update parameters\n",
    "        theta1 = theta1 - lr * dLdtheta1\n",
    "        theta0 = theta0 - lr * dLdtheta0\n",
    "\n",
    "        # Compute Mean Squared Error loss\n",
    "        loss_history.append(mse_loss_np(y, y_pred))\n",
    "\n",
    "    return theta0, theta1, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2658494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 200\n",
    "lr = 1e-3\n",
    "\n",
    "theta0, theta1, loss_history = gradient_descent_np(x, y, lr, n_iters)\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x, y, color=\"C0\", label=\"Data\")\n",
    "plt.plot(x, true_theta1 * x + true_theta0, color=\"C1\", lw=2, label=\"True linear regreassion\")\n",
    "plt.plot(x, theta1 * x + theta0, color=\"C2\", lw=2, linestyle=\"--\", label=rf\"Learned model ($\\theta_1$={theta1:.2f}, $\\theta_0$={theta0:.2f})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Linear Regression with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a5f64",
   "metadata": {},
   "source": [
    "### Implementation with Jax\n",
    "\n",
    "The previous implementation required us to perform manual differentiation of our loss function to understand how to update the parameters. However, large developments have been made in the field of automatic differentiation. \n",
    "\n",
    "The recent library [JAX](https://github.com/google/jax) extends NumPy with this automatic differentiation feature, while providing a functional approach to numerical computing, allowing for easy gradient computation. Its ability to handle complex and custom gradients makes JAX particularly well-suited for advanced research projects. Similar to NumPy, we strongly encourage you to learn JAX through the set of [tutorials](https://jax.readthedocs.io/en/latest/).\n",
    "\n",
    "Note that `JAX` has been thought as an extension of `NumPy`, therefore an extremely large portion of its API simply mirrors the `NumPy` functions by adding automatic differentiation features to it.\n",
    "\n",
    "**Useful functions**\n",
    "- `random.PRNGKey`: function to generate a key for the pseudorandom number generator. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html)\n",
    "- `jit`: function to compile a function for faster execution [Documentation](https://jax.readthedocs.io/en/latest/jit.html)\n",
    "- `grad`: function to compute the gradient of a function [Documentation](https://docs.jax.dev/en/latest/_autosummary/jax.grad.html#jax.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "124d4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c444b85",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We can simply keep our previous approach to generating the dataset but we will rely on `jnp.ndarray` instead of `np.ndarray`. \n",
    "\n",
    "**Useful functions**\n",
    "- `random.split`: function to split a PRNGKey into a list of subkeys [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html)\n",
    "- `random.uniform`: function to generate an array of uniformly distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html)\n",
    "- `random.normal`: function to generate an array of normally distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c317e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the key and uses the keys to create data points\n",
    "key_0, key_1, key = random.split(key, 3)\n",
    "x = random.uniform(key_0, (100,))\n",
    "y = true_theta0 + true_theta1 * x + (random.normal(key_1, (100,)) * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8afe5b",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Similarly we have to define the loss function and its gradient.\n",
    "\n",
    "**Useful function**\n",
    "- `jnp.mean`: function to compute the mean of an array [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d276632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def loss_function_jax(x, y, theta0, theta1):\n",
    "    y_pred = theta1 * x + theta0\n",
    "    return jnp.mean((y_pred - y)**2)\n",
    "\n",
    "# Optimization with gradient descent using jax's grad function \n",
    "def gradient_descent_jax(key, x, y, lr, n_iter):\n",
    "    # Initializing the parameters\n",
    "    k_0, k_1 = random.split(key, 2)\n",
    "    theta0 = random.normal(k_0, (1,))\n",
    "    theta1 = random.normal(k_1, (1,))\n",
    "    loss_history = []\n",
    "\n",
    "    grad_loss_function = grad(loss_function_jax, argnums=[2, 3])\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients\n",
    "        dLdtheta = grad_loss_function(x, y, theta0, theta1)\n",
    "        \n",
    "        # Update the parameters\n",
    "        theta0 -= lr * dLdtheta[0]\n",
    "        theta1 -= lr * dLdtheta[1]\n",
    "\n",
    "        # Compute Mean Squared Error loss\n",
    "        loss_history.append(loss_function_jax(x, y, theta0, theta1))\n",
    "\n",
    "    return theta0, theta1, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iter = 200\n",
    "lr = 0.05\n",
    "\n",
    "theta0, theta1, loss_history = gradient_descent_jax(key,x, y, lr, n_iters)\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x, y, color=\"C0\", label=\"Data\")\n",
    "plt.plot(x, (true_theta1 * x + true_theta0), color=\"C1\", lw=2, label=\"True linear regreassion\")\n",
    "plt.plot(x, theta1 * x + theta0, color=\"C2\", lw=2, linestyle=\"--\", label=rf\"Learned model ($\\theta_1$={theta1[0]:.2f}, $\\theta_0$={theta0[0]:.2f})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Linear Regression with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33272143",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "In binary classification, the goal is to assign each input $ \\mathbf{x} $ in $ \\mathbb{R}^d $ to one of two possible classes, typically represented as $ y \\in \\{-1, +1\\} $.\n",
    "\n",
    "For this exercise we will use a linear classifier model. The relationship between inputs and outputs use a linear decision function:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "$$\n",
    "\n",
    "The predicted label is obtained from the sign of this function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(f_{\\theta}(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "The parameters $ \\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2) $ are estimated by minimizing the Hinge loss function that penalizes incorrect or uncertain predictions.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data set\n",
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 2\n",
    "\n",
    "# Input\n",
    "x0 = np.linspace(0, 1.1, n_obs)\n",
    "x1 = np.linspace(0.5,2, n_obs)\n",
    "\n",
    "# Output \n",
    "y0 = -3 * x0**3 + 13 + epsilon\n",
    "y1 = 2 * x1**3 + 2 + epsilon\n",
    "\n",
    "X = np.vstack([\n",
    "    np.stack([x0, y0], axis=1),\n",
    "    np.stack([x1, y1], axis=1)\n",
    "])\n",
    "Y = np.concatenate([\n",
    "    -np.ones_like(x0),\n",
    "    np.ones_like(x1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d743e8f",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "\n",
    "The Hinge Loss function is Loss function that is commonly used in binary classification models. \n",
    "\n",
    "\n",
    "\n",
    "For a single data point $(x_i, y_i)$, where $ y_i \\in \\{-1, +1\\} $, the hinge loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{hinge}} = \\frac{1}{N} \\sum \\max(0, 1 - y_i f_{\\theta}(x_i))\n",
    "$$\n",
    "\n",
    "- If the prediction $\\hat{y}$ has the same sign as $ y_i $ and its magnitude is ≥ 1, then the loss is 0: the point is correctly classified and outside the margin.  \n",
    "- If the prediction is incorrect or within the margin, the loss increases linearly as $f(x_i)$ moves away from the correct side.\n",
    "\n",
    "\n",
    "This time we will use Stochastic Gradient Descent (SGD) as the optimazation algorithm. With SGD parameters are updated using gradient from a single randomly selected sample per iteration.\n",
    "\n",
    "### Gradient Descent vs. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Both gradient descent and stochastic gradient descent (SGD) are optimization algorithms used to minimize a loss function by iteratively updating model parameters in the direction of the negative gradient.\n",
    "\n",
    "**Gradient Descent**\n",
    "- Uses all training samples to compute the gradient at each iteration.  \n",
    "- Provides a precise estimate of the true gradient.  \n",
    "- Updates are smooth and stable, but computation can be expensive for large datasets.  \n",
    "- The parameter update rule is:\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} - \\eta \\nabla_\\theta \\mathcal{\\hat{R}}(\\boldsymbol{\\theta_t}) = \\boldsymbol{\\theta_t} - \\frac{\\eta}{N} \\displaystyle\\sum_{i=1}^N \\nabla_\\theta \\ell(f_{\\theta_t}(x_{i_t}) , y_{i_t})\n",
    "  $$\n",
    "\n",
    "  where $ \\eta $ is the learning rate and $ \\ell$ is a single sample loss function.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**\n",
    "- Uses one randomly chosen training sample per iteration to approximate the gradient.  \n",
    "- Each update is fast, but the gradient estimate is noisy.  \n",
    "- The loss fluctuates more during training but still tends to decrease on average.  \n",
    "- This noise can help escape shallow local minima and improve generalization.\n",
    "\n",
    "The update rule becomes:\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\ell(f_{\\theta_t}(x_{i_t}) , y_{i_t})\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "<!-- #### Summary\n",
    "\n",
    "| Method | Gradient Computation | Stability  Computation per Step | Typical Use |\n",
    "|---------|----------------------|------------|----------------------|--------------|\n",
    "| Gradient Descent | Full dataset | Smooth, stable | High | Small datasets |\n",
    "| Stochastic Gradient Descent | One sample | Noisy, oscillatory | Very low | Large datasets |\n",
    "\n",
    "In practice, many modern algorithms use a compromise called **mini-batch SGD**, which computes the gradient on a small random subset of the data to balance efficiency and stability. -->\n",
    "\n",
    "\n",
    "### Numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss function\n",
    "def hinge_loss_np(y, y_pred):\n",
    "    losses = 1 - y * y_pred\n",
    "    losses = np.maximum(0, losses)\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Manually calculated gradients of the Hinge loss function \n",
    "def hinge_gradient_np(x_i, y_i, theta0, theta1, theta2):\n",
    "    f_i = theta0 + theta1 * x_i[0] + theta2 * x_i[1]\n",
    "    if 1 - y_i * f_i > 0:  # only if violating margin\n",
    "        dtheta0 = -y_i\n",
    "        dtheta1 = -y_i * x_i[0]\n",
    "        dtheta2 = -y_i * x_i[1]\n",
    "    else:\n",
    "        dtheta0 = dtheta1 = dtheta2 = 0.0\n",
    "    return dtheta0, dtheta1, dtheta2\n",
    "\n",
    "# Optimization with stochastic gradient descent using numpy \n",
    "def stochastic_gradient_descent_np(x, y, lr=0.05, n_iter=1000):\n",
    "    theta0 = np.random.randn()\n",
    "    theta1 = np.random.randn()\n",
    "    theta2 = np.random.randn()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        i_t = np.random.randint(len(x))\n",
    "        x_i = x[i_t]\n",
    "        y_i = y[i_t]\n",
    "\n",
    "        \n",
    "        # Compute loss (for monitoring)\n",
    "        y_pred_full = theta0 + theta1 * x[:,0] + theta2 * x[:,1]\n",
    "        loss_history.append(hinge_loss_np(y, y_pred_full))\n",
    "\n",
    "\n",
    "        # Gradients\n",
    "        dLdtheta0, dLdtheta1, dLdtheta2 = hinge_gradient_np(x_i, y_i, theta0, theta1, theta2)\n",
    "\n",
    "        # Update parameters\n",
    "        theta0 -= lr * dLdtheta0\n",
    "        theta1 -= lr * dLdtheta1\n",
    "        theta2 -= lr * dLdtheta2\n",
    "        theta = [theta0,theta1,theta2]\n",
    "\n",
    "    return theta, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 5000\n",
    "lr = 0.003\n",
    "\n",
    "theta, loss_history = stochastic_gradient_descent_np(X, Y, lr, n_iters)\n",
    "\n",
    "theta0, theta1, theta2 = theta\n",
    "\n",
    "x_line = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "y_line = -(theta0 + theta1 * x_line ) / theta2\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x0, y0, color=\"C0\", label=\"Class 0\")\n",
    "plt.scatter(x1, y1, color=\"C1\", label=\"Class 1\")\n",
    "plt.plot(x_line, y_line, color=\"black\", lw=2, label=rf\"sign boundary ($\\theta_0 = {round(theta0,2)}$, $\\theta_1 = ${round(theta1,2)}, $\\theta_2 = ${round(theta2,2)})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(rf\"Binary classification with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20442d7",
   "metadata": {},
   "source": [
    "### Jax Implementation\n",
    "\n",
    "We will repeat the same exercize using the JAX library to get a better understanding of JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18e8cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "epsilon = random.normal(key, (n_obs,)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d12619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss function  \n",
    "def hinge_loss_jax(theta, x, y):\n",
    "    theta0, theta1, theta2 = theta\n",
    "    f = theta0 + theta1 * x[:, 0] + theta2 * x[:, 1]\n",
    "    losses = 1 - y * f\n",
    "    losses = jnp.maximum(0, losses)\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "# Compute gradient for one-sample loss\n",
    "def single_loss_jax(theta, x_i, y_i):\n",
    "    f_i = theta[0] + theta[1] * x_i[0] + theta[2] * x_i[1]\n",
    "    return jnp.maximum(0, 1 - y_i * f_i)\n",
    "\n",
    "# Optimization with stochastic gradient descent using jax's grad\n",
    "def stochastic_gradient_descent_jax(key, x, y, lr=0.05, n_iter=1000):\n",
    "    theta = random.normal(key, (3,))  # [theta0, theta1, theta2]\n",
    "    \n",
    "    loss_history = []\n",
    "    for i in range(n_iters):\n",
    "        # Sample one random observation\n",
    "        key, subkey = random.split(key)\n",
    "        i_t = random.randint(subkey, shape=(), minval=0, maxval=len(x))\n",
    "        x_i = x[i_t]\n",
    "        y_i = y[i_t]\n",
    "\n",
    "\n",
    "        grads = grad(single_loss_jax)(theta, x_i, y_i)\n",
    "\n",
    "        # Parameter update\n",
    "        theta = theta - lr * grads\n",
    "\n",
    "        # Compute full loss (for monitoring)\n",
    "        loss_history.append(hinge_loss_jax(theta, x, y))\n",
    "\n",
    "    return theta, loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 5000\n",
    "lr = 0.003\n",
    "\n",
    "theta, loss_history = stochastic_gradient_descent_jax(key, X, Y, lr, n_iters)\n",
    "\n",
    "theta0, theta1, theta2 = theta\n",
    "\n",
    "x_line = jnp.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "y_line = -(theta0 + theta1 * x_line) / theta2\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x0, y0, color=\"C0\", label=\"Class 0\")\n",
    "plt.scatter(x1, y1, color=\"C1\", label=\"Class 1\")\n",
    "plt.plot(x_line, y_line, color=\"black\", lw=2, label=\"Decision boundary\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Binary Classification with SGD (JAX, {n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Hinge Loss\")\n",
    "plt.title(\"Training Loss (SGD, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77905f29",
   "metadata": {},
   "source": [
    "## Train–Test Split and Model Evaluation\n",
    "\n",
    "To properly assess how well a machine learning model generalizes, we must evaluate it on data it has not seen during training.  \n",
    "If we only measure performance on the training data, there is a risk of overfitting the model. Overfitting the model means using overly complex models (e.g., high-degree polynomials) can lead to fitting noise rather than the underlying pattern, resulting in poor generalization to new data.\n",
    "\n",
    "<!-- A standard approach is to split the available dataset into three parts:\n",
    "- **Training set:** used to fit the model parameters.\n",
    "- **Test set:** used only for evaluating the model after training.\n",
    "- **Validation set:** used to test how well the  -->\n",
    "\n",
    "The model’s performance on the test set provides an estimate of how well it will perform on new, unseen data.\n",
    "\n",
    "Below, we’ll split our dataset from the first exercize about linear regression, train the model on the training data only, and compare the loss on both the training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "37b9bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset 80/20\n",
    "train_size = int(0.8 * len(x))\n",
    "\n",
    "x_train, x_valid = x[:train_size], x[train_size:]\n",
    "y_train, y_valid = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "abe0c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent coefficients: theta0 = [2.1299999], theta1 = [2.7517393]\n"
     ]
    }
   ],
   "source": [
    "n_iters = 200\n",
    "lr = 0.03\n",
    "\n",
    "theta0, theta1, loss_history = gradient_descent_jax(key, x_train, y_train, lr, n_iters)\n",
    "\n",
    "print(rf\"Gradient Descent coefficients: theta0 = {round(theta0,2)}, theta1 = {theta1}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "16125cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Gradient Descent for the training data: 0.040729832\n",
      "MSE for Gradient Descent for the validation data: 0.036479842\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "y_pred_train = theta0 + theta1 * x_train\n",
    "y_pred_valid = theta0 + theta1 * x_valid \n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
    "\n",
    "print(\"MSE for Gradient Descent for the training data:\", mse_train)\n",
    "print(\"MSE for Gradient Descent for the validation data:\",  mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1acf8",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1 - Polinomial Regression\n",
    "\n",
    "In this exercise, you will implement polynomial regression using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0976e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "def polynomial_regression_dataset(\n",
    "        n_observations: int     = 200,\n",
    "        noise: float            = 0.2,\n",
    "        c1_center: List[float]  = [-2, -1],\n",
    "        c2_center: List[float]  = [2, 1]\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f087ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_np(\n",
    "    y: np.ndarray, \n",
    "    y_bar: np.ndarray\n",
    "  ) -> float:\n",
    "  \"\"\"\n",
    "    Numpy-based function to compute the _normalized_ MSE loss.\n",
    "    Input:\n",
    "        y       : (np.ndarray) array of groundtruth classes\n",
    "        y_pred   : (np.ndarray) array of approximate classes\n",
    "    Returns:\n",
    "        loss    : (float) value of the MSE\n",
    "  \"\"\"\n",
    "  ######################\n",
    "  # YOUR CODE GOES HERE\n",
    "  ######################\n",
    "  return loss\n",
    "\n",
    "def infer_np(\n",
    "        x: np.ndarray,\n",
    "        theta0: float, \n",
    "        tetha1: float,\n",
    "        degree: int\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Numpy-based function to classify input points (model function)\n",
    "    Input:\n",
    "        x       : (np.ndarray) array of input observations\n",
    "        theta0      : (float) bias\n",
    "        theta1      : (float) coefficient weight 1\n",
    "        theta2      : (float) coefficient weight 2\n",
    "    Returns:\n",
    "        y_bar   : (np.ndarray) array of infered classes (-1 / +1)\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return y_pred\n",
    "\n",
    "def gd_regression_np(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray, \n",
    "    n_iter: int, \n",
    "    lr: float, \n",
    "    degree: int,\n",
    "    loss_func: Callable[[np.ndarray, np.ndarray], np.ndarray], \n",
    "    seed: int = 0\n",
    "  ) -> Tuple[float, float, float, List[float]]:\n",
    "  \"\"\"\n",
    "    This function performs gradient descent for linear classification model.\n",
    "    Input:\n",
    "        x               : (np.ndarray) an array of observations\n",
    "        y               : (np.ndarray) an array of classes corresponding to the observations\n",
    "        n_iter          : (int) number of iterations\n",
    "        lr              : (float) learning rate\n",
    "        degree          : (int) polynomial degree\n",
    "        loss_function   : (function) loss function\n",
    "        seed            : (int) random number generator seed\n",
    "    Ouput:\n",
    "        theta0              : (float) bias\n",
    "        tetha1              : (float) coefficient weight 1\n",
    "        loss_history    : (list) an array of loss values obtained from each iteration\n",
    "  \"\"\"\n",
    "  ######################\n",
    "  # YOUR CODE GOES HERE\n",
    "  ######################\n",
    "  return theta0, theta1, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa914006",
   "metadata": {},
   "source": [
    "### Exercise 2 - Normal Equations for linear regression\n",
    "Instead of using gradient descent to find $\\boldsymbol{\\theta}$, we can derive a closed-form solution by setting the gradient of the MSE loss to zero. This leads to the Normal Equation:\n",
    "$$\n",
    "\\boldsymbol{\\theta}^* = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "$$\n",
    "\n",
    "1. Create a dataset:\n",
    "    Create a simple linear relationship between $x$ and $y$: $ y = 4x + 1 + \\epsilon $.\n",
    "    Use 50 samples.\n",
    "\n",
    "2. Construct the design matrix\n",
    "\n",
    "3. Implement the Normal Equation\n",
    "\n",
    "4. Make predictions\n",
    "\n",
    "5. Visualize the results**\n",
    "   - Plot the original data points and the fitted regression line.\n",
    "   - Optionally compute and print the **Mean Squared Error (MSE)** of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4aece635",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
