{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3698e7d5",
   "metadata": {},
   "source": [
    "# Lesson 1: Machine learning fundamentals\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover \n",
    "1. A recap on the concept of machine learning\n",
    "2. Creating our own dataset\n",
    "3. A detailed implementation of simple linear regression\n",
    "4. A detailed implementation of simple binary classification\n",
    "5. An explanation on Train–Test Split and Model Evaluation\n",
    "6. Two exercises to explore polynomial regression and Normal Equations for linear regression\n",
    "\n",
    "This script was inspired by the Creative Machine Learning course of Philippe Esling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9020ae",
   "metadata": {},
   "source": [
    "# Defining Machine Learning\n",
    "\n",
    "Machine learning aims to approximate an unknown mapping between input and output spaces, $\\mathcal{X} \\mapsto \\mathcal{Y}$,\n",
    "given only a finite set of Data\n",
    "$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{N},\n",
    "$\n",
    "where $(x_i \\in \\mathcal{X})$ and $(y_i \\in \\mathcal{Y})$.\n",
    "\n",
    "We define a **parametric model**\n",
    "$ f_{\\boldsymbol{\\theta}} \\in \\mathcal{F}$,\n",
    "with parameters $(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta})$, chosen such that\n",
    "$\n",
    "f_{\\boldsymbol{\\theta}}(x) \\approx y$.\n",
    "\n",
    "\n",
    "Any machine learning problem involves these core components:\n",
    "1. **Dataset** $(\\mathcal{D})$: A collection of input-output pairs $ \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1} $, The dataset must be representative of the true underlying function $(f^*: \\mathcal{X} \\mapsto \\mathcal{Y})$  \n",
    "2. **Function or Model** $(f_{\\boldsymbol{\\theta}})$: A parameterized function that maps inputs $\\mathbf{x}_i$ to predicted outputs $\\mathbf{\\hat{y}}_i = f_{{\\theta}}(\\mathbf{x_i})$ The choice of function defines the function space $\\mathcal{F}_{\\Theta}$\n",
    "3. **Parameters ($\\theta$)**: The set of parameters that define the specific function within the function space. These parameters are adjusted during training to minimize the empirical risk. come from the parameter space $\\boldsymbol{\\Theta}$\n",
    "4. **Lossfunction**: A function that quantifies the difference between the predicted outputs $\\mathbf{\\hat{y}}_i$ and the true labels $\\mathbf{y}_i$. The choice of loss function depends on the task (e.g., regression vs. classification).\n",
    "5. **Optimization Algorithm**: A method for adjusting the parameters $\\boldsymbol{\\theta}$ to minimize the empirical risk $\\hat{R}(\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1557ba4",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "To understand how a machine learning model works mathematically, we start with the case of simple linear regression, where $\\mathbf{x}\\in\\mathbb{R}$. This implies that our model will follow\n",
    "$$f_{\\theta}(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "We will measure the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}(\\theta) = \\frac{1}{N}\\sum_{i=1}^{N} ( y_{i} - f_{\\theta}(x_i))^{2} = \\frac{1}{N} \\sum_{i=1}^{N}  ( y_{i} - (\\theta_{0} + \\theta_{1} x_{i}) )^{2}$$\n",
    "\n",
    "Then our goal is to find the most adequate set of parameters $\\theta = \\{\\theta_{0}, \\theta_{1}\\}$, which are those that minimize the MSE loss defined previously. To do so, we will implement the gradient descent algorithm.\n",
    "\n",
    "### Manual implementation in NumPy\n",
    "\n",
    "We start by performing a full manual implementation, in the sense that we need to manually derive the gradient in order to apply the gradient descent updates. To do so, we will rely on Numpy.\n",
    "\n",
    "We start by importing libraries and set a random seed to ensure that the random number generator produces always a reproducible series of random numbers.\n",
    "\n",
    "**Used functions**\n",
    "- `np.random.seed`: sets the seed for the NumPy random number generator. [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41625c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2a87f",
   "metadata": {},
   "source": [
    "### Generate a synthetic dataset.\n",
    "\n",
    "For the sake of this exercise, we will generate the data ourselves, so that we know the true values that we are looking for in advance. As in most real-world cases, our measurements are not perfect, they contain some amount of observation noise $\\epsilon$ . Hence, we define a linear relationship following\n",
    "$$y_i = f^*(\\mathbf{x}_i) + \\epsilon_i$$\n",
    "\n",
    "**Useful functions**\n",
    "- `np.random.rand`: generates an array of random numbers uniformly distributed over [0, 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
    "- `np.random.randn`: generates an array of random numbers from the standard normal distribution (mean 0, variance 1). [Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html)\n",
    "- `np.random.uniform`\" draws samples from a uniform distribution. [Documentation](https://numpy.org/doc/2.1/reference/random/generated/numpy.random.uniform.html)\n",
    "\n",
    "Below, we will visualize synthetic data with generated noise levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the parameters\n",
    "true_theta_0 = 4\n",
    "true_theta_1 = -3\n",
    "n_obs = 100\n",
    "\n",
    "epsilon = np.random.randn(n_obs) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the data set\n",
    "def generate_data(true_theta_0, true_theta_1, eps=0.7, n_obs=100):\n",
    "    x = np.linspace(0, 1, n_obs) \n",
    "    y =  true_theta_1 * x + true_theta_0 +  eps * epsilon\n",
    "    return x, y\n",
    "\n",
    "# Plotting the data and the true function\n",
    "def plot_data(eps=0.7):\n",
    "    x, y = generate_data(true_theta_0, true_theta_1, eps, n_obs)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.scatter(x, y, color=\"C0\", label=\"Created data\")\n",
    "    plt.plot(x, (true_theta_1*x + true_theta_0), color=\"C1\", label=\"True function\" )\n",
    "    plt.title(rf\"$y = {true_theta_1:.2f}*x + {true_theta_0:.2f} + ϵ, ϵ \\in[-{eps:.2f},{eps:.2f}]$\")\n",
    "    plt.xlabel(\"$x$\")\n",
    "    plt.ylabel(\"$y$\")\n",
    "    plt.xlim(0, 1)  \n",
    "    plt.ylim(0, 5)\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', lw=1.5) \n",
    "    plt.axvline(0, color='black', lw=1.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot with interactive slider for epsilon\n",
    "interact(\n",
    "    plot_data,\n",
    "    eps=widgets.FloatSlider(value=0.3, min=0.0, max=2, step=0.05, description=\"ϵ range\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8b8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set for the linear regression with set epsilon\n",
    "true_theta_0 = 2\n",
    "true_theta_1 = 3\n",
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 0.2\n",
    "\n",
    "x = np.linspace(0, 1, n_obs)\n",
    "y = (true_theta_1 * x + true_theta_0) + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa98deb",
   "metadata": {},
   "source": [
    "### Computing the MSE loss\n",
    "\n",
    "The next step is to calculate the errors made by our model by using the Mean Squared Error (MSE) loss, defined as \n",
    "$$\\mathcal{L}_{MSE}\\left( \\hat{\\mathbf{y}},\\theta \\right) = \\sum_{i=1}^{n} ( y_{i} - \\hat{y}_{i} )^{2} = \\sum_{i=1}^{n} ( y_{i} - ( \\theta_{1} x_{i} + \\theta_{0}) )^{2}$$\n",
    "\n",
    "This will allow us to evaluate the performances of our model, but is also the basis for the gradient descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6885d97",
   "metadata": {},
   "source": [
    "### Implement the gradient descent algorithm.\n",
    "\n",
    "First we have to Iinitialize the parameters $\\theta_1$ and $\\theta_0$ to small random values\n",
    "1. Evaluate the predictions made by our model \n",
    "$$\\hat{y} = \\theta_{1}x + \\theta_0$$\n",
    "2. Compute the mean squared error (MSE) loss between the predicted values and the ground truth labels: \n",
    "$$\\mathcal{L}_{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i} )^2$$\n",
    "3. Compute the gradients of the loss with respect to the parameters: \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} = -\\frac{2}{N} \\sum_{i} (y_{i} - \\hat{y}_{i})*x_{i}$$\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_0} = -\\frac{2}{N} \\sum_{i} (y_{i} -\\hat{y}_{i})$$\n",
    "4. Update the parameters using the gradients and a learning rate $\\eta$: \n",
    "$$\\theta_1 \\leftarrow \\theta_1 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}$$\n",
    "\n",
    "$$\\theta_0 \\leftarrow \\theta_0 - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta_0}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dac220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def mse_loss_np(y, y_pred):\n",
    "    return np.mean((y - y_pred)** 2)\n",
    "\n",
    "def model_np(x, theta_0, theta_1):\n",
    "    return theta_1 * x + theta_0\n",
    "\n",
    "# Manually calculated gradients of the MSE loss function\n",
    "def np_grad_loss_function(y,y_pred, x):\n",
    "    dL_dtheta_1 = - 2 * np.mean((y - y_pred) * x)\n",
    "    dL_dtheta_0 = - 2 * np.mean((y - y_pred ))\n",
    "    return dL_dtheta_0, dL_dtheta_1\n",
    "\n",
    "# Optimization with gradient descent\n",
    "def gradient_descent_np(x, y, lr, n_iter):\n",
    "    #  Initializing the parameters\n",
    "    theta_1 = np.random.randn()\n",
    "    theta_0 = np.random.randn()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Gradient Descent Loop \n",
    "    for i in range(n_iter):\n",
    "        # Predictions\n",
    "        y_pred = model_np(x, theta_0, theta_1)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dL_dtheta_0, dL_dtheta_1 = np_grad_loss_function(y,y_pred,x)\n",
    "\n",
    "        # Update parameters\n",
    "        theta_1 = theta_1 - lr * dL_dtheta_1\n",
    "        theta_0 = theta_0 - lr * dL_dtheta_0\n",
    "\n",
    "        # Compute Mean Squared Error loss\n",
    "        loss_history.append(mse_loss_np(y, y_pred))\n",
    "\n",
    "    return theta_0, theta_1, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2658494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 200\n",
    "lr = 0.05\n",
    "\n",
    "theta_0, theta_1, loss_history = gradient_descent_np(x, y, lr, n_iters)\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x, y, color=\"C0\", label=\"Data\")\n",
    "plt.plot(x, true_theta_1 * x + true_theta_0, color=\"C1\", lw=2, label=\"True linear regreassion\")\n",
    "plt.plot(x, theta_1 * x + theta_0, color=\"C2\", lw=2, linestyle=\"--\", label=rf\"Learned model ($\\theta_1$={theta_1:.2f}, $\\theta_0$={theta_0:.2f})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Linear Regression with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a5f64",
   "metadata": {},
   "source": [
    "### Implementation with Jax\n",
    "\n",
    "The previous implementation required us to perform manual differentiation of our loss function to understand how to update the parameters. However, large developments have been made in the field of automatic differentiation. \n",
    "\n",
    "The recent library [JAX](https://github.com/google/jax) extends NumPy with this automatic differentiation feature, while providing a functional approach to numerical computing, allowing for easy gradient computation. Its ability to handle complex and custom gradients makes JAX particularly well-suited for advanced research projects. Similar to NumPy, we strongly encourage you to learn JAX through the set of [tutorials](https://jax.readthedocs.io/en/latest/).\n",
    "\n",
    "Note that `JAX` has been thought as an extension of `NumPy`, therefore an extremely large portion of its API simply mirrors the `NumPy` functions by adding automatic differentiation features to it.\n",
    "\n",
    "**Useful functions**\n",
    "- `random.PRNGKey`: function to generate a key for the pseudorandom number generator. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.PRNGKey.html)\n",
    "- `jit`: function to compile a function for faster execution [Documentation](https://jax.readthedocs.io/en/latest/jit.html)\n",
    "- `grad`: function to compute the gradient of a function [Documentation](https://docs.jax.dev/en/latest/_autosummary/jax.grad.html#jax.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124d4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, random\n",
    "\n",
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c444b85",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We can simply keep our previous approach to generating the dataset but we will rely on `jnp.ndarray` instead of `np.ndarray`. \n",
    "\n",
    "**Useful functions**\n",
    "- `random.split`: function to split a PRNGKey into a list of subkeys [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html)\n",
    "- `random.uniform`: function to generate an array of uniformly distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html)\n",
    "- `random.normal`: function to generate an array of normally distributed random numbers [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7c317e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the key and uses the keys to create data points\n",
    "key_0, key_1, key = random.split(key, 3)\n",
    "x = random.uniform(key_0, (n_obs,))\n",
    "epsilon = random.normal(key_1, (n_obs,)) * 0.2\n",
    "y = true_theta_0 + true_theta_1 * x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8afe5b",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Similarly we have to define the loss function and its gradient.\n",
    "\n",
    "**Useful function**\n",
    "- `jnp.mean`: function to compute the mean of an array [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d276632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss function\n",
    "def mse_loss_jax(y, y_pred):\n",
    "    return jnp.mean((y - y_pred)**2)\n",
    "\n",
    "def model_jax(x, theta_0, theta_1):\n",
    "    return theta_1 * x + theta_0\n",
    "\n",
    "# Combined loss function that takes parameters directly\n",
    "# This is needed for jax's grad function\n",
    "def loss_with_params_jax(theta_0, theta_1, x, y):\n",
    "    y_pred = model_jax(x, theta_0, theta_1)\n",
    "    return mse_loss_jax(y, y_pred)\n",
    "\n",
    "# Optimization with gradient descent using jax's grad function \n",
    "def gradient_descent_jax(key, x, y, lr, n_iter):\n",
    "    # Initializing the parameters\n",
    "    k_0, k_1 = random.split(key, 2)\n",
    "    theta_0 = random.normal(k_0, (1,))\n",
    "    theta_1 = random.normal(k_1, (1,))\n",
    "    loss_history = []\n",
    "\n",
    "    # Compute gradients with respect to theta_0 (arg 0) and theta_1 (arg 1)\n",
    "    grad_loss_function = grad(loss_with_params_jax, argnums=[0, 1])\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients with respect to parameters\n",
    "        dL_dtheta = grad_loss_function(theta_0, theta_1, x, y)\n",
    "        \n",
    "        # Update the parameters\n",
    "        theta_0 -= lr * dL_dtheta[0]\n",
    "        theta_1 -= lr * dL_dtheta[1]\n",
    "\n",
    "        # Compute predictions and loss for monitoring\n",
    "        y_pred = model_jax(x, theta_0, theta_1)\n",
    "        loss_history.append(mse_loss_jax(y, y_pred))\n",
    "\n",
    "    return theta_0, theta_1, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iter = 200\n",
    "lr = 0.05\n",
    "\n",
    "theta_0, theta_1, loss_history = gradient_descent_jax(key,x, y, lr, n_iters)\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x, y, color=\"C0\", label=\"Data\")\n",
    "plt.plot(x, (true_theta_1 * x + true_theta_0), color=\"C1\", lw=2, label=\"True linear regreassion\")\n",
    "plt.plot(x, theta_1 * x + theta_0, color=\"C2\", lw=2, linestyle=\"--\", label=rf\"Learned model ($\\theta_1$={theta_1[0]:.2f}, $\\theta_0$={theta_0[0]:.2f})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Linear Regression with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33272143",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "In binary classification, the goal is to assign each input $ \\mathbf{x} $ in $ \\mathbb{R}^d $ to one of two possible classes, typically represented as $ y \\in \\{-1, +1\\} $.\n",
    "\n",
    "For this exercise we will use a linear classifier model. The relationship between inputs and outputs use a linear decision function:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "$$\n",
    "\n",
    "The predicted label is obtained from the sign of this function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign}(f_{\\theta}(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "The parameters $ \\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2) $ are estimated by minimizing the Hinge loss function that penalizes incorrect or uncertain predictions.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a37a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data set\n",
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 2\n",
    "\n",
    "# Input\n",
    "x_class_0_0 = np.linspace(0, 1.1, n_obs)\n",
    "x_class_1_0 = np.linspace(0.5, 2, n_obs)\n",
    "\n",
    "# Output \n",
    "x_class_0_1 = -3 * x_class_0_0**3 + 13 + epsilon\n",
    "x_class_1_1 = 2 * x_class_1_0**3 + 2 + epsilon\n",
    "\n",
    "X = np.vstack([\n",
    "    np.stack([x_class_0_0, x_class_0_1], axis=1),\n",
    "    np.stack([x_class_1_0, x_class_1_1], axis=1)\n",
    "])\n",
    "Y = np.concatenate([\n",
    "    -np.ones_like(x_class_0_0),\n",
    "    np.ones_like(x_class_1_0)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d743e8f",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "\n",
    "The Hinge Loss function is Loss function that is commonly used in binary classification models. \n",
    "\n",
    "\n",
    "\n",
    "For a single data point $(x_i, y_i)$, where $ y_i \\in \\{-1, +1\\} $, the hinge loss is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L_{hinge}} = \\frac{1}{N} \\sum{i}^N \\max(0, 1 - y_i f_{\\theta}(x_i))\n",
    "$$\n",
    "\n",
    "- If the prediction $f(x_i)$ has the same sign as $ y_i $ and its magnitude is ≥ 1, then the loss is 0: the point is correctly classified and outside the margin.  \n",
    "- If the prediction is incorrect or within the margin, the loss increases linearly as $f(x_i)$ moves away from the correct side.\n",
    "\n",
    "\n",
    "This time we will use Stochastic Gradient Descent (SGD) as the optimazation algorithm. With SGD parameters are updated using gradient from a single randomly selected sample per iteration.\n",
    "\n",
    "### Gradient Descent vs. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Both gradient descent and stochastic gradient descent (SGD) are optimization algorithms used to minimize a loss function by iteratively updating model parameters in the direction of the negative gradient.\n",
    "\n",
    "**Gradient Descent**\n",
    "- Uses all training samples to compute the gradient at each iteration.  \n",
    "- Provides a precise estimate of the true gradient.  \n",
    "- Updates are smooth and stable, but computation can be expensive for large datasets.  \n",
    "- The parameter update rule is:\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} - \\eta \\nabla_\\theta \\hat{R}(\\boldsymbol{\\theta_t}) = \\boldsymbol{\\theta_t} - \\frac{\\eta}{N} \\displaystyle\\sum_{i=1}^N \\nabla_\\theta \\ell(f_{\\theta_t}(x_{i_t}) , y_{i_t})\n",
    "  $$\n",
    "\n",
    "  where $ \\eta $ is the learning rate and $ \\ell$ is a single sample loss function.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**\n",
    "- Uses one randomly chosen training sample per iteration to approximate the gradient.  \n",
    "- Each update is fast, but the gradient estimate is noisy.  \n",
    "- The loss fluctuates more during training but still tends to decrease on average.  \n",
    "- This noise can help escape shallow local minima and improve generalization.\n",
    "\n",
    "The update rule becomes:\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\ell(f_{\\theta_t}(x_{i_t}) , y_{i_t})\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "<!-- #### Summary\n",
    "\n",
    "| Method | Gradient Computation | Stability  Computation per Step | Typical Use |\n",
    "|---------|----------------------|------------|----------------------|--------------|\n",
    "| Gradient Descent | Full dataset | Smooth, stable | High | Small datasets |\n",
    "| Stochastic Gradient Descent | One sample | Noisy, oscillatory | Very low | Large datasets |\n",
    "\n",
    "In practice, many modern algorithms use a compromise called **mini-batch SGD**, which computes the gradient on a small random subset of the data to balance efficiency and stability. -->\n",
    "\n",
    "\n",
    "### Numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f09e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss function\n",
    "def hinge_loss_np(y, y_pred):\n",
    "    losses = 1 - y * y_pred\n",
    "    losses = np.maximum(0, losses)\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Manually calculated gradients of the Hinge loss function \n",
    "def hinge_gradient_np(x_i, y_i, theta_0, theta_1, theta_2):\n",
    "    f_i = theta_0 + theta_1 * x_i[0] + theta_2 * x_i[1]\n",
    "    if 1 - y_i * f_i > 0:  # only if violating margin\n",
    "        dtheta_0 = -y_i\n",
    "        dtheta_1 = -y_i * x_i[0]\n",
    "        dtheta_2 = -y_i * x_i[1]\n",
    "    else:\n",
    "        dtheta_0 = dtheta_1 = dtheta_2 = 0.0\n",
    "    return dtheta_0, dtheta_1, dtheta_2\n",
    "\n",
    "# Optimization with stochastic gradient descent using numpy \n",
    "def stochastic_gradient_descent_np(x, y, lr=0.05, n_iter=1000):\n",
    "    theta_0 = np.random.randn()\n",
    "    theta_1 = np.random.randn()\n",
    "    theta_2 = np.random.randn()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        i_t = np.random.randint(len(x))\n",
    "        x_i = x[i_t]\n",
    "        y_i = y[i_t]\n",
    "\n",
    "        \n",
    "        # Compute loss (for monitoring)\n",
    "        y_pred_full = theta_0 + theta_1 * x[:,0] + theta_2 * x[:,1]\n",
    "        loss_history.append(hinge_loss_np(y, y_pred_full))\n",
    "\n",
    "\n",
    "        # Gradients\n",
    "        dL_dtheta_0, dL_dtheta_1, dL_dtheta_2 = hinge_gradient_np(x_i, y_i, theta_0, theta_1, theta_2)\n",
    "\n",
    "        # Update parameters\n",
    "        theta_0 -= lr * dL_dtheta_0\n",
    "        theta_1 -= lr * dL_dtheta_1\n",
    "        theta_2 -= lr * dL_dtheta_2\n",
    "        theta = [theta_0,theta_1,theta_2]\n",
    "\n",
    "    return theta, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 5000\n",
    "lr = 0.003\n",
    "\n",
    "theta, loss_history = stochastic_gradient_descent_np(X, Y, lr, n_iters)\n",
    "\n",
    "theta_0, theta_1, theta_2 = theta\n",
    "\n",
    "x_line = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "y_line = -(theta_0 + theta_1 * x_line ) / theta_2\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_class_0_0, x_class_0_1, color=\"C0\", label=\"Class 0\")\n",
    "plt.scatter(x_class_1_0, x_class_1_1, color=\"C1\", label=\"Class 1\")\n",
    "plt.plot(x_line, y_line, color=\"black\", lw=2, label=rf\"sign boundary ($\\theta_0 = {round(theta_0,2)}$, $\\theta_1 = ${round(theta_1,2)}, $\\theta_2 = ${round(theta_2,2)})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(rf\"Binary classification with Gradient Descent ({n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20442d7",
   "metadata": {},
   "source": [
    "### Jax Implementation\n",
    "\n",
    "We will repeat the same exercize using the JAX library to get a better understanding of JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e8cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "epsilon = random.normal(key, (n_obs,)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7d12619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge loss function  \n",
    "def hinge_loss_jax(theta, x, y):\n",
    "    theta_0, theta_1, theta_2 = theta\n",
    "    f = theta_0 + theta_1 * x[:, 0] + theta_2 * x[:, 1]\n",
    "    losses = 1 - y * f\n",
    "    losses = jnp.maximum(0, losses)\n",
    "    return jnp.mean(losses)\n",
    "\n",
    "# Compute gradient for one-sample loss\n",
    "def single_loss_jax(theta, x_i, y_i):\n",
    "    f_i = theta[0] + theta[1] * x_i[0] + theta[2] * x_i[1]\n",
    "    return jnp.maximum(0, 1 - y_i * f_i)\n",
    "\n",
    "# Optimization with stochastic gradient descent using jax's grad\n",
    "def stochastic_gradient_descent_jax(key, x, y, lr=0.05, n_iter=1000):\n",
    "    theta = random.normal(key, (3,))  # [theta_0, theta_1, theta_2]\n",
    "    \n",
    "    loss_history = []\n",
    "    for i in range(n_iters):\n",
    "        # Sample one random observation\n",
    "        key, subkey = random.split(key)\n",
    "        i_t = random.randint(subkey, shape=(), minval=0, maxval=len(x))\n",
    "        x_i = x[i_t]\n",
    "        y_i = y[i_t]\n",
    "\n",
    "\n",
    "        grads = grad(single_loss_jax)(theta, x_i, y_i)\n",
    "\n",
    "        # Parameter update\n",
    "        theta = theta - lr * grads\n",
    "\n",
    "        # Compute full loss (for monitoring)\n",
    "        loss_history.append(hinge_loss_jax(theta, x, y))\n",
    "\n",
    "    return theta, loss_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of iterations and learning rate\n",
    "n_iters = 5000\n",
    "lr = 0.003\n",
    "\n",
    "theta, loss_history = stochastic_gradient_descent_jax(key, X, Y, lr, n_iters)\n",
    "\n",
    "theta_0, theta_1, theta_2 = theta\n",
    "\n",
    "x_line = jnp.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "y_line = -(theta_0 + theta_1 * x_line) / theta_2\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_class_0_0, x_class_0_1, color=\"C0\", label=\"Class 0\")\n",
    "plt.scatter(x_class_1_0, x_class_1_1, color=\"C1\", label=\"Class 1\")\n",
    "plt.plot(x_line, y_line, color=\"black\", lw=2, label=\"Decision boundary\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(f\"Binary Classification with SGD (JAX, {n_iters} iterations)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over iterations\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(loss_history, color=\"C3\", lw=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Hinge Loss\")\n",
    "plt.title(\"Training Loss (SGD, JAX)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77905f29",
   "metadata": {},
   "source": [
    "## Train–Valid-Test Split and Model Evaluation\n",
    "\n",
    "To properly assess how well a machine learning model generalizes, we must evaluate it on data it has not seen during training.  \n",
    "If we only measure performance on the training data, there is a risk of overfitting the model. Overfitting the model means using overly complex models (e.g., high-degree polynomials) can lead to fitting noise rather than the underlying pattern, resulting in poor generalization to new data.\n",
    "\n",
    "A standard approach is to split the available dataset into three parts:\n",
    "- **Training set:** used to fit the model parameters.\n",
    "- **Validation set:** used for an unbiased evaluation of the model\n",
    "- **Test set:** used for the final evaluation of the model. This data is only used at the very end.\n",
    "\n",
    "\n",
    "Below, we’ll split our dataset from the first exercise on linear regression into two data sets, a training and a validation set, train the model on the training data, and compare the loss on both the training and validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37b9bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset 80/20\n",
    "train_size = int(0.8 * len(x))\n",
    "\n",
    "x_train, x_valid = x[:train_size], x[train_size:]\n",
    "y_train, y_valid = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe0c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent coefficients: theta_0 = [2.1299999], theta_1 = [2.7517393]\n"
     ]
    }
   ],
   "source": [
    "n_iters = 200\n",
    "lr = 0.03\n",
    "\n",
    "theta_0, theta_1, loss_history = gradient_descent_jax(key, x_train, y_train, lr, n_iters)\n",
    "\n",
    "print(rf\"Gradient Descent coefficients: theta_0 = {round(theta_0,2)}, theta_1 = {theta_1}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16125cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Gradient Descent for the training data: 0.040729832\n",
      "MSE for Gradient Descent for the validation data: 0.036479842\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "y_pred_train = theta_0 + theta_1 * x_train\n",
    "y_pred_valid = theta_0 + theta_1 * x_valid \n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
    "\n",
    "print(\"MSE for Gradient Descent for the training data:\", mse_train)\n",
    "print(\"MSE for Gradient Descent for the validation data:\",  mse_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48136e56",
   "metadata": {},
   "source": [
    "Since the model is fitted well we see that the MSE is almost the same for both the training set and the validation set. When we overfit this error will be bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1acf8",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1 - Polynomial Regression\n",
    "\n",
    "In this exercise, you will implement polynomial regression. You can choose to use NumPy or JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976e95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 100\n",
    "epsilon = np.random.randn(n_obs) * 2\n",
    "\n",
    "x = np.linspace(0, 1, n_obs)\n",
    "y = np.sin(2 * np.pi * x) + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268db63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, thetas):\n",
    "    \"\"\"\n",
    "    Compute polynomial predictions for input x.\n",
    "    Inputs:\n",
    "        x       : array of input values\n",
    "        thetas  : array of parameters [θ₀, θ₁, ..., θ_d]\n",
    "    Returns:\n",
    "        y_pred  : array of predicted values\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def mse_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Error (MSE) loss.\n",
    "    Inputs:\n",
    "        y       : array of true target values\n",
    "        y_pred  : array of predicted target values\n",
    "    Returns:\n",
    "        loss    : float, mean squared error\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return loss\n",
    "\n",
    "def np_grad_loss_function(x, y, y_pred):\n",
    "   \"\"\"\n",
    "    Compute the gradient of the loss function.\n",
    "    Inputs:\n",
    "        x       : array of input values\n",
    "        y       : array of true target values\n",
    "        y_pred  : array of predicted target values\n",
    "    Returns:\n",
    "        dL_dtheta  : array of gradients to update the parameters\n",
    "    \"\"\"\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "    return dL_dtheta\n",
    "\n",
    "def gd_regression(x, y, n_iter, lr, degree, loss_func, seed=0):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for polynomial regression.\n",
    "    Inputs:\n",
    "        x          : array of observations\n",
    "        y          : array of target values\n",
    "        n_iter     : number of iterations\n",
    "        lr         : learning rate\n",
    "        degree     : polynomial degree\n",
    "        loss_func  : function to compute loss (e.g. mse_loss)\n",
    "        seed       : random seed\n",
    "    Returns:\n",
    "        thetas        : array of learned coefficients [θ₀, θ₁, ..., θ_d]\n",
    "        loss_history  : list of loss values per iteration\n",
    "    \"\"\"\n",
    "    # Random initialization\n",
    "\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        pass\n",
    "\n",
    "    return thetas, loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dl4ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
