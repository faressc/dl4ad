{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "095768b1",
   "metadata": {},
   "source": [
    "# Lesson 4: Recurrent layers\n",
    "\n",
    "*Teachers:* Fares Schulz, Lina Campanella\n",
    "\n",
    "In this course we will cover:\n",
    "1. Building a hybrid CNN-LSTM architecture for Bach chorale generation\n",
    "2. Temperature-controlled sampling techniques for creative AI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bec742",
   "metadata": {},
   "source": [
    "## Generating Bach Chorales\n",
    "\n",
    "Disclaimer: This example is in large part taken from Aurélien Gérons book Hands-On Machine Learning, you can find the original implementation [here](https://github.com/ageron/handson-ml2/blob/master/15_processing_sequences_using_rnns_and_cnns.ipynb).\n",
    "\n",
    "For this exercise we will use the JSB Chorales dataset. It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long, and each time step contains 4 integers, where each integer corresponds to a midi note (except for the value 0, which means that no note is played). We will train a model with both convolutional and recurrent layers, that can predict the next time step (four notes), given a sequence of time steps from a chorale. Then use this model to generate Bach-like music, one note at a time: you can do this by giving the model the start of a chorale and asking it to predict the next time step, then appending these time steps to the input sequence and asking the model for the next note, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset using urllib and extract with tarfile\n",
    "download_link = \"https://github.com/iCorv/jsb-chorales-dataset/raw/main/jsb_chorales.tar\"\n",
    "data_dir = Path('resources/_data/jsb_chorales')\n",
    "tar_path = data_dir / 'jsb_chorales.tar'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the file if it doesn't already exist\n",
    "if not tar_path.exists():\n",
    "    print(f\"Downloading dataset from {download_link}\")\n",
    "    urllib.request.urlretrieve(download_link, tar_path)\n",
    "    print(f\"Downloaded to {tar_path}\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {tar_path}\")\n",
    "\n",
    "# Extract the tar file\n",
    "if tar_path.exists() and not (data_dir / 'jsb_chorales').exists():\n",
    "    print(f\"Extracting {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r') as tar:\n",
    "        tar.extractall(path=data_dir)\n",
    "    print(f\"Extracted to {data_dir}\")\n",
    "\n",
    "filepath = str(tar_path)\n",
    "print(f\"Dataset available at: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsb_chorales_dir = Path(filepath).parent\n",
    "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
    "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
    "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79061695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chorales(filepaths):\n",
    "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
    "\n",
    "train_chorales = load_chorales(train_files)\n",
    "valid_chorales = load_chorales(valid_files)\n",
    "test_chorales = load_chorales(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c869c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = set()\n",
    "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
    "    for chorale in chorales:\n",
    "        for chord in chorale:\n",
    "            notes |= set(chord)\n",
    "\n",
    "n_notes = len(notes)\n",
    "min_note = min(notes - {0})\n",
    "max_note = max(notes)\n",
    "\n",
    "assert min_note == 36\n",
    "assert max_note == 81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c3fd9",
   "metadata": {},
   "source": [
    " Notes range from 36 (C1 = C on octave 1) to 81 (A5 = A on octave 5), plus 0 for silence. This is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45074869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chorales are saved as a chord progressions\n",
    "train_chorales[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c81f1e",
   "metadata": {},
   "source": [
    " We use this simple numpy synthesizer to play some of the chorales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93319fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources._code.synthesizer import SimpleSynth\n",
    "\n",
    "baroque_synth = SimpleSynth(tempo=160, amplitude=0.1, sample_rate=44100, baroque_tuning=True)\n",
    "devine_synth = SimpleSynth(tempo=160, amplitude=0.1, sample_rate=44100, baroque_tuning=False)\n",
    "\n",
    "for idx in range(1):\n",
    "    baroque_synth.play_chorale(train_chorales[idx])\n",
    "    devine_synth.play_chorale(train_chorales[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d1184",
   "metadata": {},
   "source": [
    "In order to be able to generate new chorales, we want to train a model that can predict the next chord given all the previous chords. If we naively try to predict the next chord in one shot, predicting all 4 notes at once, we run the risk of getting notes that don't go very well together. It's much better and simpler to predict one note at a time. So we will need to preprocess every chorale, turning each chord into an arpegio (i.e., a sequence of notes rather than notes played simultaneuously). So each chorale will be a long sequence of notes (rather than chords), and we can just train a model that can predict the next note given all the previous notes. We will use a sequence-to-sequence approach, where we feed a window to the neural net, and it tries to predict that same window shifted one time step into the future.\n",
    "\n",
    "We will also shift the values so that they range from 0 to 46, where 0 represents silence, and values 1 to 46 represent notes 36 (C1) to 81 (A5).\n",
    "And we will train the model on windows of 128 notes (i.e., 32 chords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def preprocess(window):\n",
    "    # Shift values: keep 0 as 0 (silence), shift other notes to start from 1\n",
    "    window = torch.where(window == 0, window, window - min_note + 1)\n",
    "    return window.reshape(-1)  # convert to arpeggio (flatten to 1D sequence)\n",
    "\n",
    "class BachDataset(Dataset):\n",
    "    def __init__(self, chorales, window_size=32, window_shift=16):\n",
    "        self.chorales = chorales\n",
    "        self.window_size = window_size\n",
    "        self.window_shift = window_shift\n",
    "        self.windows = self._create_windows()\n",
    "    \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        for chorale in self.chorales:\n",
    "            chorale_tensor = torch.tensor(chorale, dtype=torch.long)\n",
    "            \n",
    "            # Create sliding windows\n",
    "            for i in range(0, len(chorale) - self.window_size, self.window_shift):\n",
    "                window = chorale_tensor[i:i + self.window_size + 1]\n",
    "                if len(window) == self.window_size + 1:  # Ensure full window\n",
    "                    windows.append(window)\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        # Preprocess: shift note values and flatten\n",
    "        preprocessed = preprocess(window)\n",
    "        \n",
    "        # Create input/target pairs \n",
    "        X = preprocessed[:-1]\n",
    "        Y = preprocessed[1:] # predict next note in each arpegio, at each step\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "def bach_dataloader(chorales, batch_size=32, shuffle=False, window_size=32, window_shift=16):\n",
    "    \n",
    "    dataset = BachDataset(chorales, window_size, window_shift)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8482579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "train_set = bach_dataloader(train_chorales, shuffle=True)\n",
    "valid_set = bach_dataloader(valid_chorales)\n",
    "test_set = bach_dataloader(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50792f",
   "metadata": {},
   "source": [
    "Now let's create the model:\n",
    "- We could feed the note values directly to the model, as floats, but this would probably not give good results. Indeed, the relationships between notes are not that simple: for example, if you replace a C3 with a C4, the melody will still sound fine, even though these notes are 12 semi-tones apart (i.e., one octave). Conversely, if you replace a C3 with a C#3, it's very likely that the chord will sound horrible, despite these notes being just next to each other. So we will use an Embedding layer to convert each note to a small vector representation. We will use 5-dimensional embeddings, so the output of this first layer will have a shape of [batch_size, window_size, 5].\n",
    "- We will then feed this data to a small WaveNet-like neural network, composed of a stack of 4 Conv1D layers with doubling dilation rates. We will intersperse these layers with BatchNormalization layers for faster better convergence.\n",
    "- Then one LSTM layer to try to capture long-term patterns.\n",
    "- And finally a Dense layer to produce the final note probabilities. It will predict one probability for each chorale in the batch, for each time step, and for each possible note (including silence). So the output shape will be [batch_size, window_size, 47]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d875ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "\n",
    "class Bach_Chorale_NN(nn.Module):\n",
    "    def __init__(self, n_notes=47, n_embedding_dims=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(n_notes, n_embedding_dims)\n",
    "        \n",
    "        # Conv1D layers with causal padding and batch normalization\n",
    "        self.conv1 = nn.Conv1d(n_embedding_dims, 32, kernel_size=2, dilation=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(32, 48, kernel_size=2, dilation=2)\n",
    "        self.bn2 = nn.BatchNorm1d(48)\n",
    "        self.conv3 = nn.Conv1d(48, 64, kernel_size=2, dilation=4)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.conv4 = nn.Conv1d(64, 96, kernel_size=2, dilation=8)\n",
    "        self.bn4 = nn.BatchNorm1d(96)\n",
    "        self.lstm = nn.LSTM(96, 256, batch_first=True) # LSTM layer\n",
    "        self.Linear1 = nn.Linear(256, n_notes) # Output layer\n",
    "        \n",
    "    def causal_pad(self, x, kernel_size, dilation):\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "        return F.pad(x, (padding, 0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len) with integer note indices\n",
    "        \n",
    "        # Embedding: (batch_size, seq_len) -> (batch_size, seq_len, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # Transpose for Conv1D: (batch_size, seq_len, embedding_dim) -> (batch_size, embedding_dim, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        # Conv1D layers with causal padding and batch normalization\n",
    "        x = self.causal_pad(x, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.causal_pad(x, 2, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.causal_pad(x, 2, 4)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.causal_pad(x, 2, 8)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        # Transpose back for LSTM: (batch_size, channels, seq_len) -> (batch_size, seq_len, channels)\n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, 256)\n",
    "        \n",
    "        output = self.Linear1(lstm_out)  # (batch_size, seq_len, n_notes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Bach_Chorale_NN(n_notes=47, n_embedding_dims=5)\n",
    "\n",
    "torchinfo.summary(model, input_data=torch.ones((32, 131), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add loss function and device setup\n",
    "device = torch.device('cpu') #'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X)  # Shape: (batch, seq_len, n_notes)\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "        Y_flat = Y.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs_flat, Y_flat)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs_flat.max(1)\n",
    "        total += Y_flat.size(0)\n",
    "        correct += predicted.eq(Y_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            outputs = model(X)\n",
    "            \n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            Y_flat = Y.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, Y_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += Y_flat.size(0)\n",
    "            correct += predicted.eq(Y_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop \n",
    "def train_model(model, train_loader, valid_loader, optimizer, criterion, epochs=15, device='cpu'):\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "# Run training\n",
    "train_model(model=model, train_loader=train_set, valid_loader=valid_set,  optimizer=optimizer, criterion=criterion, epochs=15, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b763ee",
   "metadata": {},
   "source": [
    "Feel free to iterate on this model now and try to optimize it. For example, you could try removing the LSTM layer and replacing it with Conv1D layers. You could also play with the number of layers, the learning rate, the optimizer, and so on.\n",
    "\n",
    "Once you're satisfied with the performance of the model on the validation set, you can evaluate it one last time on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e22451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            outputs = model(X)\n",
    "            \n",
    "            # Reshape for loss computation\n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            Y_flat = Y.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, Y_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs_flat.max(1)\n",
    "            total += Y_flat.size(0)\n",
    "            correct += predicted.eq(Y_flat).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    print(f'Test Results:')\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_loss, test_acc = evaluate_model(model, test_set, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f71b0c",
   "metadata": {},
   "source": [
    "Now let's write a function that will generate a new chorale. We will give it a few seed chords, it will convert them to arpegios (the format expected by the model), and use the model to predict the next note, then the next, and so on. In the end, it will group the notes 4 by 4 to create chords again, and return the resulting chorale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chorale(model, seed_chords, length, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Convert seed chords to tensor and preprocess\n",
    "        seed_tensor = torch.tensor(seed_chords, dtype=torch.long)\n",
    "        arpegio = preprocess(seed_tensor)\n",
    "        arpegio = arpegio.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        \n",
    "        # Generate new notes\n",
    "        for chord in range(length):\n",
    "            for note in range(4):\n",
    "                # Get model prediction for the current sequence\n",
    "                outputs = model(arpegio)  # Shape: (1, seq_len, n_notes)\n",
    "                \n",
    "                # Get the prediction for the last timestep\n",
    "                last_output = outputs[0, -1, :]  # Shape: (n_notes,)\n",
    "                \n",
    "                # Get the most likely next note\n",
    "                next_note = torch.argmax(last_output, dim=-1, keepdim=True)  # Shape: (1,)\n",
    "                \n",
    "                # Append the predicted note to the sequence\n",
    "                arpegio = torch.cat([arpegio, next_note.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # Convert back to original note range (reverse the preprocessing)\n",
    "        arpegio = torch.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "        \n",
    "        # Reshape to chord format (group every 4 notes)\n",
    "        arpegio_flat = arpegio.squeeze(0)  # Remove batch dimension\n",
    "        n_total_notes = len(arpegio_flat)\n",
    "        n_complete_chords = n_total_notes // 4\n",
    "        \n",
    "        # Take only complete chords and reshape\n",
    "        chorale = arpegio_flat[:n_complete_chords * 4].reshape(-1, 4)\n",
    "        \n",
    "        return chorale.numpy()  # Convert back to numpy for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47274ffa",
   "metadata": {},
   "source": [
    "To test this function, we need some seed chords. Let's use the first 12 chords of one of the test chorales (it's actually just 3 different chords, each played 4 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8eed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_chords = test_chorales[2][:12]\n",
    "baroque_synth.play_chorale(seed_chords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae4b60",
   "metadata": {},
   "source": [
    "Now we are ready to generate our first chorale! Let's ask the function to generate 20 more chords, for a total of 32 chords, i.e., 8 bars (assuming 4 chords per bar, i.e., a 4/4 signature):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305655f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chorale = generate_chorale(model, seed_chords, 32)\n",
    "baroque_synth.play_chorale(new_chorale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68798d",
   "metadata": {},
   "source": [
    "This approach has one major flaw: it is often too conservative. Indeed, the model will not take any risk, it will always choose the note with the highest score, and since repeating the previous note generally sounds good enough, it's the least risky option, so the algorithm will tend to make notes last longer and longer. Pretty boring. Plus, if you run the model multiple times, it will always generate the same melody.\n",
    "\n",
    "So let's spice things up a bit! Instead of always picking the note with the highest score, we will pick the next note randomly, according to the predicted probabilities. For example, if the model predicts a C3 with 75% probability, and a G3 with a 25% probability, then we will pick one of these two notes randomly, with these probabilities. We will also add a temperature parameter that will control how \"hot\" (i.e., daring) we want the system to feel. A high temperature will bring the predicted probabilities closer together, reducing the probability of the likely notes and increasing the probability of the unlikely ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08776472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def generate_chorale_v2(model, seed_chords, length, temperature=1, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        # Convert seed chords to tensor and preprocess\n",
    "        seed_tensor = torch.tensor(seed_chords, dtype=torch.long)\n",
    "        arpegio = preprocess(seed_tensor)\n",
    "        arpegio = arpegio.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        for chord in range(length):\n",
    "            for note in range(4):\n",
    "\n",
    "                outputs = model(arpegio)  # Shape: (1, seq_len, n_notes)\n",
    "                \n",
    "                # Get the prediction for the last timestep\n",
    "                last_output = outputs[0, -1, :]  # Shape: (n_notes,)   \n",
    "\n",
    "                rescaled_logits = last_output / temperature\n",
    "\n",
    "                categorical = Categorical(logits=rescaled_logits)\n",
    "                next_note = categorical.sample().unsqueeze(0) # Shape: (1,)\n",
    "               \n",
    "                # Append the predicted note to the sequence\n",
    "                arpegio = torch.cat([arpegio, next_note.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Convert back to original note range (reverse the preprocessing)\n",
    "    arpegio = torch.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
    "    \n",
    "    # Reshape to chord format (group every 4 notes)\n",
    "    arpegio_flat = arpegio.squeeze(0)  # Remove batch dimension\n",
    "    n_total_notes = len(arpegio_flat)\n",
    "    n_complete_chords = n_total_notes // 4\n",
    "    \n",
    "    # Take only complete chords and reshape\n",
    "    chorale = arpegio_flat[:n_complete_chords * 4].reshape(-1, 4)\n",
    "    \n",
    "    return chorale.numpy()  # Convert back to numpy for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb597aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold_chorale = generate_chorale_v2(model, seed_chords, 32, temperature=0.8)\n",
    "baroque_synth.play_chorale(cold_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_chorale = generate_chorale_v2(model, seed_chords, 32, temperature=1)\n",
    "baroque_synth.play_chorale(medium_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_chorale = generate_chorale_v2(model, seed_chords, 32, temperature=1.5)\n",
    "baroque_synth.play_chorale(hot_chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f138921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
