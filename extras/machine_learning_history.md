# Machine Learning History

## (Focus on Supervised and Unsupervised Learning)

**1676** – Chain rule for differential calculus, fundamental for backpropagation — Leibniz, G. W. (1676).
**1763** – Bayes' theorem for probabilistic inference — Bayes, T. (1763). *An Essay towards solving a Problem in the Doctrine of Chances* (published posthumously).  
**1805** – Method of least squares, foundation of linear regression — Legendre, A. M. (1805).  
**1809** – Normal equations for closed-form solution to linear regression — Gauss, C. F. (1809).  
**1812** – Formalization and popularization of Bayesian probability theory — Laplace, P. S. (1812). *Théorie Analytique des Probabilités*.  
**1815** – Gaussian distribution, cornerstone of statistical modeling — Gauss, C. F. (1815).
**1820–1840** – Central Limit Theorem establishing convergence to normal distribution — Various mathematicians (1820-1840).
**1843** – First computer algorithm, precursor to machine learning programs — Lovelace, A. (1843).  
**1847** – Gradient descent optimization method — Cauchy, A. L. (1847).  
**1858** – Eigenvalue theory and matrix analysis foundations — Cayley, A. & Hamilton, W. R. (1858).  
**1901** – Principal Component Analysis (PCA) introduced by Karl Pearson for dimensionality reduction and data analysis — Pearson, K. (1901).  
**1913** – Discovery of Markov Chains: Andrey Markov first describes techniques he used to analyse a poem. The techniques later become known as Markov chains — Markov, A. (1913).  
**1922** – Ronald Fisher formalizes Maximum Likelihood Estimation, becoming a fundamental statistical method — Fisher, R. (1922).  
**1936** – Universal Turing machine, theoretical foundation of computation — Turing, A. (1936).
**1943** – First mathematical model of artificial neurons — McCulloch, W. & Pitts, W. (1943).  
**1947** – Linear programming and simplex algorithm for constrained optimization — Dantzig, G. (1947).  
**1948** – Information theory, quantifying information content and uncertainty — Shannon, C. (1948). *A Mathematical Theory of Communication*.  
**1957** – Perceptron, first trainable artificial neural network — Rosenblatt, F. (1957). *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*.  
**1965** – First practical deep neural network learning algorithms — Ivakhnenko, A. & Lapa, V. (1965).  
**1967–68** – Stochastic gradient descent for neural network optimization — Amari, S. (1967-68).
**1970** – Automatic differentiation algorithm, theoretical basis for backpropagation — Linnainmaa, S. (1970). *The Representation of the Cumulative Rounding Error of an Algorithm as a Taylor Expansion of the Local Rounding Errors*.  
**1979** – Neocognitron, first hierarchical convolutional neural network — Fukushima, K. (1979).  
**1979** – k-means clustering algorithm for unsupervised partitioning of data into k clusters based on nearest centroids — Hartigan, J. A. & Wong, M. A. (1979). *Algorithm AS 136: A K-Means Clustering Algorithm. Building on MacQueen's 1967 concept*.
**1982** – Recurrent Networks enabling associative memory and pattern completion — Hopfield, J. (1982). *Neural networks and physical systems with emergent collective computational abilities*.
**1986** – Backpropagation algorithm enabling efficient neural network training — Hinton, G., Rumelhart, D., & Williams, R. (1986). *Learning Representations by Back-Propagating Errors*.  
**1989** – LSTM invented. Networks solving vanishing gradient problem in RNNs — Hochreiter, S. & Schmidhuber, J. (1997). *Long Short-Term Memory*.  
**1992** – Weight decay (L2 regularization) preventing overfitting in neural networks — Krogh, A. & Hertz, J. (1992). *A Simple Weight Decay Can Improve Generalization*.  
**1995–1999** – Support Vector Machines with kernel trick for non-linear classification — Vapnik, V. (1995-1999).  
**1997** – Deep Blue defeats world chess champion, AI milestone — IBM (1997).
**1998** – MNIST dataset standard benchmark for handwritten digit recognition — LeCun, Y. et al. (1998). *Gradient-Based Learning Applied to Document Recognition*.  
**1998** – LeNet-5: successful application of CNNs to handwritten digit recognition — LeCun, Y. et al. (1998). *Gradient-Based Learning Applied to Document Recognition*.  
**1999-2001** – Gradient Boosting machines combining weak learners sequentially to correct errors — Friedman, J. (1999-2001). *Greedy Function Approximation: A Gradient Boosting Machine*.
**2001** – Random Forests, powerful ensemble method for classification and regression — Breiman, L. (2001). *Random Forests*.  
**2002** – Torch framework democratizing machine learning research — Torch Development Team (2002).
**2006** – Deep Belief Networks enabling unsupervised pre-training of deep networks — Hinton, G. et al. (2006). *A Fast Learning Algorithm for Deep Belief Nets*.  
**2007** – CUDA platform enabling massively parallel GPU computation — NVIDIA (2007).  
**2009** – ImageNet dataset establishing large-scale visual recognition benchmark — Deng, J. et al. (2009).  
**2010** – ReLU activation function enabling deeper network training — Nair, V. & Hinton, G. (2010).  
**2010** – Xavier initialization solving gradient flow in deep networks — Glorot, X. & Bengio, Y. (2010). *Understanding the difficulty of training deep feedforward neural networks*.  
**2011** – Siri democratizing AI through voice interfaces — Apple Inc. (2011).  
**2012** – Dropout preventing overfitting through random neuron deactivation — Hinton, G. et al. (2012). *Improving neural networks by preventing co-adaptation of feature detectors*.  
**2012** – AlexNet breakthrough: deep CNNs + GPUs dominating computer vision — Krizhevsky, A., Sutskever, I., & Hinton, G. (2012).  
**2013** – Word2Vec creating dense semantic word representations — Mikolov, T. et al. (2013). *Efficient Estimation of Word Representations in Vector Space*.  
**2014** – Attention mechanism enabling focus on relevant input regions — Bahdanau, D. et al. (2014). *Neural Machine Translation by Jointly Learning to Align and Translate*.  
**2014** – Variational Autoencoders combining probabilistic modeling with deep learning — Kingma, D. P. & Welling, M. (2014). *Auto-Encoding Variational Bayes*.  
**2014** – Generative Adversarial Networks: adversarial training for realistic data generation — Goodfellow, I. et al. (2014). *Generative Adversarial Nets*.  
**2015** – Batch Normalization stabilizing and accelerating deep network training — Ioffe, S. & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift*.  
**2015** – Adam optimizer combining momentum with adaptive learning rates — Kingma, D. P. & Ba, J. (2015). *Adam: A Method for Stochastic Optimization*.  
**2015** – ResNet using skip connections to train extremely deep networks — He, K. et al. (2015). *Deep Residual Learning for Image Recognition*.  
**2015** – U-Net encoder-decoder architecture for precise image segmentation — Ronneberger, O. et al. (2015). *U-Net: Convolutional Networks for Biomedical Image Segmentation*.  
**2015** – Diffusion models using thermodynamic principles for generation — Sohl-Dickstein, J. et al. (2015). *Deep Unsupervised Learning using Nonequilibrium Thermodynamics*.  
**2015** – YOLO unified single-shot object detection architecture — Redmon, J. et al. (2015). *You Only Look Once: Unified, Real-Time Object Detection*.  
**2016** – Layer Normalization improving training stability across sequence lengths — Ba, J. L. et al. (2016). *Layer Normalization*.  
**2016** – Neural style transfer combining content and artistic style in images — Gatys, L. A. et al. (2016). *Image Style Transfer Using Convolutional Neural Networks*.  
**2016** – WaveNet generating raw audio waveforms with dilated convolutions — van den Oord, A. et al. (2016). *WaveNet: A Generative Model for Raw Audio*.  
**2016** – AlphaGo achieving superhuman performance in complex strategy game — Silver, D. et al. (2016). Google DeepMind.  
**2017** – PyTorch enabling dynamic neural networks with eager execution — Paszke, A. et al. (2017). Facebook AI Research.  
**2017** – Transformer architecture replacing RNNs with self-attention mechanisms — Vaswani, A. et al. (2017). *Attention Is All You Need*.  
**2018** – GPT-1 demonstrating unsupervised pre-training for language understanding — Radford, A. et al. (2018). *Improving Language Understanding by Generative Pre-Training*.  
**2018** – BERT achieving bidirectional context understanding through masked language modeling — Devlin, J. et al. (2018). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*.  
**2020** – GPT-3 exhibiting emergent few-shot learning capabilities at massive scale — Brown, T. B. et al. (2020). *Language Models are Few-Shot Learners*.  
**2020** – DDPM making diffusion models practical for high-quality image synthesis — Ho, J. et al. (2020). *Denoising Diffusion Probabilistic Models*.
**2021** – Vision Transformer proving Transformers can excel beyond NLP — Dosovitskiy, A. et al. (2021). *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*.  
**2021** – CLIP learning joint text-image representations through contrastive learning — Radford, A. et al. (2021). *Learning Transferable Visual Representations from Natural Language Supervision*.  
**2021** – S4 achieving linear-time sequence modeling with structured state spaces — Gu, A. et al. (2021). *Efficiently Modeling Long Sequences with Structured State Spaces*.  
**2021** – RAVE enabling real-time neural audio synthesis and manipulation — Caillon, A. & Esling, P. (2021).  *RAVE: A Real-time Audio Variational Autoencoder for End-to-End Sound Modeling*.
**2022** – ChatGPT demonstrating conversational AI capabilities to mainstream audiences — OpenAI (2022). Based on GPT-3.5.  
**2022** – Stable Diffusion open-sourcing high-quality text-to-image generation — Rombach, R. et al. (2022). Stability AI.  
**2022** – DiT (Diffusion Transformer) replacing U-Net with Transformer architecture in diffusion — Peebles, W. & Xie, S. (2022). *Scalable Diffusion Models with Transformers*.  
**2023** – LLaMA demonstrating efficient training of competitive open-source language models — Touvron, H. et al. (2023). *LLaMA: Open and Efficient Foundation Language Models*.  
**2023** – Mamba achieving linear scaling for sequence length with selective state spaces — Gu, A. & Dao, T. (2023). *Mamba: Linear-Time Sequence Modeling with Selective State Spaces*.
